---
author: David Carslaw
---

# Utility functions {#sec-utility}

## Selecting data by date {#sec-selectByDate}

Selecting by date/time in R can be intimidating for new users---and time-consuming for all users. The `selectByDate` function aims to make this easier by allowing users to select data based on the British way of expressing date i.e. d/m/y. This function should be very useful in circumstances where it is necessary to select only part of a data frame.

First load the packages we need.

```{r}
#| message: false
#| warning: false
library(openair)
library(tidyverse)
```

```{r }
## select all of 1999
data.1999 <- selectByDate(mydata, start = "1/1/1999", end = "31/12/1999")
head(data.1999)
tail(data.1999)

## easier way
data.1999 <- selectByDate(mydata, year = 1999)

## more complex use: select weekdays between the hours of 7 am to 7 pm
sub.data <- selectByDate(mydata, day = "weekday", hour = 7:19)

## select weekends between the hours of 7 am to 7 pm in winter (Dec, Jan, Feb)
sub.data <- selectByDate(mydata, day = "weekend", hour = 7:19,
                           month = c(12, 1, 2))
```

The function can be used directly in other functions. For example, to make a polar plot using year 2000 data:

```{r}
#| eval: false
polarPlot(selectByDate(mydata, year = 2000), pollutant = "so2")
```

## Making intervals --- cutData {#sec-cutData}

The `cutData` function is a utility function that is called by most other functions but is useful in its own right. Its main use is to partition data in many ways, many of which are built-in to [openair]{.pkg}

Note that all the date-based types e.g. month/year are derived from a column `date`. If a user already has a column with a name of one of the date-based types it will not be used.

For example, to cut data into seasons:

```{r}
#| label: cutdataexam
mydata <- cutData(mydata, type = "season")
head(mydata)
```

This adds a new field `season` that is split into four seasons. There is an option `hemisphere` that can be used to use southern hemisphere seasons when set as `hemisphere = "southern"`.

The `type` can also be another field in a data frame e.g.

```{r}
#| label: cutdata2
mydata <- cutData(mydata, type = "pm10")
head(mydata)
data(mydata) ## re-load mydata fresh
```

This divides PM~10~ concentrations into four *quantiles* --- roughly equal numbers of PM~10~ concentrations in four levels.

Most of the time users do not have to call `cutData` directly because most functions have a `type` option that is used to call `cutData` directly e.g.

```{r}
#| eval: false
polarPlot(mydata, pollutant = "so2", type = "season")
```

However, it can be useful to call `cutData` *before* supplying the data to a function in a few cases. First, if one wants to set seasons to the southern hemisphere as above. Second, it is possible to override the division of a numeric variable into four quantiles by using the option `n.levels`. More details can be found in the `cutData` help file.

## Selecting run lengths of values above a threshold --- pollution episodes {#sec-selectRunning}

A seemingly easy thing to do that has relevance to air pollution episodes is to select run lengths of contiguous values of a pollutant above a certain threshold. For example, one might be interested in selecting O~3~ concentrations where there are at least 8 consecutive hours above 90\~ppb. In other words, a selection that combines both a threshold and *persistence*. These periods can be very important from a health perspective and it can be useful to study the conditions under which they occur. But how do you select such periods easily? The `selectRunning` utility function has been written to do this. It could be useful for all sorts of situations e.g.

-   Selecting hours when primary pollutant concentrations are persistently high --- and then applying other [openair]{.pkg} functions to analyse the data in more depth.

-   In the study of particle suspension or deposition etc. it might be useful to select hours when wind speeds remain high or rainfall persists for several hours to see how these conditions affect particle concentrations.

-   It could be useful in health impact studies to select blocks of data where pollutant concentrations remain above a certain threshold.

As an example we are going to consider O~3~ concentrations at a semi-rural site in south-west London (Teddington). The data can be downloaded as follows:

```{r}
#| label: importTed
ted <- importKCL(site = "td0", year = 2005:2009, met = TRUE)
## see how many rows there are
nrow(ted)
```

We are going to contrast two pollution roses of O~3~ concentration. The first shows hours where the criterion is not met, and the second where it is met. The subset of hours is defined by O~3~ concentrations above 90 ppb for periods of at least 8-hours i.e. what might be considered as ozone episode conditions.

```{r}
#| label: episodeSelect
ted <- selectRunning(ted, pollutant = "o3", 
                         threshold = 90, 
                         run.len = 8)
```

The `selectRunning` function returns a new column `criterion` that flags whether the condition is met or not. The user can control the text provided, which by default is "yes" and "no".

```{r}
#| label: criterion
table(ted$criterion)
```

Now we are going to produce two pollution roses shown in @fig-RunningRes. Note, however that many other types of analysis could be carried out now the data have been partitioned.

```{r}
#| label: fig-RunningRes
#| fig-cap: Example of using the `selectRunning` function to select episode hours to produce pollution roses of O~3~ concentration.
#| fig-width: 8
#| fig-height: 5
#| out-width: 90%
pollutionRose(ted, pollutant = "o3", 
          type = "criterion")
```

The results are shown in @fig-RunningRes. The pollution rose for the "no" criterion (left plot of @fig-RunningRes) shows that the highest O~3~ concentrations tend to occur for wind directions from the south-west, where there is a high proportion of measurements. By contrast, the when the criterion is met (right plot of @fig-RunningRes) is very different. In this case there is a clear set of conditions where these criteria are met i.e. lengths of at least 8-hours where the O~3~ concentration is at least 90 ppb. It is clear the highest concentrations are dominated by south-easterly conditions i.e. corresponding to easterly flow from continental Europe where there has been time to the O~3~ chemistry to take place.

The code below shows (as an example), that the summer of 2006 had a high proportion of conditions where the criterion was met.

```{r}
#| eval: false
timeProp(ted, pollutant = "o3", 
         proportion = "criterion", 
         avg.time = "month", 
         cols = "viridis")
```

It is also useful to consider what controls the highest NO~x~ concentrations at a central London roadside site. For example, the code below (not plotted) shows very strongly that the persistently highest NO~x~ concentrations are dominated by south-westerly winds. As mentioned earlier, there are many other types of analysis that can be carried out now the data set identifies where the criterion is or is not met.

```{r}
#| label: runningNOx
#| eval: false
episode <- selectRunning(mydata, pollutant = "nox", 
                         threshold = 500, 
                         run.len = 5)

pollutionRose(episode, pollutant = "nox", type = "criterion")
```

## Calculating rolling means {#sec-rollingMean}

Some air pollution statistics such as for O~3~ and particulate matter are expressed as rolling means and it is useful to be able to calculate these. It can also be useful to help smooth-out data for clearer plotting. The `rollingMean` function makes these calculations. One detail that can be important is that for some statistics a mean is only considered valid if there are a sufficient number of valid readings over the averaging period. Often there is a requirement for at least 75% data capture. For example, with an averaging period of 8 hours and a data capture threshold of 75%, at least 6 hours are required to calculate the mean.

The function is called as follows; in this case to calculate 8-hour rolling mean concentrations of O~3~.

```{r}
#| label: selectBydate
mydata <- rollingMean(mydata, pollutant = "o3", width = 8,
                       new.name = "rollingo3", data.thresh = 75)
tail(mydata)
```

Note that calculating rolling means shortens the length of the data set. In the case of O~3~, no calculations are made for the last 7 hours.

Type `?rollingMean` into R for more details. Note that the function currently only works with a single site.

## Aggregating data by different time intervals {#sec-timeAverage}

Aggregating data by different averaging periods is a common and important task. There are many reasons for aggregating data in this way:

-   Data sets may have different averaging periods and there is a need to combine them. For example, the task of combining an hourly air quality data set with a 15-minute average meteorological data set. The need here would be to aggregate the 15-minute data to 1-hour before merging.

-   It is extremely useful to consider data with different averaging times straightforwardly. Plotting a very long time series of hourly or higher resolution data can hide the main features and it would be useful to apply a specific (but flexible) averaging period to the data for plotting.

-   Those who make measurements during field campaigns (particularly for academic research) may have many instruments with a range of different time resolutions. It can be useful to re-calculate time series with a common averaging period; or maybe help reduce noise.

-   It is useful to calculate statistics other than means when aggregating e.g. percentile values, maximums etc.

-   For statistical analysis there can be short-term autocorrelation present. Being able to choose a longer averaging period is sometimes a useful strategy for minimising autocorrelation.

In aggregating data in this way, there are a couple of other issues that can be useful to deal with at the same time. First, the calculation of proper vector-averaged wind direction is essential. Second, sometimes it is useful to set a minimum number of data points that must be present before the averaging is done. For example, in calculating monthly averages, it may be unwise to not account for data capture if some months only have a few valid points.

When a data capture threshold is set through `data.thresh` it is necessary for `timeAverage` to know what the original time interval of the input time series is. The function will try and calculate this interval based on the most common time gap (and will print the assumed time gap to the screen). This works fine most of the time but there are occasions where it may not e.g. when very few data exist in a data frame. In this case the user can explicitly specify the interval through `interval` in the same format as `avg.time` e.g. `interval = "month"`. It may also be useful to set `start.date` and `end.date` if the time series do not span the entire period of interest. For example, if a time series ended in October and annual means are required, setting `end.date` to the end of the year will ensure that the whole period is covered and that `data.thresh` is correctly calculated. The same also goes for a time series that starts later in the year where `start.date` should be set to the beginning of the year.

All these issues are (hopefully) dealt with by the `timeAverage` function. The options are shown below, but as ever it is best to check the help that comes with the [openair]{.pkg} package.

To calculate daily means from hourly (or higher resolution) data:

```{r}
#| label: dailyAve
daily <- timeAverage(mydata, avg.time = "day")
daily
```

Monthly 95th percentile values:

```{r}
#| label: percentile95
monthly <- timeAverage(mydata, avg.time = "month", 
                       statistic = "percentile",
                       percentile = 95)
monthly
```

2-week averages but only calculate if at least 75% of the data are available:

```{r}
#| label: twoweek
twoweek <- timeAverage(mydata, avg.time = "2 week", 
                       data.thresh = 75)
twoweek
```

Note that `timeAverage` has a `type` option to allow for the splitting of variables by a grouping variable. The most common use for `type` is when data are available for different sites and the averaging needs to be done on a per site basis.

First, retaining by site averages:

```{r}
#| label: timeAvType
# import some data for two sites
dat <- importAURN(c("kc1", "my1"), year = 2011:2013)

# annual averages by site
timeAverage(dat, avg.time = "year", type = "site")
```

Retain site name and site code:

```{r}
#| label: timeAvTypeCode
# can also retain site code
timeAverage(dat, avg.time = "year", type = c("site", "code"))
```

Average all data across sites (drops site and code):

```{r}
#| label: timeAvNoType
timeAverage(dat, avg.time = "year")
```

`timeAverage` also works the other way in that it can be used to derive higher temporal resolution data e.g. hourly from daily data or 15-minute from hourly data. An example of usage would be the combining of daily mean particle data with hourly meteorological data. There are two ways these two data sets can be combined: either average the meteorological data to daily means or calculate hourly means from the particle data. The `timeAverage` function when used to 'expand' data in this way will repeat the original values the number of times required to fill the new time scale. In the example below we calculate 15-minute data from hourly data. As it can be seen, the first line is repeated four times and so on.

```{r}
#| label: timeAvgFill
data15 <- timeAverage(mydata, 
                      avg.time = "15 min", 
                      fill = TRUE)
head(data15, 20)
```

The `timePlot` can apply this function directly to make it very easy to plot data with different averaging times and statistics.

## Calculating percentiles {#sec-calcPercentile}

`calcPercentile` makes it straightforward to calculate percentiles for a single pollutant. It can take account of different averaging periods, data capture thresholds --- see @sec-timeAverage for more details.

For example, to calculate the 25, 50, 75 and 95th percentiles of O~3~ concentration by year:

```{r}
#| label: calcpercentile
calcPercentile(mydata, pollutant  = "o3", 
               percentile = c(25, 50, 75, 95),
               avg.time = "year")
```

## Correlation matrices {#sec-corPlot}

Understanding how different variables are related to one another is always important. However, it can be difficult to easily develop an understanding of the relationships when many different variables are present. One of the useful techniques used is to plot a *correlation matrix*, which provides the correlation between all pairs of data. The basic idea of a correlation matrix has been extended to help visualise relationships between variables by [@friendly2002] and [@Sarkar2008].

The `corPlot` shows the correlation coded in three ways: by shape (ellipses), colour and the numeric value. The ellipses can be thought of as visual representations of scatter plot. With a perfect positive correlation a line at 45 degrees positive slope is drawn. For zero correlation the shape becomes a circle --- imagine a 'fuzz' of points with no relationship between them.

With many variables it can be difficult to see relationships between variables i.e. which variables tend to behave most like one another. For this reason hierarchical clustering is applied to the correlation matrices to group variables that are most similar to one another (if `cluster = TRUE`.)

It is also possible to use the [openair]{.pkg} type option to condition the data in many flexible ways, although this may become difficult to visualise with too many panels.

An example of the `corPlot` function is shown in @fig-corPlot. In this Figure it can be seen the highest correlation coefficient is between PM~10~ and PM~2.5~ (r = 0. 84) and that the correlations between SO~2~, NO~2~ and NO~x~ are also high. O~3~ has a negative correlation with most pollutants, which is expected due to the reaction between NO and O~3~. It is not that apparent in @fig-corPlot that the order the variables appear is due to their similarity with one another, through hierarchical cluster analysis. In this case we have chosen to also plot a *dendrogram* that appears on the right of the plot. Dendrograms provide additional information to help with visualising how groups of variables are related to one another. Note that dendrograms can only be plotted for `type = "default"` i.e. for a single panel plot.

```{r}
#| label: fig-corPlot
#| fig-cap: Example of a correlation matrix showing the relationships between variables.
#| fig-width: 7
#| fig-height: 6
#| out-width: 75%
corPlot(mydata, dendrogram = TRUE)
```

Note also that the `corPlot` accepts a `type` option, so it possible to condition the data in many flexible ways, although this may become difficult to visualise with too many panels. For example:

```{r}
#| eval: false
corPlot(mydata, type = "season")
```

When there are a very large number of variables present, the `corPlot` is a very effective way of quickly gaining an idea of how variables are related. As an example (not plotted) it is useful to consider the hydrocarbons measured at Marylebone Road. There is a lot of information in the hydrocarbon plot (about 40 species), but due to the hierarchical clustering it is possible to see that isoprene, ethane and propane behave differently to most of the other hydrocarbons. This is because they have different (non-vehicle exhaust) origins. Ethane and propane results from natural gas leakage whereas isoprene is biogenic in origin (although some is from vehicle exhaust too). It is also worth considering how the relationships change between the species over the years as hydrocarbon emissions are increasingly controlled, or maybe the difference between summer and winter blends of fuels and so on.

```{r}
#| eval: false
hc <- importAURN(site = "my1", year = 2005, hc = TRUE)
## now it is possible to see the hydrocarbons that behave most
## similarly to one another
corPlot(hc)
```
