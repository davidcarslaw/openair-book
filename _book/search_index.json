[
["index.html", "openair Preface Hello and welcome", " openair David Carslaw 2020-07-25 Preface Hello and welcome This document has been a long time coming. The openair project started with funding from the UK Natural Environment Environment Research Council (NERC) over 10 years ago. The main aim was to fill a perceived gap in that there was a lack of a dedicated set of easily accessible, open source tools for analysing air quality data. At that time R was beginning to become increasingly popular but far, far less than it is today. openair is a product of the time it was started and used the highly capable lattice package for plotting. This was a time before ggplot2 and the ‘tidyverse’. Nevertheless, the package is used extensively around the world (see downloads here) and if anything, growing in popularity and use. The original aims of the project have been met in providing tools for academia, the private and public sectors — all of which continue to use the software. At some point there will need to be a transition to ggplot2; in particular to capitalise on the many extensions available and which openair can benefit from. The reason for writing this book (or manual) in this form i.e. a website rather than a pdf or Word document is convenience for all involved. For me it makes it much easier to keep the information up to date and ensure that the information is reproducible. For the reader it is something that can easily be read and navigated in a browser. Where code is involved — as it is heavily in this book — it is very easy to use the copy icon at the top right of each code block to make it easy to copy into R. Finally, it is increasingly the case that information can be plotted interactively, which is not something that can easily be done in a pdf or Word document. This book will be developed over the coming months. To cite openair please use Carslaw and Ropkins (2012). This document was produced using R version 4.0.2 and openair version 2.7-6. References "],
["intro.html", "Section1 Introduction", " Section1 Introduction This document provides information on the use of R to analyse air pollution data. The document supports an initiative to develop and make available a consistent set of tools for analysing and understanding air pollution data in a free, open-source environment. The amount of monitoring data available is substantial and increasing. In the UK alone there are thought to be over 1000 continuous monitoring sites. Much of the data available is only briefly analysed; perhaps with the aim of comparing pollutant concentrations with national and international air quality limits. However, as it will hopefully be seen, the critical analysis of air pollution data can be highly rewarding, perhaps yielding important information on pollutant sources that was previously unknown or unquantified. While many of the options in these functions allow quite a sophisticated analysis to be undertaken, the defaults generally use the simplest (and fastest) assumptions. A more detailed analysis can refine these assumptions e.g. by accounting for autocorrelation, or fine-control over the appearance of a plot. It should be noted that while the aim is to keep this documentation up to date, the primary source of information related to the different functions is contained within the package itself. Once loaded, type ?openair to see all the help pages associated with the package. The website for openair is https://davidcarslaw.github.io/openair/. The next Section contains important information on loading the openair package for the first time and the input data requirements. Users will need to consider the advice in this section to ensure that openair can be used without problems.} "],
["sec-openair-package.html", "Section2 The openair package 2.1 Access to source code 2.2 Input data requirements 2.3 Brief overview of openair 2.4 The type option 2.5 Controlling font size 2.6 Using colours 2.7 Automatic text formatting 2.8 Multiple plots on a page", " Section2 The openair package SECTION NEEDS UPDATING In this book two packages are frequently used and it is a good idea to load both. library(openair) library(tidyverse) Because the openair package (and R itself) are continually updated, it will be useful to know this document was produced using R version 4.0.2 and openair version 2.7-6. 2.1 Access to source code All R code is accessible. On CRAN, you will see there are various versions of packages: Package source, MacOS binary and Windows binary. The source code is contained in the Package source, which is a tar.gz (compressed file). For openair all development is carried out using Github for version control. Users can access all code used in openair at (https://github.com/davidcarslaw/openair). 2.2 Input data requirements The openair package applies certain constraints on input data requirements. It is important to adhere to these requirements to ensure that data are correctly formatted for use in openair. The principal reason for insisting on specific input data format is that there will be less that can go wrong and it is easier to write code for a more limited set of conditions. Data should be in a data frame (or tibble). The date/time field should be called date — note the lower case. No other name is acceptable. The wind speed and wind direction should be named ws and wd, respectively (note again, lower case). Wind directions follow the UK Met Office format and are represented as degrees from north e.g. 90 degrees is east. North is taken to be 360 degrees Where fields should have numeric data e.g. concentrations of NOx, then the user should ensure that no other characters are present in the column, accept maybe something that represents missing data e.g. `no data’. . Other variables names can be upper/lower case but should not start with a number. If column names do have white spaces, R will automatically replace them with a full-stop. While PM2.5 as a field name is perfectly acceptable, it is a pain to type it in—better just to use pm25 (openair will recognise pollutant names like this and automatically format them as PM2.5 in plots). 2.3 Brief overview of openair This section gives a brief overview of the functions in openair. Having read some data into a data frame it is then straightforward to run any function. Almost all functions are run as: functionName(thedata, options, ...) The usage is best illustrated through a specific example, in this case the polarPlot function. The details of the function are shown in Section 8 and through the help pages (type ?polarPlot). As it can be seen there are numerous options associated with polarPlot — and most other functions and each of these has a default. For example, the default pollutant considered in polarPlot is nox. If the user has a data frame called theData then polarPlot could minimally be called by: polarPlot(theData) which would plot a nox polar plot if nox was available in the data frame theData. Note that the options do not need to be specified in order nor is it always necessary to write the whole word. For example, it is possible to write: polarPlot(theData, type = &quot;year&quot;, poll = &quot;so2&quot;) In this case writing poll is sufficient to uniquely identify that the option is pollutant. Also there are many common options available in functions that are not explicitly documented, but are part of lattice graphics. Some common ones are summarised in Table 2.1. The layout option allows the user to control the layout of multi-panel plots e.g. layout = c(4, 1) would ensure a four-panel plot is 4 columns by 1 row. Table 2.1: Common options used in openair plots that can be set by the user but are generally not explicitly documented. option description xlab x-axis label ylab y-axis label main title of the plot pch plotting symbol used for points cex size of symbol plotted lty line type lwd line width layout the plot layout e.g. c(2, 2) 2.4 The type option One of the central themes in openair is the idea of conditioning. Rather than plot \\(x\\) against \\(y\\), considerably more information can usually be gained by considering a third variable, \\(z\\). In this case, \\(x\\) is plotted against \\(y\\) for many different intervals of \\(z\\). This idea can be further extended. For example, a trend of NOx against time can be conditioned in many ways: NOx vs. time split by wind sector, day of the week, wind speed, temperature, hour of the day … and so on. This type of analysis is rarely carried out when analysing air pollution data, in part because it is time consuming to do. However, thanks to the capabilities of R and packages such as lattice and ggplot2, it becomes easier to work in this way. In most openair functions conditioning is controlled using the type option. type can be any other variable available in a data frame (numeric, character or factor). A simple example of type would be a variable representing a ‘before’ and ‘after’ situation, say a variable called period i.e. the option type = \"period\" is supplied. In this case a plot or analysis would be separately shown for ‘before’ and ‘after’. When type is a numeric variable then the data will be split into four quantiles and labelled accordingly. Note however the user can set the quantile intervals to other values using the option n.levels. For example, the user could choose to plot a variable by different levels of temperature. If n.levels = 3 then the data could be split by ‘low’, ‘medium’ and ‘high’ temperatures, and so on. Some variables are treated in a special way. For example if type = \"wd\" then the data are split into 8 wind sectors (N, NE, E, …) and plots are organised by points of the compass. There are a series of pre-defined values that type can take related to the temporal components of the data. To use these there must be a date field so that it can be calculated. These pre-defined values of type are shown below are both useful and convenient. Given a data frame containing several years of data it is easy to analyse the data e.g. plot it, by year by supplying the option type = \"year\". Other useful and straightforward values are “hour” and “month”. When type = \"season\" openair will split the data by the four seasons (winter = Dec/Jan/Feb etc.). Note for southern hemisphere users that the option hemisphere = \"southern\" can be given. When type = \"daylight\" is used the data are split between nighttime and daylight hours. In this case the user can also supply the options latitude and longitude for their location (the default is London). And a brief summary of in-built types are shown in Table 2.2 Table 2.2: Built-in ways of splitting data in openair using the type options that is available for most functions. option description ‘year’ splits data by year ‘month’ splits data by month of the year ‘week’ splits data by week of the year ‘monthyear’ splits data by year and month ‘season’ splits data by season. Note in this case the user can also supply a hemisphere option that can be either ‘northern’ (default) or ‘southern’ ‘weekday’ splits data by day of the week ‘weekend’ splits data by Saturday, Sunday, weekday ‘daylight’ splits data by nighttime/daytime. Note the user must supply a longitude and latitude ‘dst’ splits data by daylight saving time and non-daylight saving time ‘wd’ if wind direction (wd) is available type = 'wd' will split the data into 8 sectors: N, NE, E, SE, S, SW, W, NW ‘seasonyear’ will split the data into year-season intervals, keeping the months of a season together. For example, December 2010 is considered as part of winter 2011 (with January and February 2011). This makes it easier to consider contiguous seasons. In contrast, type = 'season' will just split the data into four seasons regardless of the year. If a categorical variable is present in a data frame e.g. site then that variables can be used directly e.g. type = \"site\". 2.5 Controlling font size All openair plot functions have an option fontsize. Users can easily vary the size of the font for each plot e.g. polarPlot(mydata, fontsize = 20) The font size will be reset to the default sizes once the plot is complete. Finer control of individual font sizes is currently not easily possible. 2.6 Using colours Many of the functions described require that colour scales are used; particularly for plots showing surfaces. It is only necessary to consider using other colours if the user does not wish to use the default scheme, shown at the top of Figure 2.1. The choice of colours does seem to be a vexing issue as well as something that depends on what one is trying to show in the first place. For this reason, the colour schemes used in openair are very flexible: if you don’t like them, you can change them easily. R itself can handle colours in many sophisticated ways; see for example the RColorBrewer package. Several pre-defined colour schemes are available to make it easy to plot data. In fact, for most situations the default colour schemes should be adequate. The choice of colours can easily be set; either by using one of the pre-defined schemes or through a user-defined scheme. More details can be found in the openair openColours function. Some defined colours are shown in Figure 2.1, together with an example of a user defined scale that provides a smooth transition from yellow to blue. Figure 2.1: Selected pre-defined colour scales in. The top colour scheme is a user-defined one. The user-defined scheme is very flexible and the following provides examples of its use. In the examples shown next, the polarPlot function is used as a demonstration of their use. # use default colours - no need to specify polarPlot(mydata) # use pre-defined &quot;jet&quot; colours polarPlot(mydata, cols = &quot;jet&quot;) # define own colours going from yellow to green polarPlot(mydata, cols = c(&quot;yellow&quot;, &quot;green&quot;)) # define own colours going from red to white to blue polarPlot(mydata, cols = c(&quot;red&quot;, &quot;white&quot;, &quot;blue&quot;)) For more detailed information on using appropriate colours, have a look at the colorspace package. colorspace provides the definitive, comprehensive approach to using colours effectively. You will need to install the package, install.packages(\"colorspace\"). To use the palettes with openair, you can for example do: library(colorspace) library(openair) windRose(mydata, cols = qualitative_hcl(4, palette = &quot;Dark 3&quot;)) Figure 2.2: Example of using a colorspace palette with an openair function. 2.7 Automatic text formatting openair tries to automate the process of annotating plots. It can be time-consuming (and tricky) to repetitively type in text to represent \\(\\mu\\)g m-3 or PM10 (\\(\\mu\\)g m-3) etc. in R. For this reason, an attempt is made to automatically detect strings such as nox or NOx and format them correctly. Where a user needs a y-axis label such as NOx (\\(\\mu\\)g m-3) it will only be necessary to type ylab = \"nox (ug/m3)\". The same is also true for plot titles. Users can override this option by setting it to FALSE. 2.8 Multiple plots on a page We often get asked how to combine multiple plots on one page. Recent changes to openair makes this a bit easier. Note that because openair uses lattice} graphics the base graphicspar} settings will not work. It is possible to arrange plots based on a column \\(\\times\\) row layout. Let’s put two plots side by side (2 columns, 1 row). First it is necessary to assign the plots to a variable: a &lt;- windRose(mydata) b &lt;- polarPlot(mydata) Now we can plot them using the split option: print(a, split = c(1, 1, 2, 1)) print(b, split = c(2, 1, 2, 1), newpage = FALSE) In the code above for the split option, the last two numbers give the overall layout (2, 1) — 2 columns, 1 row. The first two numbers give the column/row index for that particular plot. The last two numbers remain constant across the series of plots being plotted. There is one difficulty with plots that already contain sub-plots such as timeVariation where it is necessary to identify the particular plot of interest (see the timeVariation help for details). However, say we want a polar plot (b above) and a diurnal plot: c &lt;- timeVariation(mydata) print(b, split = c(1, 1, 2, 1)) print(c, split = c(2, 1, 2, 1), subset = &quot;hour&quot;, newpage = FALSE) For more control it is possible to use the position argument. position is a vector of 4 numbers, c(xmin, ymin, xmax, ymax) that give the lower-left and upper-right corners of a rectangle in which the plot is to be positioned. The coordinate system for this rectangle is [0–1] in both the x and y directions. As an example, consider plotting the first plot in the lower left quadrant and the second plot in the upper right quadrant: print(a, position = c(0, 0, 0.5, 0.5), more = TRUE) print(b, position = c(0.5, 0.5, 1, 1)) The position argument gives more fine control over the plot location. "],
["sec-importAURN.html", "Section3 Accessing UK Air Quality Data 3.1 Accessing data 3.2 Site Meta Data 3.3 Plot Sites on a Map", " Section3 Accessing UK Air Quality Data The UK has a surprisingly large amount of air quality data that is publicly accessible. The main UK AURN archive and regional (England, Scotland, Wales and Northern Ireland) together with Imperial College London’s London Air Quality Network (LAQN) are important and large databases of information that allow free public access. Storing and managing data in this way has many advantages including consistent data format, and underlying high quality methods to process and store the data. 3.1 Accessing data openair has a family of functions that provide users with extensive access to UK air quality data. Ricardo Energy &amp; Environment have provided .RData files (R workspaces) for several important air quality networks in the UK. These files are updated on a daily basis. This approach requires a link to the Internet to work. The networks include: importAURN For importing data from the UK national network called Automatic Urban and Rural Network}. This is the main UK network. importSAQN For accessing data from Air Quality Scotland network. importWAQN For accessing data from the Air Quality Wales network. importAQE For accessing data from the Air Quality England network of sites. importNI For accessing data from the Northern Ireland network of sites. importEurope A simplified version of a function to give basic access to hourly European data based on Stuart Grange’s package — see https://github.com/skgrange/saqgetr. The openair function has a similar approach to other openair import functions i.e. requires a site code(s) and year(s) to be supplied. importKCL For accessing data from the sites operated by King’s College London, primarily including the The London Air Quality Network. Many users download hourly data from the air quality archive at http://www.airquality.co.uk. Most commonly, the data are emailed to the user as .csv files and have a fixed format as shown below. This is a useful facility but does have some limitations and frustrations, many of which have been overcome using a new way of storing and downloading the data described below. There are several advantages over the web portal approach where .csv files are downloaded. First, it is quick to select a range of sites, pollutants and periods (see examples below). Second, storing the data as .RData objects is very efficient as they are about four times smaller than .csv files (which are already small) — which means the data downloads quickly and saves bandwidth. Third, the function completely avoids any need for data manipulation or setting time formats, time zones etc. Finally, it is easy to import many years of data. The final point makes it possible to download several long time series in one go. The site codes and pollutant names can be upper or lower case. The function will issue a warning when data less than six months old is downloaded, which may not be ratified. Type ?importAURN for a full listing of sites and their codes. Some examples of usage are shown below. First load the packages we need. library(openair) library(tidyverse) 3.2 Site Meta Data The first question is, what sites are available and what do they measure? Users can access the details of air pollution monitoring sites using the importMeta function. The user only needs to provide the network name and (optionally) whether all data should be returned. By default only site type, latitude and longitude are returned. aurn_meta &lt;- importMeta(source = &quot;aurn&quot;) aurn_meta ## # A tibble: 273 x 5 ## site code latitude longitude site_type ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Aberdeen ABD 57.2 -2.09 Urban Background ## 2 Aberdeen Union Street Roadside ABD7 57.1 -2.11 Urban Traffic ## 3 Aberdeen Wellington Road ABD8 57.1 -2.09 Urban Traffic ## 4 Armagh Roadside ARM6 54.4 -6.65 Urban Traffic ## 5 Aston Hill AH 52.5 -3.03 Rural Background ## 6 Auchencorth Moss ACTH 55.8 -3.24 Rural Background ## 7 Ballymena Antrim Road BAAR 54.9 -6.27 Urban Traffic ## 8 Ballymena Ballykeel BALM 54.9 -6.25 Urban Background ## 9 Barnsley BARN 53.6 -1.48 Urban Background ## 10 Barnsley 12 BAR2 53.6 -1.49 Urban Background ## # … with 263 more rows Or return much more detailed data: aurn_meta &lt;- importMeta(source = &quot;aurn&quot;, all = TRUE) aurn_meta The example below uses sites on the AURN that measure NO2, but can easily be extended to the other data sources. To see how many sites measure NO2 in the AURN that are ‘urban traffic’: aurn_detailed &lt;- importMeta(source = &quot;aurn&quot;, all = TRUE) no2_sites &lt;- filter( aurn_detailed, variable == &quot;NO2&quot;, site_type == &quot;Urban Traffic&quot; ) nrow(no2_sites) ## [1] 89 To import data, you can use the different versions of importAURN. Some examples are below. ## import all pollutants from Marylebone Rd from 2000:2005 mary &lt;- importAURN(site = &quot;my1&quot;, year = 2000:2005) ## import nox, no2, o3 from Marylebone Road and Nottingham Centre for 2000 thedata &lt;- importAURN(site = c(&quot;my1&quot;, &quot;nott&quot;), year = 2000, pollutant = c(&quot;nox&quot;, &quot;no2&quot;, &quot;o3&quot;)) ## import over 30 years of Mace Head O3 data! o3 &lt;- importAURN(site = &quot;mh&quot;, year = 1987:2019) ## import hydrocarbon data from Marylebone Road hc &lt;- importAURN(site = &quot;my1&quot;, year = 2008, hc = TRUE) ## Import data from the AQE network (York data in this case) yk13 &lt;- importAQE(site = &quot;yk13&quot;, year = 2018) And to include basic meta data when importing air pollution data: kc1 &lt;- importAURN(site = &quot;kc1&quot;, year = 2018, meta = TRUE) glimpse(kc1) ## Rows: 8,760 ## Columns: 17 ## $ site &lt;chr&gt; &quot;London N. Kensington&quot;, &quot;London N. Kensington&quot;, &quot;London N. … ## $ code &lt;chr&gt; &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC… ## $ date &lt;dttm&gt; 2018-01-01 00:00:00, 2018-01-01 01:00:00, 2018-01-01 02:00… ## $ co &lt;dbl&gt; 0.114872, 0.111043, 0.112000, 0.100512, 0.091897, 0.100512,… ## $ nox &lt;dbl&gt; 8.32519, 8.89934, 9.41967, 9.36584, 7.21277, 7.64339, 10.17… ## $ no2 &lt;dbl&gt; 8.11153, 8.54325, 8.99235, 8.93852, 6.94570, 7.26948, 10.01… ## $ no &lt;dbl&gt; 0.13935, 0.23224, 0.27869, 0.27869, 0.17418, 0.24386, 0.104… ## $ o3 &lt;dbl&gt; 70.98040, 67.52118, 69.69982, 70.49810, 71.74542, 70.49810,… ## $ so2 &lt;dbl&gt; NA, 2.40953, 2.49812, 2.12606, 2.39181, 2.28551, 2.23236, 2… ## $ pm10 &lt;dbl&gt; 12.425, 7.375, 5.625, 3.200, 3.875, 5.050, 9.400, 12.400, 1… ## $ pm2.5 &lt;dbl&gt; 8.892, 4.363, 3.137, 1.792, 2.146, 2.618, 4.575, 6.109, 7.0… ## $ ws &lt;dbl&gt; 5.5, 5.0, 4.8, 4.8, 5.3, 5.3, 4.4, 3.0, 2.6, 1.6, 1.6, 1.1,… ## $ wd &lt;dbl&gt; 263.3, 256.4, 251.0, 246.8, 248.4, 248.0, 245.8, 239.5, 232… ## $ air_temp &lt;dbl&gt; 5.5, 5.1, 4.9, 4.7, 4.9, 5.0, 5.0, 4.6, 4.2, 3.7, 5.4, 5.7,… ## $ latitude &lt;dbl&gt; 51.52105, 51.52105, 51.52105, 51.52105, 51.52105, 51.52105,… ## $ longitude &lt;dbl&gt; -0.213492, -0.213492, -0.213492, -0.213492, -0.213492, -0.2… ## $ site_type &lt;chr&gt; &quot;Urban Background&quot;, &quot;Urban Background&quot;, &quot;Urban Background&quot;,… The latter is useful if you then want to plot the sites on a map, as shown below. By default, the function returns data where each pollutant is in a separate column. However, it is possible to return the data in a tidy format (column for pollutant name, column for value) by using the option to_narrow: my1 &lt;- importAURN(&quot;my1&quot;, year = 2018, to_narrow = TRUE) It is also possible to return information on whether the data have been ratifed or not using the option ratified (FALSE by default). So, add the option ratified = TRUE if you want this information. 3.3 Plot Sites on a Map In the example below the unique sites are selected from aurn_detailed because the site repeats the number of pollutants that are measured. Information is also collected for the map popups and then the map is plotted. library(leaflet) aurn_unique &lt;- distinct(aurn_detailed, site, .keep_all = TRUE) # information for map markers content &lt;- paste( paste( aurn_unique$site, paste(&quot;Code:&quot;, aurn_unique$code), paste(&quot;Start:&quot;, aurn_unique$start_date), paste(&quot;End:&quot;, aurn_unique$end_date), paste(&quot;Site Type:&quot;, aurn_unique$site_type), sep = &quot;&lt;br/&gt;&quot; ) ) # plot map leaflet(aurn_unique) %&gt;% addTiles() %&gt;% addMarkers(~ longitude, ~ latitude, popup = content, clusterOptions = markerClusterOptions()) "],
["sec-worldmet.html", "Section4 Access meteorological data 4.1 The worldmet package", " Section4 Access meteorological data 4.1 The worldmet package Most of the import functions described in Section 3 return basic modelled hourly meteorological data (wind speed, wind direction and surface temperature). These data are derived from the WRF model that Ricardo runs to provide the data. Alternatively it may be advantageous to use surface measurements. worldmet provides an easy way in which to access surface meteorological data from &gt;30,000 sites across the world (Carslaw 2020). The package accesses the NOAA webservers to download hourly data. See https://github.com/davidcarslaw/worldmet and https://www.ncdc.noaa.gov/isd for further information. Access to surface meteorological data is very useful in general but is especially useful when using openair and functions such as polarPlot. To install the package, type: install.packages(&quot;worldmet&quot;) There are two main functions in the package: getMeta and importNOAA. The former helps the user find meteorological sites by name, country and proximity to a location based on the latitude and longitude. getMeta will also return a code that can be supplied to importNOAA, which then imports the data. Probably the most common use of getMeta is to search around a location of interest based on its latitude and longitude. First we will load the worldmet package: library(worldmet) As an example, we will search for the 10 nearest sites to Dublin (latitude = 53.3, longitude = -6.3)1: getMeta(lat = 53.3, lon = -6.3, returnMap = TRUE) Figure 4.1: Map of returned area of interest. The user can interactively select a site of interest and find its code to import data. Note that it is just as easy to access all the site information at once because it is quick to use the map to select the site and its code i.e. getMeta() We can use the map that is produced to select a site of interest and import the data. For example, to import data for Dublin Airport and look at some of the data: dublin_met &lt;- importNOAA(code = &quot;039690-99999&quot;, year = 2019) # first few lines of data dublin_met ## # A tibble: 8,760 x 24 ## code station date latitude longitude elev ws wd ## &lt;fct&gt; &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0396… DUBLIN… 2019-01-01 00:00:00 53.4 -6.27 73.8 5.07 250. ## 2 0396… DUBLIN… 2019-01-01 01:00:00 53.4 -6.27 73.8 4.73 247. ## 3 0396… DUBLIN… 2019-01-01 02:00:00 53.4 -6.27 73.8 4.07 250. ## 4 0396… DUBLIN… 2019-01-01 03:00:00 53.4 -6.27 73.8 4.40 250. ## 5 0396… DUBLIN… 2019-01-01 04:00:00 53.4 -6.27 73.8 5.47 257. ## 6 0396… DUBLIN… 2019-01-01 05:00:00 53.4 -6.27 73.8 4.90 260 ## 7 0396… DUBLIN… 2019-01-01 06:00:00 53.4 -6.27 73.8 4.90 260 ## 8 0396… DUBLIN… 2019-01-01 07:00:00 53.4 -6.27 73.8 4.40 254. ## 9 0396… DUBLIN… 2019-01-01 08:00:00 53.4 -6.27 73.8 5.27 270 ## 10 0396… DUBLIN… 2019-01-01 09:00:00 53.4 -6.27 73.8 4.73 263. ## # … with 8,750 more rows, and 16 more variables: air_temp &lt;dbl&gt;, ## # atmos_pres &lt;dbl&gt;, visibility &lt;dbl&gt;, dew_point &lt;dbl&gt;, RH &lt;dbl&gt;, ## # ceil_hgt &lt;dbl&gt;, cl_1 &lt;dbl&gt;, cl_2 &lt;dbl&gt;, cl_3 &lt;dbl&gt;, cl &lt;dbl&gt;, ## # cl_1_height &lt;dbl&gt;, cl_2_height &lt;dbl&gt;, cl_3_height &lt;dbl&gt;, precip_12 &lt;dbl&gt;, ## # precip_6 &lt;dbl&gt;, precip &lt;dbl&gt; Plot a wind rose. windRose(dublin_met) References "],
["sec-windRose.html", "Section5 Wind and Pollution Roses 5.1 Example of use 5.2 Comparing two meteorological data sets", " Section5 Wind and Pollution Roses The wind rose is a very useful way of summarising meteorological data. It is particularly useful for showing how wind speed and wind direction conditions vary by year. The windRose function can plot wind roses in a variety of ways: summarising all available wind speed and wind direction data, plotting individual wind roses by year, and also by month. The latter is useful for considering how meteorological conditions vary by season. Data are summarised by direction, typically by 45 or 30\\(^\\circ\\) and by different wind speed categories. Typically, wind speeds are represented by different width ‘paddles’. The plots show the proportion (here represented as a percentage) of time that the wind is from a certain angle and wind speed range. The windRose function also calculates the percentage of ‘calms’ i.e. when the wind speed is zero. UK Met Office data assigns these periods to 0 degrees wind direction with valid northerly winds being assigned to 360\\(^\\circ\\). The windRose function will also correct for bias when wind directions are rounded to the nearest 10 degrees but are displayed at angles that 10 degrees is not exactly divisible into e.g. 22.5\\(^\\circ\\). When such data are binned, some angles i.e. N, E, S, W will comprise three intervals whereas others will comprise two, which can lead to significant bias. This issue and its solution is discussed by Droppo and Napier (2008) and Applequist (2012).2 openair uses a simple method to correct for the bias by globally rescaling the count in each wind direction bin by the number of directions it represents relative to the average. Thus, the primary four directions are each reduced by a factor of 0.75 and the remaining 12 directions are multiplied by 1.125. 5.1 Example of use First we load the packages: library(openair) library(tidyverse) The function is very simply called as shown for Figure 5.1. windRose(mydata) Figure 5.1: Use of windRose function to plot wind speed/direction frequencies. Wind speeds are split into the intervals shown by the scale in each panel. The grey circles show the % frequencies. Figure 5.2 highlights some interesting differences between the years. In 2000, for example, there were numerous occasions when the wind was from the SSW and 2003 clearly had more occasions when the wind was easterly. It can also be useful to use type = \"month\" to get an idea of how wind speed and direction vary seasonally. windRose(mydata, type = &quot;year&quot;, layout = c(4, 2)) Figure 5.2: Use of windRose function to plot wind speed/direction frequencies by year. Wind speeds are split into the intervals shown by the scale in each panel. The grey circles show the 10 and 20% frequencies. The type option is very flexible in openair and can be used to quickly consider the dependencies between variables. Section 22.2 describes the basis of this option in openair plot. As an example, consider the question: what are the meteorological conditions that control high and low concentrations of PM10? By setting type = \"pm10\", openair will split the PM10 concentrations into four quantiles i.e. roughly equal numbers of points in each level. The plot will then show four different wind roses for each quantile level, although the default number of levels can be set by the user — see ?cutData for more details. Figure 5.3 shows the results of setting type = \"pm10\". For the lowest concentrations of PM10 the wind direction is dominated by northerly winds, and relatively low wind speeds. By contrast, the highest concentrations (plot furthest right) are dominated by relatively strong winds from the south-west. It is therefore very easy to obtain a good idea about the conditions that tend to lead to high (or low) concentrations of a pollutant. Furthermore, the type option is available in almost all openair functions. windRose(mydata, type = &quot;pm10&quot;, layout = c(4, 1)) Figure 5.3: Wind rose for four different levels of PM10 concentration. The levels are defined as the four quantiles of PM10 concentration and the ranges are shown on each of the plot labels. A comparison of the effect that bias has can be seen by plotting the following. Note the prominent frequencies for W, E and N in particular that are due to the bias issue discussed by Applequist (2012). ## no bias correction windRose(mydata, angle = 22.5, bias.corr = FALSE) ## bias correction (the default) windRose(mydata, angle = 22.5) pollutionRose is a variant of windRose that is useful for considering pollutant concentrations by wind direction, or more specifically the percentage time the concentration is in a particular range. This type of approach can be very informative for air pollutant species, as demonstrated by Ronald Henry and co-authors in Henry et al. (2009). You can produce similar pollution roses using the pollutionRose function in recent versions of openair, e.g. as in Figure 5.4: pollutionRose(mydata, pollutant = &quot;nox&quot;) Figure 5.4: NOx pollution rose produced using pollutionRose and default pollutionRose settings. pollutionRose is wrapper for windRose. It simply replaces the wind speed data series in the supplied data set with another variable using the argument pollutant before passing that on to windRose. It also modifies breaks to estimate a sensible set of break points for that pollutant and uses a slightly different set of default options (key to right, wedge style plot) but otherwise handles arguments just like the parent windRose function. While Figure 5.4 indicates that higher NOx concentrations are also associated with the SW, conditioning allows you to be much informative. For example, conditioning by SO2 (Figure 5.5 demonstrates that higher NOx concentrations are associated with the SW and much of the higher SO2 concentrations. However, it also highlights a notable NOx contribution from the E, most apparent at highest SO2 concentrations that is obscured in Figure 5.4 by a relatively high NOx background (Figure 5.5. pollutionRose(mydata, pollutant = &quot;nox&quot;, type = &quot;so2&quot;, layout = c(4, 1)) Figure 5.5: NOx pollution rose conditioned by SO2 concentration. pollutionRose can also usefully be used to show which wind directions dominate the overall concentrations. By supplying the option statistic = \"prop.mean\" (proportion contribution to the mean), a good idea can be gained as to which wind directions contribute most to overall concentrations, as well as providing information on the different concentration levels. A simple plot is shown in Figure 5.6, which clearly shows the dominance of south-westerly winds controlling the overall mean NOx concentrations at this site. Indeed, almost half the overall NOx concentration is contributed by two wind sectors to the south-west. The polarFreq function can also show this sort of information, but the pollution rose is more effective because both length and colour are used to show the contribution. These plots are very useful for understanding which wind directions control the overall mean concentrations. pollutionRose(mydata, pollutant = &quot;nox&quot;, statistic = &quot;prop.mean&quot;) Figure 5.6: Pollution rose showing which wind directions contribute most to overall mean concentrations. It is sometimes useful to more clearly understand the contributions from wind directions that have low frequencies. For example, for a pollution rose of SO2 there are few occurrences of easterly winds making it difficult to see how the concentration intervals are made up. Try: pollutionRose(mydata, pollutant = &quot;so2&quot;, seg = 1) However, each wind sector can be normalised to give a probability between 0 and 1 to help show the variation within each wind sector more clearly. An example is shown in Figure 5.7 where for easterly winds it is now clearer that a greater proportion of the time the concentration is made up of high SO2 concentrations. In this plot each wind sector is scaled between 0 and 1. Also shown with a black like is an indication of the wind direction frequency to remind us that winds from the east occur with a low frequency. pollutionRose(mydata, pollutant = &quot;so2&quot;, normalise = TRUE, seg = 1) Figure 5.7: SO2 pollution rose produced using pollutionRose normalised by each wind sector. 5.2 Comparing two meteorological data sets The pollutionRose function is also useful for comparing two meteorological data sets. In this case a ‘reference’ data set is compared with a second data set. There are many reasons for doing so e.g. to see how one site compares with another or for meteorological model evaluation (more on that in later sections). In this case, ws and wd are considered to the the reference data sets with which a second set of wind speed and wind directions are to be compared (ws2 and wd2). The first set of values is subtracted from the second and the differences compared. If for example, wd2 was biased positive compared with wd then pollutionRose will show the bias in polar coordinates. In its default use, wind direction bias is colour-coded to show negative bias in one colour and positive bias in another. Note that this plot is mostly aimed at showing wind direction biases. It does also show the wind speed bias but only if there is a wind direction bias also. However, in most practical situations the plot should show both wind speed and direction biases together. An example of a situation where no wind speed bias would be shown would be for westerly winds where there was absolutely no bias between two data sets in terms of westerly wind direction but there was a difference in wind speed. Users should be aware of this limitation. In the next example, some artificial wind direction data are generated by adding a positive bias of 30~degrees with some normally distributed scatter. Also, the wind speed data are given a positive bias. The results are shown in Figure 5.8. The Figure clearly shows the mean positive bias in wind direction i.e. the direction is displaced from north (no bias). The colour scale also shows the extent to which wind speeds are biased i.e. there is a higher proportion of positively biased wind speeds shown by the red colour compared with the negatively biased shown in blue. Also shown in Figure 5.8 is the mean wind speed and direction bias as numerical values. Note that the type option can be used in Figure 5.8 e.g. type = \"month\" to split the analysis in useful ways. This is useful if one wanted to see whether a site or the output from a model was biased for different periods. For example, type = \"daylight\" would show whether there are biases between nighttime and daytime conditions. ## $example of comparing 2 met sites ## first we will make some new ws/wd data with a postive bias mydata &lt;- mutate(mydata, ws2 = ws + 2 * rnorm(nrow(mydata)) + 1, wd2 = wd + 30 * rnorm(nrow(mydata)) + 30) ## need to correct negative wd id &lt;- which(mydata$wd2 &lt; 0) mydata$wd2[id] &lt;- mydata$wd2[id] + 360 ## results show postive bias in wd and ws pollutionRose(mydata, ws = &quot;ws&quot;, wd = &quot;wd&quot;, ws2 = &quot;ws2&quot;, wd2 = &quot;wd2&quot;, grid.line = 5) Figure 5.8: Pollution rose showing the difference between two meteorological data sets. The colours are used to show whether data tend to be positively or negatively biased with respect to the reference data set. An example of using user-supplied breaks is shown in Figure 5.9. In this case six intervals are chosen including one that spans -0.5 to +0.5 that is useful to show wind speeds that do not change. ## add some wd bias to some nighttime hours id &lt;- which(as.numeric(format(mydata$date, &quot;%H&quot;)) %in% c(23, 1, 2, 3, 4, 5)) mydata$wd2[id] &lt;- mydata$wd[id] + 30 * rnorm(length(id)) + 120 id &lt;- which(mydata$wd2 &lt; 0) mydata$wd2[id] &lt;- mydata$wd2[id] + 360 pollutionRose(mydata, ws = &quot;ws&quot;, wd = &quot;wd&quot;, ws2 = &quot;ws2&quot;, wd2 = &quot;wd2&quot;, breaks = c(-11, -2, -1, -0.5, 0.5, 1, 2, 11), cols = c(&quot;dodgerblue4&quot;, &quot;white&quot;, &quot;firebrick&quot;), grid.line = 5, type = &quot;daylight&quot;) Figure 5.9: Pollution rose showing the difference between two meteorological data sets. The colours are used to show whether data tend to be positively or negatively biased with respect to the reference data set. In this case the example shows how to use user-defined breaks and split the data by day/night for a latitude assumed to be London. References "],
["sec-polarFreq.html", "Section6 Polar frequencies 6.1 Background and examples", " Section6 Polar frequencies 6.1 Background and examples This compactly shows the distribution of wind speeds and directions from meteorological measurements. It is similar to the traditional wind rose, but includes a number of enhancements to also show how concentrations of pollutants and other variables vary. It can summarise all available data, or show it by different time periods e.g. by year, month, day of the week. It can also consider a wide range of statistics. This section shows an example output and use, using our data frame mydata. The function is very simply run as shown in Figure 6.1. polarFreq(mydata) Figure 6.1: Use of polarFreq function to plot wind speed/directions. Each cell gives the total number of hours the wind was from that wind speed/direction in a particular year. The number of hours is coded as a colour scale shown to the right. The scale itself is non-linear to help show the overall distribution. The dashed circular grey lines show the wind speed scale. The date range covered by the data is shown in the strip. By setting type = \"year\", the frequencies are shown separately by year as shown in Figure 6.2, which shows that most of the time the wind is from a south-westerly direction with wind speeds most commonly between 2–6 m s-1. In 2000 there seemed to be a lot of conditions where the wind was from the south-west (leading to high pollutant concentrations at this location). The data for 2003 also stand out due to the relatively large number of occasions where the wind was from the east. Note the default colour scale, which has had a square-root transform applied, is used to provide a better visual distribution of the data. polarFreq(mydata, type = &quot;year&quot;) Figure 6.2: Use of polarFreq function to plot wind speed/directions by year. Each cell gives the total number of hours the wind was from that wind speed/direction in a particular year. The number of hours is coded as a colour scale shown to the right. The scale itself is non-linear to help show the overall distribution. The dashed circular grey lines show the wind speed scale. The polarFreq function can also usefully consider pollutant concentrations. Figure 6.3 shows the mean concentration of SO2 by wind speed and wind direction and clearly highlights that SO2 concentrations tend to be highest for easterly winds and for 1998 in particular. By weighting the concentrations by the frequency of occasions the wind is from a certain direction and has a certain speed, gives a better indication of the conditions that dominate the overall mean concentrations. Figure 6.4 shows the weighted mean concentration of SO2 and highlights that annual mean concentrations are dominated by south-westerly winds i.e. contributions from the road itself — and not by the fewer higher hours of concentrations when the wind is easterly. However, 2003 looks interesting because for that year, significant contributions to the overall mean were due to easterly wind conditions. These plots when applied to other locations can reveal some useful features about different sources. For example, it may be that the highest concentrations measured only occur infrequently, and the weighted mean plot can help show this. The code required to make Figure 6.3 and Figure 6.4 is shown below. polarFreq(mydata, pollutant = &quot;so2&quot;, type = &quot;year&quot;, statistic = &quot;mean&quot;, min.bin = 2) Figure 6.3: Use of polarFre} function to plot mean SO2 concentrations (ppb) by wind speed/directions and year. # weighted mean SO2 concentrations polarFreq(mydata, pollutant = &quot;so2&quot;, type = &quot;year&quot;, statistic = &quot;weighted.mean&quot;, min.bin = 2) Figure 6.4: Use of `polarFreq} function to plot weighted mean SO2 concentrations (ppb) by wind speed/directions and year Users are encouraged to try out other plot options. However, one potentially useful plot is to select a few specific years of user interest. For example, what if you just wanted to compare two years e.g. 2000 and 2003? This is easy to do by sending a subset of data to the function. Use here can be made of the openair selectByDate function (see Section 22.1. # wind rose for just 2000 and 2003 polarFreq(selectByDate(mydata, year = c(2000, 2003)), cols = &quot;jet&quot;, type = &quot;year&quot;) The polarFreq function can also be used to gain an idea about the wind directions that contribute most to the overall mean concentrations. As already shown, use of the option statistic = \"weighted.mean\" will show the percentage contribution by wind direction and wind speed bin. However, often it is unnecessary to consider different wind speed intervals. To make the plot more effective, a few options are set as shown in Figure 6.5. First, the statistic = \"weighted.mean\" is chosen to ensure that the plot shows concentrations weighted by their frequency of occurrence of wind direction. For this plot, we are mostly interested in just the contribution by wind direction and not wind speed. Setting the ws.int to be above the maximum wind speed in the data set ensures that all data are shown in one interval. Rather than having a square-root transform applied to the colour scale, we choose to have a linear scale by setting trans = FALSE. Finally, to show a ‘disk’, the offset is set at 80. Increasing the value of the offset will narrow the disk. While Figure 6.5 is useful — e.g. it clearly shows that concentrations of NOx at this site are totally dominated by south-westerly winds, the use of pollutionRose for this type of plot is more effective, as shown in Section 5. polarFreq(mydata, pollutant = &quot;nox&quot;, ws.int = 30, statistic = &quot;weighted.mean&quot;, offset = 80, trans = FALSE, col = &quot;heat&quot;) Figure 6.5: The percentage contribution to overall mean concentrations of NOx at Marylebone Road. "],
["sec-percentileRose.html", "Section7 Percentile roses 7.1 Introduction 7.2 Examples 7.3 Condtional probability function", " Section7 Percentile roses 7.1 Introduction percentileRose calculates percentile levels of a pollutant and plots them by wind direction. One or more percentile levels can be calculated and these are displayed as either filled areas or as lines. By default, the function plots percentile concentrations in 10 degree segments. Alternatively, the levels by wind direction are calculated using a cyclic smooth cubic spline. The wind directions are rounded to the nearest 10 degrees, consistent with surface data from the UK Met Office before a smooth is fitted. The percentileRose function compliments other similar functions including windRose, pollutionRose, polarFreq or polarPlot. It is most useful for showing the distribution of concentrations by wind direction and often can reveal different sources e.g. those that only affect high percentile concentrations such as a chimney stack. Similar to other functions, flexible conditioning is available through the `type} option. It is easy for example to consider multiple percentile values for a pollutant by season, year and so on. See examples below. 7.2 Examples The first example is a basic plot of percentiles of O3 shown in Figure 7.1. library(openair) percentileRose(mydata, pollutant = &quot;o3&quot;) Figure 7.1: A percentileRose plot of O3 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction. It shows for example that higher concentrations occur for northerly winds, as expected at this location. However, it also shows, for example the actual value of the 95th percentile O3 concentration. A slightly more interesting plot is shown in Figure 7.2 for SO2 concentrations. We also take the opportunity of changing some default options. In this case it can be clearly seen that the highest concentrations of SO2 are dominated by east and south-easterly winds; likely reflecting the influence of stack emissions in those directions. percentileRose(mydata, pollutant = &quot;so2&quot;, percentile = c(25, 50, 75, 90, 95, 99, 99.9), col = &quot;brewer1&quot;, key.position = &quot;right&quot;, smooth = TRUE) Figure 7.2: A percentileRose plot of SO2 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction. This plot sets some user-defined percentile levels to consider the higher SO2 concentrations, moves the key to the right and uses an alternative colour scheme. Lots more insight can be gained by considering how percentile values vary by other factors i.e. conditioning. For example, what do O3 concentrations look like split by season and whether it is daylight or nighttime hours? We can set the type to consider season and whether it is daylight or nighttime.3 This Figure reveals some interesting features. First, O3 concentrations are higher in the spring and summer and when the wind is from the north. O3 concentrations are higher on average at this site in spring due to the peak of northern hemispheric O3 and to some extent local production. This may also explain why O3 concentrations are somewhat higher at nighttime in spring compared with summer. Second, peak O3 concentrations are higher during daylight hours in summer when the wind is from the south-east. This will be due to more local (UK/European) production that is photochemically driven — and hence more important during daylight hours. percentileRose(mydata, type = c(&quot;season&quot;, &quot;daylight&quot;), pollutant = &quot;o3&quot;, col = &quot;Set3&quot;, mean.col = &quot;black&quot;) Figure 7.3: A percentileRose plot of O3 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction.The plot shows the variation by season and whether it is nighttime or daylight hours. 7.3 Condtional probability function The percentileRose function can also plot conditional probability functions (CPF) (Ashbaugh, Malm, and Sadeh 1985). The CPF is defined as CPF = \\(m_\\theta/n_\\theta\\), where \\(m_\\theta\\) is the number of samples in the wind sector \\(\\theta\\) with mixing ratios greater than some `high’ concentration, and \\(n_\\theta\\) is the total number of samples in the same wind sector. CPF analysis is very useful for showing which wind directions are dominated by high concentrations and give the probability of doing so. In openair, a CPF plot can be produced as shown in 7.4. Note that in these plots only one percentile is provided and the method must be supplied. In 7.4 it is clear that the high concentrations (greater than the 95th percentile of all observations) is dominated by easterly wind directions. There are very low conditional probabilities of these concentrations being experienced for other wind directions. percentileRose(mydata, poll=&quot;so2&quot;, percentile = 95, method = &quot;cpf&quot;, col = &quot;darkorange&quot;, smooth = TRUE) Figure 7.4: A CPF plot of SO2 concentrations at Marylebone Road. It is easy to plot several species on the same plot and this works well because they all have the same probability scale (i.e. 0 to 1). In the example below (not shown) it is easy to see for each pollutant the wind directions that dominate the contributions to the highest (95th percentile) concentrations. For example, the highest CO and concentrations are totally dominated by south/south-westerly winds and the probability of their being such high concentrations from other wind directions is effectively zero. percentileRose(mydata, pollutant = c(&quot;nox&quot;, &quot;so2&quot;, &quot;o3&quot;, &quot;co&quot;, &quot;pm10&quot;, &quot;pm25&quot;), percentile = 95, method = &quot;cpf&quot;, col = &quot;darkorange&quot;, layout = c(3, 2)) Figure 7.5: A CPF plot of many pollutants at Marylebone Road. References "],
["sec-polarPlot.html", "Section8 Polar plots 8.1 Introduction to polar plots 8.2 Examples 8.3 Nonparametric Wind Regression, NWR 8.4 Conditional Probability Function (CPF) plot 8.5 Pairwise statistics 8.6 Clustering 8.7 Polar plots on an interactive map", " Section8 Polar plots This Sections considers the flexible polarPlot function that is used to provide information about source characteritics. The clustering of polar plots is also considered using polarCluster. 8.1 Introduction to polar plots The polarPlot function plots a bivariate polar plot of concentrations. Concentrations are shown to vary by wind speed and wind direction. In many respects they are similar to the plots shown in Section 6 but include some additional enhancements. These enhancements include: plots are shown as a continuous surface and surfaces are calculated through modelling using smoothing techniques. These plots are not entirely new as others have considered the joint wind speed-direction dependence of concentrations (see for example Yu et al. (2004)). However, plotting the data in polar coordinates and for the purposes of source identification is new. Furthermore, the basic polar plot is since been enhanced in many ways as described below. Publications that describe or use the technique are Carslaw et al. (2006) and Westmoreland et al. (2007). These plots have proved to be useful for quickly gaining a graphical impression of potential sources influences at a location. The polarPlot function is described in more detail in Carslaw et al. (2006) where it is used to triangulate sources in an airport setting, Carslaw and Beevers (2013) where it is used with a clustering technique to identify features in a polar plot with similar characteristics and Uria-Tellaetxe and Carslaw (2014) where it is extended to include a conditional probability function to extract more information from the plots. For many, maybe most situations, increasing wind speed generally results in lower concentrations due to increased dilution through advection and increased mechanical turbulence. There are, however, many processes that can lead to interesting concentration-wind speed dependencies, and we will provide a more theoretical treatment of this in due course. However, below are a few reasons why concentrations can change with increasing wind speeds. Buoyant plumes from tall stacks can be brought down to ground-level resulting in high concentrations under high wind speed conditions. Particle suspension increases with increasing wind speeds e.g. PM10 from spoil heaps and the like. ‘Particle’ suspension can be important close to coastal areas where higher wind speeds generate more sea spray. The wind speed dependence of concentrations in a street canyon can be very complex: higher wind speeds do not always results in lower concentrations due to re-circulation. Bivariate polar plots are very good at revealing these complexities. As Carslaw et al. (2006) showed, aircraft emissions have an unusual wind speed dependence and this can help distinguish them from other sources. If several measurement sites are available, polar plots can be used to triangulate different sources. Concentrations of NO2 can increase with increasing wind speed — or at least not decline steeply due to increased mixing. This mixing can result in O3-rich air converting NO to NO2. The function has been developed to allow variables other than wind speed to be plotted with wind direction in polar coordinates. The key issue is that the other variable plotted against wind direction should be discriminating in some way. For example, temperature can help reveal high-level sources brought down to ground level in unstable atmospheric conditions, or show the effect a source emission dependent on temperature e.g. biogenic isoprene. For research applications where many more variables could be available, discriminating sources by these other variables could be very insightful. Bivariate polar plots are constructed in the following way. First, wind speed, wind direction and concentration data are partitioned into wind speed-direction bins and the mean concentration calculated for each bin. Testing on a wide range of data suggests that wind direction intervals at 5–10\\(^\\circ\\) and 40 wind speed intervals capture sufficient detail of the concentration distribution. The wind direction data typically available are generally rounded to 10\\(^\\circ\\) and for typical surface measurements. Binning the data in this way is not strictly necessary but acts as an effective data reduction technique without affecting the fidelity of the plot itself. Furthermore, because of the inherent wind direction variability in the atmosphere, data from several weeks, months or years typically used to construct a bivariate polar plot tends to be diffuse and does not vary abruptly with either wind direction or speed and more finely resolved bin sizes or working with the raw data directly does not yield more information. The wind components, \\(u\\) and \\(v\\) are calculated i.e. \\[\\begin{equation} u = \\overline{u} . sin\\left(\\frac{2\\pi}{\\theta}\\right), v = \\overline{u} . cos\\left(\\frac{2\\pi}{\\theta}\\right) \\tag{8.1} \\end{equation}\\] with \\(\\overline{u}\\) is the mean hourly wind speed and \\(\\theta\\) is the mean wind direction in degrees with 90\\(^\\circ\\) as being from the east. The calculations above provides a \\(u\\), \\(v\\), concentration (\\(C\\)) surface. While it would be possible to work with this surface data directly a better approach is to apply a model to the surface to describe the concentration as a function of the wind components \\(u\\) and \\(v\\) to extract real source features rather than noise. A flexible framework for fitting a surface is to use a Generalized Additive Model (GAM) e.g. Hastie and Tibshirani (1990), Wood (2006). GAMs are a useful modelling framework with respect to air pollution prediction because typically the relationships between variables are non-linear and variable interactions are important, both of which issues can be addressed in a GAM framework. GAMs can be expressed as shown in Equation (8.2): \\[\\begin{equation} \\sqrt{C_i} = \\beta_0 + \\sum_{j=1}^{n}s_j(x_{ij}) + e_i \\tag{8.2} \\end{equation}\\] where \\(C_i\\) is the ith pollutant concentration, \\(\\beta_0\\) is the overall mean of the response, \\(s_j(x_{ij})\\) is the smooth function of ith value of covariate \\(j\\), \\(n\\) is the total number of covariates, and \\(e_i\\) is the \\(i\\)th residual. Note that \\(C_i\\) is square-root transformed as the transformation generally produces better model diagnostics e.g. normally distributed residuals. The model chosen for the estimate of the concentration surface is given by @ref(eq:mod}. In this model the square root-transformed concentration is a smooth function of the bivariate wind components \\(u\\) and \\(v\\). Note that the smooth function used is isotropic because \\(u\\) and \\(v\\) are on the same scales. The isotropic smooth avoids the potential difficulty of smoothing two variables on different scales e.g. wind speed and direction, which introduces further complexities. \\[\\begin{equation} \\sqrt{C_i} = s(u, v) + e_i \\tag{8.3} \\end{equation}\\] 8.2 Examples We first use the function in its simplest form to make a polar plot of NOx. The code is very simple as shown in Figure 8.1. polarPlot(mydata, pollutant = &quot;nox&quot;) Figure 8.1: Default use of the polarPlot function applied to Marylebone Road NOx concentrations. This produces Figure 8.1. The scale is automatically set using whatever units the original data are in. This plot clearly shows highest NOx concentrations when the wind is from the south-west. Given that the monitor is on the south side of the street and the highest concentrations are recorded when the wind is blowing away from the monitor is strong evidence of street canyon recirculation. Figure 8.2 and Figure 8.3 shows polar plots using different defaults and for other pollutants. In the first (Figure 8.2, a different colour scheme is used and some adjustments are made to the key. In Figure 8.3, SO2 concentrations are shown. What is interesting about this plot compared with either Figure 8.2 or Figure 8.1 is that the concentration pattern is very different i.e. high concentrations with high wind speeds from the east. The most likely source of this SO2 are industrial sources to the east of London. The plot does still however show evidence of a source to the south-west, similar to the plot for NOx, which implies that road traffic sources of SO2 can also be detected. These plots often show interesting features at higher wind speeds. For these conditions there can be very few measurements and therefore greater uncertainty in the calculation of the surface. There are several ways in which this issue can be tackled. First, it is possible to avoid smoothing altogether and use polarFreq. The problem with this approach is that it is difficult to know how best to bin wind speed and direction: the choice of interval tends to be arbitrary. Second, the effect of setting a minimum number of measurements in each wind speed-direction bin can be examined through min.bin. It is possible that a single point at high wind speed conditions can strongly affect the surface prediction. Therefore, setting min.bin = 3, for example, will remove all wind speed-direction bins with fewer than 3 measurements before fitting the surface. This is a useful strategy for testing how sensitive the plotted surface is to the number of measurements available. While this is a useful strategy to get a feel for how the surface changes with different min.bin settings, it is still difficult to know how many points should be used as a minimum. Third, consider setting uncertainty = TRUE. This option will show the predicted surface together with upper and lower 95% confidence intervals, which take account of the frequency of measurements. The uncertainty approach ought to be the most robust and removes any arbitrary setting of other options. There is a close relationship between the amount of smoothing and the uncertainty: more smoothing will tend to reveal less detail and lower uncertainties in the fitted surface and vice-versa. The default however is to down-weight the bins with few data points when fitting a surface. Weights of 0.25, 0.5 and 0.75 are used for bins containing 1, 2 and 3 data points respectively. The advantage of this approach is that no data are actually removed (which is what happens when using min.bin). This approach should be robust in a very wide range of situations and is also similar to the approaches used when trying to locate sources when using back trajectories as described in Section 18. Users can ignore the automatic weighting by supplying the option weights = c(1, 1, 1). ## NOx plot polarPlot(mydata, pollutant = &quot;nox&quot;, col = &quot;jet&quot;, key.position = &quot;bottom&quot;, key.header = &quot;mean nox (ug/m3)&quot;, key.footer = NULL) Figure 8.2: Example plots using the polarPlot function with different options for the mean concentration of NOx. polarPlot(mydata, pollutant = &quot;so2&quot;) Figure 8.3: Example plots using the polarPlot function for the mean concentration of SO2. A very useful approach for understanding air pollution is to consider ratios of pollutants. One reason is that pollutant ratios can be largely independent of meteorological variation. In many circumstances it is possible to gain a lot of insight into sources if pollutant ratios are considered. First, it is necessary to calculate a ratio, which is easy in R. In this example we consider the ratio of SO2/NOx: library(tidyverse) mydata &lt;- mutate(mydata, ratio = so2 / nox) This makes a new variable called ratio. Sometimes it can be problematic if there are values equal to zero on the denominator, as is the case here. The mean and maximum value of the ratio is infinite, as shown by the Inf in the statistics below. Luckily, R can deal with infinity and the openair functions will remove these values before performing calculations. It is very simple therefore to calculate ratios. summary(mydata$ratio) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 0.018 0.024 Inf 0.034 Inf 11782 A polar plot of the SO2/NOx ratio is shown in Figure 8.4. The plot highlights some new features not seen before. First, to the north there seems to be evidence that the air tends to have a higher SO2/NOx ratio. Also, the source to the east has a higher SO2/NOx ratio compared with that when the wind is from the south-west i.e. dominated by road sources. It seems therefore that the easterly source(s), which are believed to be industrial sources have a different SO2/NOx ratio compared with road sources. This is a very simple analysis, but ratios can be used effectively in many functions and are particularly useful in the presence of high source complexity. polarPlot(mydata, pollutant = &quot;ratio&quot;, main = &quot;so2/nox ratio&quot;) Figure 8.4: Bivariate polar plot of the ratio of SO2/NOx. Sometimes when considering ratios it might be necessary to limit the values in some way; perhaps due to some unusually low value denominator data resulting in a few very high values for the ratio. This is easy to do with the dplyr filter command. The code below selects ratios less than 0.1. polarPlot(filter(mydata, ratio &lt; 0.1), pollutant = &quot;ratio&quot;) The uncertainties in the surface can be calculated by setting the option uncertainty = TRUE. The details are described above and here we show the example of SO2 concentrations (Figure 8.5. In general the uncertainties are higher at high wind speeds i.e. at the ‘fringes’ of a plot where there are fewer data. However, the magnitude depends on both the frequency and magnitude of the concentration close to the points of interest. The pattern of uncertainty is not always obvious and it can differ markedly for different pollutants. polarPlot(mydata, pollutant = &quot;so2&quot;, uncertainty = TRUE) Figure 8.5: Bivariate polar plot of SO2 concentrations at Marylebone Road. Three surfaces are shown: the central prediction (middle) and the lower and upper 95% estimated uncertainties. These plots help to show that in this particular case, some of the concentrations for strong easterly and south-easterly winds are rather uncertain. However, the central feature to the east remains, suggesting this feature is real and not an artifact of there being too few data. The polarPlot function can also produce plots dependent on another variable (see the type option). For example, the variation of SO2 concentrations at Marylebone Road by hour of the day in the code below. The function was called as shown in below, and in this case the minimum number of points in each wind speed/direction was set to 2. polarPlot(mydata, pollutant = &quot;so2&quot;, type = &quot;hour&quot;, min.bin = 2) This plot shows that concentrations of SO2 tend to be highest from the east (as also shown in Figure 8.3) and for hours in the morning. Together these plots can help better understand different source types. For example, does a source only seem to be present during weekdays, or winter months etc. In the case of type = \"hour\", the more obvious presence during the morning hours could be due to meteorological factors and this possibility should be investigated as well. In other settings where there are many sources that vary in their source emission and temporal characteristics, the polarPlot function should prove to be very useful. One issue to be aware of is the amount of data required to generate some of these plots; particularly the hourly plots. If only a relatively short time series is available there may not be sufficient information to produce useful plots. Whether this is important or not will depend on the specific circumstances e.g. the prevalence of wind speeds and directions from the direction of interest. When used to produce many plots (e.g. type = \"hour\"), the run time can be quite long. 8.3 Nonparametric Wind Regression, NWR An alternative approach to modelling the surface concentrations with a GAM is to use kernel smoothers, as described by Henry et al. (2009). In NWR, smoothing is achieved using nonparametric kernel smoothers that weight concentrations on a surface according to their proximity to defined wind speed and direction intervals. In the approach adopted in openair (which is not identical to Henry et al. (2009)), Gaussian smoothers are used for both wind direction and wind speed. Unlike the default GAM approach in openair, the NWR technique works directly with the raw (often hourly) data. It tends to provide similar results to openair but may have advantages in certain situations e.g. when there is insufficient data available to use a GAM. The width of the Gaussian kernels (\\(\\sigma\\)) is controlled by the options wd_spread and ws_spread. An example for SO2 concentrations is shown in Figure 8.6, which can be compared with Figure 8.3. polarPlot(mydata, pollutant = &quot;so2&quot;, statistic = &quot;nwr&quot;) Figure 8.6: polarPlot of SO2 concentrations at Marylebone Road based on the NWR approach. 8.4 Conditional Probability Function (CPF) plot The conditional probability functions (CPF) was described on in the context of the percentileRose function. The CPF was originally used to show the wind directions that dominate a (specified) high concentration of a pollutant; showing the probability of such concentrations occurring by wind direction Ashbaugh, Malm, and Sadeh (1985). However, these ideas can very usefully be applied to bivariate polar plots. In this case the CPF is defined as CPF = \\(m_{\\theta,j}/n_{\\theta,j}\\), where \\(m_{\\theta,j}\\) is the number of samples in the wind sector \\(\\theta\\) and wind speed interval \\(j\\) with mixing ratios greater than some ‘high’ concentration, and \\(n_{\\theta, j}\\) is the total number of samples in the same wind direction-speed interval. Note that \\(j\\) does not have to be wind speed but could be any numeric variable e.g. ambient temperature. CPF analysis is very useful for showing which wind direction, wind speed intervals are dominated by high concentrations and give the probability of doing so. A full explanation of the development and use of the bivariate case of the CPF is described in Uria-Tellaetxe and Carslaw (2014) where it is applied to monitoring data close to steelworks. polarPlot(mydata, pollutant = &quot;so2&quot;, statistic = &quot;cpf&quot;, percentile = 90) Figure 8.7: polarPlot of SO2 concentrations at Marylebone Road based on the CPF function. An example of a CPF polar plot is shown in Figure 8.7 for the 90th percentile concentration of SO2. This plot shows that for most wind speed-directions the probability of SO2 concentrations being greater than the 90th percentile is zero. The clearest areas where the probability is higher is to the east. Indeed, the plot now clearly reveals two potential sources of SO2, which are not as apparent in the ‘standard’ plot shown in Figure 8.3. Note that Figure 8.7 also gives the calculated percentile at the bottom of the plot (9.2 ppb in this case). Figure 8.7 can also be compared with the CPF plot based only on wind direction shown in Figure 7.4. While Figure 7.4 very clearly shows that easterly wind dominate high concentrations of SO2, Figure 8.7 provides additional valuable information by also considering wind speed, which in this case is able to discriminate between two sources (or groups of sources) to the east. The polar CPF plot is therefore potentially very useful for source identification and characterisation. It is, for example, worth also considering other percentile levels and other pollutants. For example, considering the 95th percentile for SO2 ‘removes’ one of the sources (the one at highest wind speed). This helps to show some maybe important differences between the sources that could easily have been missed. Similarly, considering other pollutants can help build up a good understanding of these sources. A CPF plot for NO2 at the 90th percentile shows the single dominance of the road source. However, a CPF plot at the 75th percentile level indicates source contributions from the east (likely tall stacks), which again are not as clear in the standard bivariate polar plot. Considering a range of percentile values can therefore help to build up a more complete understanding of source contributions. polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(0, 10)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(10, 20)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(20, 30)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(30, 40)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(40, 50)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(50, 60)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(60, 70)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(70, 80)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(80, 90)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(90, 100)) Figure 8.8: polarPlot of SO2 concentrations at Marylebone Road based on the CPF function for a range of percentile intervals from 0–10, 10–20, … , 90–100. However, even more useful information can be gained by considering intervals of percentiles e.g. 50–60, 60–70 etc. By considering intervals of percentiles it becomes clear that some sources only affect a limited percentile range. polarPlot can accept a percentile argument of length two e.g. percentile = c(80, 90). In this case concentrations in the range from the lower to upper percentiles will be considered. In Figure 8.8 for example, it is apparent that the road source to the south west is only important between the 60 to 90th percentiles. As mentioned previously, the chimney stacks to the east are important for the higher percentiles (90 to 100). What is interesting though is the emergence of what appears to be other sources at the lower percentile intervals. These potential sources are not apparent in Figure 8.3. The other interesting aspect is that it does seem that specific sources tend to be prominent for specific percentile ranges. If this characteristic is shown to be the case more generally, then CPF intervals could be a powerful way in which to identify many sources. Whether these particular sources are important or not is questionable and depends on the aims of the analysis. However, there is no reason to believe that the potential sources shown in the percentile ranges 0 to 50 are artefacts. They could for example be signals from more distant point sources whose plumes have diluted more over longer distances. Such sources would be ‘washed out’ in an ordinary polar plot. For a fuller example of this approach see Uria-Tellaetxe and Carslaw (2014). Note that it is easy to work out what the concentration intervals are for the percentiles shown in Figure 8.8: quantile(mydata$so2, probs = seq(0, 1, by = 0.1), na.rm = TRUE) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% ## 0.0000 1.0125 1.8825 2.5000 3.2500 4.0000 4.9375 5.9100 7.2375 9.2500 ## 100% ## 63.2050 To plot the Figures on one page it is necessary to make the plot objects first and then decide how to plot them. To plot the Figures in a particular layout see Section 2.8. 8.5 Pairwise statistics Grange, Lewis, and Carslaw (2016) further developed the capabilities of the polarPlot function by allowing pairwise statistics to be used. This method makes it possible to consider the relationship between two pollutants to be considered. The relationship between two pollutants often yields useful source apportionment information and when combined with the polarPlot function can provide enhanced information. The pairwise statistics that can be considered include: The Pearson or Spearman correlation coefficient, \\(r\\); The robust slope (gradient) resulting from a linear regressions between two pollutants. The quantile slope from a quantile regression applied to two variables with a quantile value of \\(tau\\). By default, the median slope (i.e. \\(tau\\) = 0.5) is used by the actual level can be set by the user. The calculation involves a weighted Pearson correlation coefficient, which is weighted by Gaussian kernels for wind direction and the radial variable (by default wind speed). More weight is assigned to values close to a wind speed-direction interval. Kernel weighting is used to ensure that all data are used rather than relying on the potentially small number of values in a wind speed-direction interval. The calculation involves a weighted statistics by Gaussian kernels for wind direction and the radial variable (by default wind speed). More weight is assigned to values close to a wind speed-direction interval. Kernel weighting is used to ensure that all data are used rather than relying on the potentially small number of values in a wind speed-direction interval. An example usage scenario is that measurements of metal concentrations are made close to a steelworks where there is interest in understanding the principal sources. While it is useful to consider the correlation between potentially many metal concentrations, the contention is that if the correlation is also considered as a function of wind speed and direction, improved information will be available of the types of sources contributing. For example, it may be that Fe and Mn are quite strongly correlated overall, but they tend to be most correlated under specific wind speed and direction ranges — suggesting a specific source origin. As an example of usage we will consider the relationship between PM2.5 and PM10 at the rural Harwell site in Oxfordshire. Additionally, we will use meteorological data from a nearby site rather than rely on modelled values that are provided in importAURN. library(worldmet) # to access met data library(tidyverse) har &lt;- importAURN(&quot;har&quot;, year = 2013) # import met data from nearby site (Benson) met &lt;- importNOAA(code = &quot;036580-99999&quot;, year = 2013) # merge AQ and met but don&#39;t use modelled ws and wd har &lt;- inner_join( select(har, -ws, -wd, -air_temp), met, by = &quot;date&quot; ) An example pairwise regression surface relationship relating PM2.5 and PM10 is shown in Figure 8.9. This plot reveals that almost all the PM10 is in the form of PM2.5 when the wind has an easterly component, which is attributed to the large secondary contribution likely dominated by ammonium nitrate. A simple scatter plot between PM2.5 and PM10 strongly suggests a 1:1 relationship and it is not obvious that there is a higher PM2.5/PM10 ratio when the wind is from the east. polarPlot(har, poll = c(&quot;pm2.5&quot;, &quot;pm10&quot;), statistic = &quot;robust_slope&quot;, col = &quot;jet&quot;, limits = c(0, 1), ws_spread = 1.5, wd_spread = 10) Figure 8.9: Use of the polarPlot function to investigate the linear regression slope between PM2.5 and PM10 at Harwell in 2013. In this case the robust slope is calculated. 8.6 Clustering The polarPlot function will often identify interesting features that would be useful to analyse further. It is possible to select areas of interest based only on a consideration of a plot. Such a selection could be based on wind direction and wind speed intervals for example e.g. subdata &lt;- filter(mydata, ws &gt; 3, wd &gt;= 180, wd &lt;= 270) which would select wind speeds &gt;3 m s-1 and wind directions from 180 to 270 degrees from mydata. That subset of data, subdata, could then be analysed using other functions. While this approach may be useful in many circumstances it is rather arbitrary. In fact, the choice of ‘interesting feature’ in the first place can even depend on the colour scale used, which is not very robust. Furthermore, many interesting patterns can be difficult to select and won’t always fall into convenient intervals of other variables such as wind speed and direction. A better approach is to use a method that can select group similar features together. One such approach is to use cluster analysis. openair uses k-means clustering as a way in which bivariate polar plot features can be identified and grouped. The main purpose of grouping data in this way is to identify records in the original time series data by cluster to enable post-processing to better understand potential source characteristics. The process of grouping data in k-means clustering proceeds as follows. First, \\(k\\) points are randomly chosen form the space represented by the objects that are being clustered into \\(k\\) groups. These points represent initial group centroids. Each object is assigned to the group that has the closest centroid. When all objects have been assigned, the positions of the \\(k\\) centroids is re-calculated. The previous two steps are repeated until the centroids no longer move. This produces a separation of the objects into groups from which the metric to be minimised can be calculated. Central to the idea of clustering data is the concept of distance i.e. some measure of similarity or dissimilarity between points. Clusters should be comprised of points separated by small distances relative to the distance between the clusters. Careful consideration is required to define the distance measure used because the effectiveness of clustering itself fundamentally depends on its choice. The similarity of concentrations shown in Figure 8.1 for example is determined by three variables: the \\(u\\) and \\(v\\) wind components and the concentration. All three variables are equally important in characterising the concentration-location information, but they exist on different scales i.e. a wind speed-direction measure and a concentration. Let \\(X = \\{x_i\\}, i = 1,\\ldots,n\\) be a set of \\(n\\) points to be clustered into \\(K\\) clusters, \\(C = \\{c_k, k = 1,\\ldots,K\\}\\). The basic k-means algorithm for \\(K\\) clusters is obtained by minimising: \\[\\begin{equation} \\sum_{k=1}^{K} \\sum_{x_i \\in c_k} || x_i - \\mu_k ||^2 \\tag{8.4} \\end{equation}\\] where \\(|| x_i - \\mu_k ||^2\\) is a chosen distance measure, \\(\\mu_k\\) is the mean of cluster \\(c_k\\). The distance measure is defined as the Euclidean distance: \\[\\begin{equation} d_{x, y} = \\left({\\sum_{j=1}^{J} (x_j - y_j) ^ 2}\\right)^{1/2} \\tag{8.5} \\end{equation}\\] Where x and y are two J-dimensional vectors, which have been standardized by subtracting the mean and dividing by the standard deviation. In the current case \\(J\\) is of length three i.e. the wind components \\(u\\) and \\(v\\) and the concentration \\(C\\), each of which is standardized e.g.: \\[\\begin{equation} x_j = \\left(\\frac{x_j - \\overline{x}}{\\sigma_x}\\right) \\tag{8.6} \\end{equation}\\] Standardization is necessary because the wind components \\(u\\) and \\(v\\) are on different scales to the concentration. In principle, more weight could be given to the concentration rather than the \\(u\\) and \\(v\\) components, although this would tend to identify clusters with similar concentrations but different source origins. polarCluster can be thought of as the ‘local’ version of clustering of back trajectories. Rather than using air mass origins, wind speed, wind direction and concentration are used to group similar conditions together. Section 18.6 provides the details of clustering back trajectories in openair. A fuller description of the clustering approach is described in Carslaw and Beevers (2013). The use of the polarCluster is very similar to the use of all openair functions. While there are many techniques available to try and find the optimum number of clusters, it is difficult for these approaches to work in a consistent way for identifying features in bivariate polar plots. For this reason it is best to consider a range of solutions that covers a number of clusters. Cluster analysis is computationally intensive and the polarCluster function can take a comparatively long time to run. The basic idea is to calculate the solution to several cluster levels and then choose one that offers the most appropriate solution for post-processing. The example given below is for concentrations of SO2, shown in Figure 8.3 and the aim is to identify features in that plot. A range of numbers of clusters will be calculated — in this case from two to ten. polarCluster(mydata, pollutant=&quot;so2&quot;, n.clusters=2:10, cols= &quot;Set2&quot;) Figure 8.10: Use of the polarCluster function applied to SO2 concentrations at Marylebone Road. In this case 2 to 10 clusters have been chosen. results &lt;- polarCluster(mydata, pollutant=&quot;so2&quot;, n.clusters = 8, cols = &quot;Set2&quot;) Figure 8.11: Use of the polarCluster function applied to SO2 concentrations at Marylebone Road. In this case 8 clusters have been chosen. The real benefit of polarCluster is being able to identify clusters in the original data frame. To do this, the results from the analysis must be read into a new variable, as in Figure 8.11, where the results are read into a data frame `results}. Now it is possible to use this new information. In the 8-cluster solution to Figure 8.11, cluster~6 seems to capture the elevated SO2 concentrations to the east well (see Figure 8.3 for comparison), while cluster 5 will strongly represent the road contribution. The results are here: head(results[[&quot;data&quot;]]) ## .id date ws wd nox no2 o3 pm10 so2 co pm25 ## 1 1 1998-01-01 00:00:00 0.60 280 285 39 1 29 4.7225 3.3725 NA ## 2 10 1998-01-01 09:00:00 3.96 170 113 39 2 12 2.9225 1.2050 NA ## 3 100 1998-01-05 03:00:00 8.76 240 53 24 16 12 1.5950 0.5400 NA ## 4 1000 1998-02-11 15:00:00 7.20 230 372 92 2 49 8.5950 3.7175 NA ## 5 10000 1999-02-21 15:00:00 6.36 290 71 32 20 10 2.6225 1.0500 8 ## 6 10001 1999-02-21 16:00:00 7.68 290 85 39 16 12 2.8700 1.4750 9 ## ws2 wd2 ratio cluster ## 1 -0.244089 312.9327 0.01657018 4 ## 2 5.366733 185.3829 0.02586283 5 ## 3 10.971351 382.0663 0.03009434 4 ## 4 9.839028 242.3029 0.02310484 4 ## 5 6.776110 344.4104 0.03693662 7 ## 6 10.861975 303.2430 0.03376471 7 Note that there is an additional column cluster that gives the cluster a particular row belongs to and that this is a character variable. It might be easier to read these results into a new data frame: results &lt;- results[[&quot;data&quot;]] It is easy to find out how many points are in each cluster: table(results[[&quot;cluster&quot;]]) ## ## 1 2 3 4 5 6 7 8 ## 206 412 160 24133 16049 2590 7925 2832 Now other openair analysis functions can be used to analyse the results. For example, to consider the temporal variations by cluster: timeVariation(results, pollutant = &quot;so2&quot;, group = &quot;cluster&quot;, key.columns = 4, col = &quot;Set2&quot;, ci = FALSE, lwd = 3) Figure 8.12: Temporal variation in SO2 split by cluster. Or if we just want to plot a couple of clusters (5 and 6) using the same colours as in Figure 8.11: timeVariation(filter(results, cluster %in% c(&quot;5&quot;, &quot;6&quot;)), pollutant = &quot;so2&quot;, group = &quot;cluster&quot;, col = openColours(&quot;Set2&quot;, 8)[5:6], lwd = 3) Figure 8.13: Temporal variation in SO2 split by cluster — showing only two clusters. polarCluster will work on any surface that can be plotted by polarPlot e.g. the radial variable does not have to be wind speed but could be another variable such as temperature. While it is not always possible for polarCluster to identify all features in a surface it certainly makes it easier to post-process polarPlots using other openair functions or indeed other analyses altogether. Another useful way of understanding the clusters is to use the timeProp function, which can display a time series as a bar chart split by a categorical variable (in this case the cluster). In this case it is useful to plot the time series of SO2 and show how much of the concentration is contributed to by each cluster. Such a plot is shown in Figure 8.14. It is now easy to see for example that many of the peaks in SO2 are associated with cluster 6 (power station sources from the east), seen in Figure 8.11. Cluster~6 is particularly prominent during springtime, but those sources also make important contributions through the whole year. timeProp(selectByDate(results, year = 2003), pollutant = &quot;so2&quot;, avg.time = &quot;day&quot;, proportion= &quot;cluster&quot;, col = &quot;Set2&quot;, key.position = &quot;top&quot;, key.columns = 8, date.breaks = 10, ylab = &quot;so2 (ug/m3)&quot;) Figure 8.14: Temporal variation in daily SO2 concentration at the Marylebone Road site show by contribution of each cluster for 2003. 8.7 Polar plots on an interactive map A developmental R package called [openairmaps{.pkg} has been developed to plot bivariate polar plots on an interactive leaflet map. This package is not on CRAN but can be installed from GitHub: library(devtools) install_github(&quot;davidcarslaw/openairmaps&quot;) The function polarMap requires information on the site (anme or code), latitude and longitude in addition to the usual information required by the polarPlot function. The package comes with some example data (polar_data) that can be used as a template for using different data. The function can plot any type of polar plot e.g. with different options for statistic. First, load the package and check out the data format. library(openairmaps) glimpse(polar_data) ## Rows: 35,040 ## Columns: 11 ## $ date &lt;dttm&gt; 2009-01-01 00:00:00, 2009-01-01 01:00:00, 2009-01-01 02:0… ## $ nox &lt;dbl&gt; 130, 63, 76, 69, 61, 61, 61, 92, 57, 50, 59, 52, 61, 92, 6… ## $ no2 &lt;dbl&gt; 48, 32, 36, 34, 31, 31, 29, 42, 32, 32, 34, 32, 34, 46, 38… ## $ site &lt;chr&gt; &quot;London Marylebone Road&quot;, &quot;London Marylebone Road&quot;, &quot;Londo… ## $ latitude &lt;dbl&gt; 51.52253, 51.52253, 51.52253, 51.52253, 51.52253, 51.52253… ## $ longitude &lt;dbl&gt; -0.154611, -0.154611, -0.154611, -0.154611, -0.154611, -0.… ## $ site.type &lt;chr&gt; &quot;Urban traffic&quot;, &quot;Urban traffic&quot;, &quot;Urban traffic&quot;, &quot;Urban … ## $ wd &lt;dbl&gt; 58.92536, 74.46675, 30.00000, 45.00000, 70.00000, 46.63627… ## $ ws &lt;dbl&gt; 2.066667, 1.900000, 1.550000, 2.100000, 1.500000, 2.100000… ## $ visibility &lt;dbl&gt; 5000.000, 4933.333, 5000.000, 4900.000, 5000.000, 6000.000… ## $ air_temp &lt;dbl&gt; 0.8666667, 0.8666667, 0.8000000, 0.8500000, 0.8666667, 0.9… Plot an interactive map. polarMap(polar_data, latitude = &quot;latitude&quot;, longitude = &quot;longitude&quot;, type = &quot;site&quot;) Or with a different base map (there are many!). See the help file for polarMap on how to find out about and use different base maps. Note that if you are using the importAURN family of functions it is useful to add the option meta = TRUE, which will return the latitude and logitude of the site together with the air quality data. polarMap(polar_data, latitude = &quot;latitude&quot;, longitude = &quot;longitude&quot;, type = &quot;site&quot;, provider = &quot;Stamen.Toner&quot;) References "],
["sec-polarAnnulus.html", "Section9 Polar annulus 9.1 Purpose 9.2 Example of use", " Section9 Polar annulus 9.1 Purpose The polarAnnulus function provides a way in which to consider the temporal aspects of a pollutant concentration by wind direction. This is another means of visualising diurnal, day of week, seasonal and trend variations. Plotting as an annulus, rather than a circle avoids to some extent the difficulty in interpreting values close to the origin. These plots have the capacity to display potentially important information regarding sources; particularly if more than one pollutant is available. 9.2 Example of use We apply the four variations of the polarAnnulus plot to PM10 concentrations at Marylebone Road. Figure 9.1 shows the different temporal components. Similar to other analyses for PM10, the trend plot show that concentrations are dominated by southerly winds and there is little overall change in concentrations from 1998 to 2005, as shown by the red colouring over the period. The seasonal plot shows that February/March is important for easterly winds, while the summer/late summer period is more important for southerly and south-westerly winds. The day of the week plot clearly shows concentrations to be elevated for during weekdays but not weekends — for all wind directions. Finally, the diurnal plot highlights that higher concentrations are observed from 6 am to 6 pm. Interestingly, the plot for NOx and CO (not shown, but easily produced) did not show such a strong contribution for south-easterly winds. This raises the question whether the higher particle concentrations seen for these wind directions are dominated by different sources (i.e. not the road itself). One explanation is that during easterly flow, concentrations are strongly affected by long-range transport. However, as shown in the diurnal plot in Figure 9.1, the contribution from the south-east also has a sharply defined profile — showing very low concentrations at night, similar to the likely contribution from the road. This type of profile might not be expected from a long-range source where emissions are well-mixed and secondary particle formation has had time to occur. The same is also true for the day of the week plot, where there is little evidence of ‘smeared-out’ long-range transport sources. These findings may suggest a different, local source of PM10 that is not the road itself. Clearly, a more detailed analysis would be required to confirm the patterns shown; but it does highlight the benefit of being able to analyse data in different ways. data(mydata) polarAnnulus(mydata, poll = &quot;pm10&quot;, period = &quot;trend&quot;, main = &quot;Trend&quot;) polarAnnulus(mydata, poll = &quot;pm10&quot;, period = &quot;season&quot;, main = &quot;Season&quot;) polarAnnulus(mydata, poll = &quot;pm10&quot;, period = &quot;weekday&quot;, main = &quot;Weekday&quot;) polarAnnulus(mydata, poll = &quot;pm10&quot;,period = &quot;hour&quot;, main = &quot;Hour&quot;) Figure 9.1: Examples of the polarAnnulus function applied to Marylebone Road. Where there is interest in considering the wind direction dependence of concentrations, it can be worth filtering for wind speeds. At low wind speed with wind direction becomes highly variable (and is often associated with high pollutant concentrations). Therefore, for some situations it might be worth considering removing the very low wind speeds. The code below provides two ways of doing this using the dplyr filter function. The first selects data where the wind speed is &gt;2 m s-1. The second part shows how to select wind speeds greater than the 10th percentile, using the quantile function. The latter way of selecting is quite useful, because it is known how much data are selected i.e. in this case 90%. It is worth experimenting with different values because it is also important not to lose information by ignoring wind speeds that provide useful information. ## wind speed &gt;2 polarAnnulus(filter(mydata, ws &gt; 2), poll=&quot;pm10&quot;, type = &quot;hour&quot;) ## wind speed &gt; 10th percentile polarAnnulus(subset(mydata, ws &gt; quantile(ws, probs = 0.1, na.rm = TRUE)), poll=&quot;pm10&quot;, type = &quot;hour&quot;) "],
["sec-timePlot.html", "Section10 Time series plots 10.1 Background 10.2 Examples of time series plotting", " Section10 Time series plots 10.1 Background The timePlot function is designed to quickly plot time series of data, perhaps for several pollutants or variables. This is, or should be, a very common task in the analysis of air pollution. In doing so, it is helpful to be able to plot several pollutants at the same time (and maybe other variables) and quickly choose the time periods of interest. It will plot time series of type Date and hourly and high time resolution data. With packages such as ggplot2 it is very easy to plot time series. However, there are a few enhancements in timePlot such as flexible time-averaging and adding annotations for wind speed and direction that make it useful in some situations. The function offers fine control over many of the plot settings such as line type, colour and width. If more than one pollutant is selected, then the time series are shown compactly in different panels with different scales. Sometimes it is useful to get and idea of whether different variables ‘go up and down’ together. Such comparisons in timePlot are made easy by setting group = TRUE, and maybe also normalise = \"mean\". The latter setting divides each variable by its mean value, thus enabling several variables to be plotted together using the same scale. The normalise option will also take a date as a string (in British format dd/mm/YYYY), in which case all data are normalise to equal 100 at that time. Normalising data like this makes it easy to compare time series on different scales e.g. emissions and ambient measurements. timePlot works very well in conjunction with selectByDate, which makes it easy to select specific time series intervals. See (Section 22.1) for examples of how to select parts of a data frame based on the date. Another useful feature of timePlot is the ability to average the data in several ways. This makes it easy, for example, to plot daily or monthly means from hourly data, or hourly means from 15-minute data. See the option avg.time for more details and (Section 22.5) where a full description of time averaging of data frames is given. 10.2 Examples of time series plotting A full set of examples is shown in the help pages — see ?timePlot for details. At the basic level, concentrations are shown using a simple call e.g. to plot time series of NOx and O3 in separate panels with their own scales. timePlot(mydata, pollutant = c(&quot;nox&quot;, &quot;o3&quot;), y.relation = &quot;free&quot;) Often it is necessary to only consider part of a time series and using the openair function selectByDate makes it easy to do this. Some examples are shown below. To plot data only for 2003: timePlot(selectByDate(mydata, year = 2003), pollutant = c(&quot;nox&quot;, &quot;o3&quot;), y.relation = &quot;free&quot;) Plots for several pollutants for August 2003, are shown in Figure 10.1. library(openair) library(tidyverse) timePlot(selectByDate(mydata, year = 2003, month = &quot;aug&quot;), pollutant = c(&quot;nox&quot;, &quot;o3&quot;, &quot;pm25&quot;, &quot;pm10&quot;, &quot;ws&quot;), y.relation = &quot;free&quot;) Figure 10.1: Time series for several variables using the timePlot and the selectByDate functions. The data shown are for August 2003. Some other examples (not plotted) are: ## plot monthly means of ozone and no2 timePlot(mydata, pollutant = c(&quot;o3&quot;, &quot;no2&quot;), avg.time = &quot;month&quot;, y.relation = &quot;free&quot;) ## plor 95th percentile monthly concentrations timePlot(mydata, pollutant = c(&quot;o3&quot;, &quot;no2&quot;), avg.time = &quot;month&quot;, statistic = &quot;percentile&quot;, percentile = 95, y.relation = &quot;free&quot;) ## plot the number of valid records in each 2-week period timePlot(mydata, pollutant = c(&quot;o3&quot;, &quot;no2&quot;), avg.time = &quot;2 week&quot;, statistic = &quot;frequency&quot;, y.relation = &quot;free&quot;) An example of normalising data is shown in Figure 10.2. In this plot we have: Averaged the data to annual means; Chosen to normalise to the beginning of 2008; Set the line width to 4 and the line type to 1 (continuous line); Chosen to group the data in one panel. Figure 10.2 shows that concentrations of NO2 and O3 have increased over the period 1998–2005; SO2 and CO have shown the greatest reductions (by about 60%), whereas NOx concentrations have decreased by about 20%. timePlot(mydata, pollutant = c(&quot;nox&quot;, &quot;no2&quot;, &quot;co&quot;, &quot;so2&quot;, &quot;pm10&quot;), avg.time = &quot;year&quot;, normalise = &quot;1/1/1998&quot;, lwd = 4, lty = 1, group = TRUE, ylim = c(0, 120)) Figure 10.2: An example of normalising time series data to fix values to equal 100 at the beginning of 1998. Another example is grouping pollutants from several sites on one plot. It is easy to import data from several sites and to plot the data in separate panels e.g. ## import data from 3 sites aq &lt;- importAURN(site = c(&quot;kc1&quot;, &quot;my1&quot;, &quot;nott&quot;), year = 2005:2010) ## plot it timePlot(aq, pollutant = &quot;nox&quot;, type = &quot;site&quot;, avg.time = &quot;month&quot;) Figure 10.3: Plot of NOx xconcentrations from three sites in separate panels. Using the code above it is also possible to include several species. But what if we wanted to plot NOx concentrations across all sites in one panel? An example of how to do this is shown below. Note, in order to make referring to the columns easier, we will drop the full (long) site name and use the site code instead. ## first drop site name aq &lt;- select(aq, -site) ## now reshape the data using the tidyr package aq &lt;- pivot_wider(aq, id_cols = date, names_from = code, values_from = co:air_temp) names(aq) ## [1] &quot;date&quot; &quot;co_KC1&quot; &quot;co_MY1&quot; &quot;co_NOTT&quot; ## [5] &quot;nox_KC1&quot; &quot;nox_MY1&quot; &quot;nox_NOTT&quot; &quot;no2_KC1&quot; ## [9] &quot;no2_MY1&quot; &quot;no2_NOTT&quot; &quot;no_KC1&quot; &quot;no_MY1&quot; ## [13] &quot;no_NOTT&quot; &quot;o3_KC1&quot; &quot;o3_MY1&quot; &quot;o3_NOTT&quot; ## [17] &quot;so2_KC1&quot; &quot;so2_MY1&quot; &quot;so2_NOTT&quot; &quot;pm10_KC1&quot; ## [21] &quot;pm10_MY1&quot; &quot;pm10_NOTT&quot; &quot;pm2.5_KC1&quot; &quot;pm2.5_MY1&quot; ## [25] &quot;pm2.5_NOTT&quot; &quot;v10_KC1&quot; &quot;v10_MY1&quot; &quot;v10_NOTT&quot; ## [29] &quot;v2.5_KC1&quot; &quot;v2.5_MY1&quot; &quot;v2.5_NOTT&quot; &quot;nv10_KC1&quot; ## [33] &quot;nv10_MY1&quot; &quot;nv10_NOTT&quot; &quot;nv2.5_KC1&quot; &quot;nv2.5_MY1&quot; ## [37] &quot;nv2.5_NOTT&quot; &quot;ws_KC1&quot; &quot;ws_MY1&quot; &quot;ws_NOTT&quot; ## [41] &quot;wd_KC1&quot; &quot;wd_MY1&quot; &quot;wd_NOTT&quot; &quot;air_temp_KC1&quot; ## [45] &quot;air_temp_MY1&quot; &quot;air_temp_NOTT&quot; The final step will make columns of each site/pollutant combination e.g. nox_KC1, pm10_KC1 and so on. It is then easy to use any of these names to make the plot (with a few plotting option enhancements): timePlot(aq, pollutant = c(&quot;nox_KC1&quot;, &quot;nox_MY1&quot;, &quot;nox_NOTT&quot;), avg.time = &quot;month&quot;, group = TRUE, lty = 1, lwd = c(1, 3, 5), ylab = &quot;nox (ug/m3)&quot; ) Figure 10.4: Plot of NOx xconcentrations from three sites grouped in a single plot. An alternative way of selecting all columns containing the character ‘nox’ is to use the grep command. For example: timePlot(thedata, pollutant = names(thedata)[grep(pattern = &quot;nox&quot;, names(thedata))], avg.time = &quot;month&quot;, group = TRUE) If wind speed ws and wind direction wd are available they can be used in plots and shown as ‘wind vectors’. Plotting data in this way conveys more information in an easy-to-understand way, which works best for relatively short time periods e.g. a pollution episode lasting a few days. As an example Figure 10.5 shows the first 48 hours of NOx and NO2 data with wind arrows shown. The arrows are controlled by a list of option that control the length, shape and colour of the arrows. The maximum length of the arrow plotted is a fraction of the plot dimension with the longest arrow being scale of the plot x-y dimension. Note, if the plot size is adjusted manually by the user it should be re-plotted to ensure the correct wind angle. The list may contain other options to panel.arrows in the lattice package. Other useful options include length, which controls the length of the arrow head and angle, which controls the angle of the arrow head. Wind vector arrows can also be used with the scatterPlot function. timePlot(head(mydata, 48), pollutant = c(&quot;nox&quot;, &quot;no2&quot;), windflow = list(scale = 0.1, lwd = 2, col = &quot;turquoise4&quot;), lwd = 3, group = FALSE, ylab = &quot;concentration (ug/m3)&quot;) Figure 10.5: An example of using the windflow option in timePlot. "],
["sec-timeVariation.html", "Section11 Temporal variations 11.1 Purpose 11.2 Applications 11.3 Output", " Section11 Temporal variations 11.1 Purpose In air pollution, the variation of a pollutant by time of day and day of week can reveal useful information concerning the likely sources. For example, road vehicle emissions tend to follow very regular patterns both on a daily and weekly basis. By contrast some industrial emissions or pollutants from natural sources (e.g. sea salt aerosol) may well have very different patterns. The timeVariation function produces four plots: day of the week variation, mean hour of day variation and a combined hour of day – day of week plot and a monthly plot. Also shown on the plots is the 95% confidence interval in the mean. These uncertainty limits can be helpful when trying to determine whether one candidate source is different from another. The uncertainty intervals are calculated through bootstrap re-sampling, which will provide better estimates than the application of assumptions based on normality, particularly when there are few data available. The function can consider one or two input variables. In addition, there is the option of ‘normalising’ concentrations (or other quantities). Normalising is very useful for comparing the patterns of two different pollutants, which often cover very different ranges in concentration. Normalising is achieved by dividing the concentration of a pollutant by its mean value. Note also that any other variables besides pollutant concentrations can be considered e.g. meteorological or traffic data. There is also an option difference which is very useful for considering the difference in two time series and how they vary over different temporal resolutions. Again, bootstrap re-sampling methods are used to estimate the uncertainty of the difference in two means. Care has been taken to ensure that wind direction (wd) is vector-averaged. Less obvious though is the uncertainty in wind direction. A pragmatic approach has been adopted here that considers how wind direction changes. For example, consider the following wind directions: 10, 10, 10, 180, 180, 180\\(^\\circ\\) The standard deviation of these numbers is 93\\(^\\circ\\). However, what actually occurs is the wind direction is constant at 10\\(^\\circ\\) then switches to 180\\(^\\circ\\). In terms of changes there is a sequence of numbers: 0, 0, 170, 0, 0 with a standard deviation of 76\\(^\\circ\\). We use the latter method as a basis of calculating the 95% confidence intervals in the mean. There are also problems with simple averaging—for example, what is the average of 20 and 200\\(^\\circ\\). It can’t be known. In some situations where the wind direction is bi-modal with differences around 180\\(^\\circ\\), the mean can be ‘unstable’. For example, wind that is funnelled along a valley forcing it to be either easterly or westerly. Consider for example the mean of 0\\(^\\circ\\) and 179\\(^\\circ\\) (89.5\\(^\\circ\\)), but a small change in wind direction to 181\\(^\\circ\\) gives a mean of 270.5\\(^\\circ\\). Some care should be exercised therefore when averaging wind direction. It is always a good idea to use thewindRosefunction with type set to ‘month’ or ‘hour’. The timeVariation function is probably one of the most useful functions that can be used for the analysis of air pollution. Here are a few uses/advantages: Variations in time are one of the most useful ways of characterising air pollution for a very wide range of pollutants including local urban pollutants and tropospheric background concentrations of ozone and the like. The function works well in conjunction with other functions such as polarPlot (see Section 8), where the latter may identify conditions of interest (say a wind speed/direction range). By sub-setting for those conditions in timeVariation the temporal characteristics of a particular source could be characterised and perhaps contrasted with another subset of conditions. The function can be used to compare a wide range of variables, if available. Suggestions include meteorological e.g. boundary layer height and traffic flows. The function can be used for comparing pollutants over different sites. The function can be used to compare one part of a time series with another. This is often a very powerful thing to do, particularly if concentrations are normalised. For example, there is often interest in knowing how diurnal/weekday/seasonal patterns vary with time. If a pollutant showed signs of an increase in recent years, then splitting the data set and comparing each part together can provide information on what is driving the change. Is there, for example, evidence that morning rush hour concentrations have become more important, or Sundays have become relatively more important? An example is given below using the splitByDate function. timeVariation can be used to consider the differences between two time series, which will have multiple benefits. For example, for model evaluation it can be very revealing to consider the difference between observations and modelled values over different time scales. Considering such differences can help reveal the character and some reasons for why a model departs from reality. 11.2 Applications We apply the timeVariation function to PM10 concentrations and take the opportunity to filter the data to maximise the signal from the road. The polarPlot function described in Section (Section 8) is very useful in this respect in highlighting the conditions under which different sources have their greatest impact. A subset of data is used filtering for wind speeds &gt; 3 m s-1 and wind directions from 100–270 degrees. The code used is: The results are shown in Figure 11.1. The plot shown at the top-left shows the diurnal variation of concentrations for all days. It shows for example that PM10 concentrations tend to peak around 9 am. The shading shows the 95% confidence intervals of the mean. The plot at the top-right shows how PM10 concentrations vary by day of the week. Here there is strong evidence that PM10 is much lower at the weekends and that there is a significant difference compared with weekdays. It also shows that concentrations tend to increase during the weekdays. Finally, the plot at the bottom shows both sets of information together to provide an overview of how concentrations vary. Note that the plot need not just consider pollutant concentrations. Other useful variables (if available) are meteorological and traffic flow or speed data. Often, the combination of several sets of data can be very revealing. The filter function is extremely useful in this respect. For example, if it were believed that a source had an effect under specific conditions; they can be isolated with the filter function. It is also useful if it is suspected that two or more sources are important that they can be isolated to some degree and compared. This is where the uncertainty intervals help — they provide an indication whether the behaviour of one source differs significantly from another. library(openair) library(tidyverse) timeVariation(filter(mydata, ws &gt; 3, wd &gt; 100, wd &lt; 270), pollutant = &quot;pm10&quot;, ylab = &quot;pm10 (ug/m3)&quot;) Figure 11.1: Example plot using the timeVariation function to plot PM10 concentrations at Marylebone Road. Figure 11.2 shows the function applied to concentrations of NOx, CO, NO2 and O3 concentrations. In this case the concentrations have been normalised. The plot clearly shows the markedly different temporal trends in concentration. For CO, there is a very pronounced increase in concentrations during the peak pm rush hour. The other important difference is on Sundays when CO concentrations are relatively much higher than NOx. This is because flows of cars (mostly petrol) do not change that much by day of the week, but flows of vans and HGVs (diesel vehicles) are much less on Sundays. Note, however, that the monthly trend is very similar in each case — which indicates very similar source origins. Taken together, the plots highlight that traffic emissions dominate this site for CO and NOx, but there are important difference in how these emissions vary by hour of day and day of week. Also shown in the very different behaviour of O3. Because O3 reacts with NO, concentrations of NOx and O3 tend to be anti-correlated. Note also the clear peak in O3 in April/May, which is due to higher northern hemispheric background concentrations in the spring. Even at a busy roadside site in central London this influence is clear to see. timeVariation(mydata, pollutant = c(&quot;nox&quot;, &quot;co&quot;, &quot;no2&quot;, &quot;o3&quot;), normalise = TRUE) Figure 11.2: Example plot using the timeVariation function to plot NOx, CO, NO2 and O3 concentrations at Marylebone Road. In this plot, the concentrations are normalised. Another example is splitting the data set by time. We use the splitByDate function to divide up the data into dates before January 2003 and after January 2003. This time the option difference is used to highlight how NO2 concentrations have changed over these two periods. The results are shown in Figure 11.3. There is some indication in this plot that data after 2003 seem to show more of a double peak in the diurnal plots; particularly in the morning rush hour. Also, the difference line does more clearly highlight a more substantial change over weekdays and weekends. Given that cars are approximately constant at this site each day, the change may indicate a change in vehicle emissions from other vehicle types. Given that it is known that primary NO2 emissions are known to have increased sharply from the beginning of 2003 onwards, this perhaps provides clues as to the principal cause. ## split data into two periods (see Utlities section for more details) mydata &lt;- splitByDate(mydata, dates= &quot;1/1/2003&quot;, labels = c(&quot;before Jan. 2003&quot;, &quot;After Jan. 2003&quot;)) timeVariation(mydata, pollutant = &quot;no2&quot;, group = &quot;split.by&quot;, difference = TRUE) Figure 11.3: Example plot using the timeVariation function to plot NO2 concentrations at Marylebone Road. In this plot, the concentrations are shown before and after January 2003. In the next example it is shown how to compare one subset of data of interest with another. Again, there can be many reasons for wanting to do this and perhaps the data set at Marylebone Road is not the most interesting to consider. Nevertheless, the code below shows how to approach such a problem. The scenario would be that one is interested in a specific set of conditions and it would be useful to compare that set, with another set. A good example would be from an analysis using the polarPlot function where a ‘feature’ of interest has been identified—maybe an indication of a different source. But does this potentially different source behave differently in terms of temporal variation? If it does, then maybe that provides evidence to support that it is a different source. In a wider context, this approach could be used in many different ways depending on available data. A good example is the analysis of model output where many diagnostic meteorological data are available. This is an area that will be developed. The approach here is to first make a new variable called feature' and fill it with the valueother’. A subset of data is defined and the associated locations in the data frame identified. The subset of data is then used to update the `feature’ field with a new description. This approach could be extended to some quite complex situations. There are a couple of things to note in Figure 11.2. There seems to be evidence that for easterly winds &gt; 4 m s-1 that concentrations of SO2 are lower at night. Also, there is some evidence that concentrations for these conditions are also lower at weekends. This might reflect that SO2 concentrations for these conditions tend to be dominated by tall stack emissions that have different activities to road transport sources. This technique will be returned to with different data sets in future. ## make a field called &quot;feature&quot; and fill: make all values = &quot;other&quot; mydata &lt;- mutate(mydata, feature = ifelse(ws &gt; 4 &amp; wd &gt; 0 &amp; wd &lt;= 180, &quot;easterly&quot;, &quot;other&quot;)) timeVariation(mydata, pollutant =&quot;so2&quot;, group = &quot;feature&quot;, ylab = &quot;so2 (ppb)&quot;, difference = TRUE) Figure 11.4: Example plot using the timeVariation function to plot SO2 concentrations at Marylebone Road. In this plot, the concentrations are shown for a subset of easterly conditions and everything else. Note that the uncertainty in the mean values for easterly winds is greater than other. This is mostly because the sample size is much lower for easterly compared with other. By default timeVariation shows the mean variation in different temporal components and the 95% confidence interval in the mean. However, it is also possible to show how the data are distributed by using a different option for statistic. When statistic = \"median\" the median line is shown together with the 25/75th and 5/95th quantile values. Users can control the quantile values shown be setting the conf.int. For example, conf.int = c(0.25, 0.99) will show the median, 25/75th and 1/99th quantile values. The statistic = \"median\" option is therefore very useful for showing how the data are distributed — somewhat similar to a box and whisker plot. Note that it is expected that only one pollutant should be shown when statistic = \"median\" is used due to potential over-plotting; although the function will display several species of required. An example is shown in Figure 11.5 for PM10 concentrations. timeVariation(mydata, pollutant = &quot;pm10&quot;, statistic = &quot;median&quot;, col = &quot;firebrick&quot;) Figure 11.5: Example plot using the timeVariation function to show the variation in the median, 25/75th and 5/95th quantile values for PM10. The shading shows the extent to the 25/75th and 5/95th quantiles. 11.3 Output The timeVariation function produces several outputs that can be used for further analysis or plotting. It is necessary to read the output into a variable for further processing. The code below shows the different objects that are returned and the code shows how to access them. myOutput &lt;- timeVariation(mydata, pollutant = &quot;so2&quot;) ## show the first part of the day/hour variation ## note that value = mean, and Upper/Lower the 95% confid. intervals head(myOutput$data$day.hour) ## # A tibble: 6 x 8 ## # Groups: ci [1] ## variable wkday hour default Mean Lower Upper ci ## &lt;fct&gt; &lt;ord&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 so2 Monday 0 01 January 1998 to 23 June 2… 2.93 2.63 3.19 0.95 ## 2 so2 Tuesday 0 01 January 1998 to 23 June 2… 3.21 3.02 3.46 0.95 ## 3 so2 Wednesday 0 01 January 1998 to 23 June 2… 3.35 3.09 3.59 0.95 ## 4 so2 Thursday 0 01 January 1998 to 23 June 2… 3.22 2.98 3.56 0.95 ## 5 so2 Friday 0 01 January 1998 to 23 June 2… 3.64 3.39 3.90 0.95 ## 6 so2 Saturday 0 01 January 1998 to 23 June 2… 4.25 4.01 4.58 0.95 ## can make a new data frame of this data e.g. day.hour &lt;- myOutput$data$day.hour head(day.hour) ## # A tibble: 6 x 8 ## # Groups: ci [1] ## variable wkday hour default Mean Lower Upper ci ## &lt;fct&gt; &lt;ord&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 so2 Monday 0 01 January 1998 to 23 June 2… 2.93 2.63 3.19 0.95 ## 2 so2 Tuesday 0 01 January 1998 to 23 June 2… 3.21 3.02 3.46 0.95 ## 3 so2 Wednesday 0 01 January 1998 to 23 June 2… 3.35 3.09 3.59 0.95 ## 4 so2 Thursday 0 01 January 1998 to 23 June 2… 3.22 2.98 3.56 0.95 ## 5 so2 Friday 0 01 January 1998 to 23 June 2… 3.64 3.39 3.90 0.95 ## 6 so2 Saturday 0 01 January 1998 to 23 June 2… 4.25 4.01 4.58 0.95 All the numerical results are given by: myOutput$data$day.hour ## are the weekday and hour results myOutput$data$hour ## are the diurnal results myOutput$data$day ## are the weekday results myOutput$data$month ## are the monthly results It is also possible to plot the individual plots that make up the (four) plots produced by timeVariation: ## just the diurnal variation plot(myOutput, subset = &quot;hour&quot;) ## day and hour plot(myOutput, subset = &quot;day.hour&quot;) ## weekday variation plot(myOutput, subset = &quot;day&quot;) ## monthly variation plot(myOutput, subset = &quot;month&quot;) "],
["sec-timeProp.html", "Section12 Time proportion plots 12.1 Background 12.2 Examples", " Section12 Time proportion plots 12.1 Background The timeProp (‘time proportion’) function shows time series plots as stacked bar charts. For a particular time, proportions of a chosen variable are shown as a stacked bar chart. The different categories in the bar chart are made up from a character or factor variable in a data frame. The function is primarily developed to support the plotting of cluster analysis output from polarCluster (see Section 8.6 and trajCluster (see Section 18.6 that consider local and regional (back trajectory) cluster analysis respectively. However, the function has more general use for understanding time series data. In order to plot time series in this way, some sort of time aggregation is needed, which is controlled by the option avg.time. The plot shows the value of pollutant on the y-axis (averaged according to avg.time). The time intervals are made up of bars split according to proportion. The bars therefore show how the total value of `pollutant} is made up for any time interval. 12.2 Examples An example of the timeProp function is shown in Figure 12.1. In this example SO2 concentrations are considered for 2003 (using the selectByDate function). The averaging period is set to 3~days and the mean concentration is plotted and the proportion contribution by wind sector is given. Other options are chosen to place the key at the top and choose the number of columns used in the key. It is apparent from Figure 12.1 that the highest SO2 concentrations are dominated by winds from an easterly sector, but actually occur throughout the year. timeProp(selectByDate(mydata, year = 2003), pollutant = &quot;so2&quot;, avg.time = &quot;3 day&quot;, proportion = &quot;wd&quot;, date.breaks = 10, key.position = &quot;top&quot;, key.columns = 8, ylab = &quot;so2 (ug/m3)&quot;) Figure 12.1: timeProp plot for SO2 concentrations in 2003. The data are categorised into 8 wind sectors for 3-day averages. Note that proportion can be an existing categorical (i.e. factor or character) variable in a data frame. If a numeric variable is supplied, then it is typically cut into four quantile levels. So, for example, the plot below would show four intervals of wind speed, which would help show the wind speed conditions that control high SO2 concentration — and importantly, when they occur. An example of using timeProp with a continuous variable is shown in Figure 12.2. In this case the wind speed values are split into 3 quantile levels. The number of quantiles used is determined by the option n.levels. This approach can be used for any numeric variables. timeProp(selectByDate(mydata, year = 2003), pollutant = &quot;so2&quot;, avg.time = &quot;3 day&quot;, n.levels = 3, cols = &quot;viridis&quot;, proportion = &quot;ws&quot;, date.breaks = 10, key.position = &quot;top&quot;, key.columns = 3) Figure 12.2: timeProp plot for SO2 concentrations in 2003. The data are categorised into 4 wind speed categories for 3-day averages. One of the key uses of timeProp is to post-process cluster analysis data. Users should consider the uses of timeProp for cluster analysis shown in Section 8.6 and Section 18.6. In both these cases the cluster analysis yields a categorical output directly i.e. cluster, which lends itself to analysis using timeProp. "],
["sec-trendLevel.html", "Section13 Trend heat maps 13.1 Another way of representing trends 13.2 Examples", " Section13 Trend heat maps 13.1 Another way of representing trends The trendLevel function provides a way of rapidly showing a large amount of data in a condensed way. It is particularly useful for plotting the level of a value against two categorical variables. These categorical variables can pre-exist in a data set or be made on the fly using openair. By default it will show the mean value of a variable against two categorical variables but can also consider a wider range of statistics e.g. the maximum, frequency, or indeed a user-defined function. The function is much more flexible than this by showing temporal data and can plot ‘heat maps’ in many flexible ways. Both continuous colour scales and user-defined categorical scales can be used. The trendLevel function shows how the value of a variable varies according to intervals of two other variables. The \\(x\\) and \\(y\\) variables can be categorical (factor or character) or numeric. The third variable (\\(z\\)) must be numeric and is coloured according to its value. Despite being called trendLevel the function is flexible enough to consider a wide range of plotting variables. If the \\(x\\) and \\(y\\) variables are not categorical they are made so by splitting the data into quantiles (using cutData). Furthermore, the user can supply as many levels as they wish for the quantile using the option n.levels. Remeber also there are lots of built-in options for x or y based on temporal variations (see Section 22.2) e.g. “month” (the default), “week”, “daylight” and so on. 13.2 Examples The standard output from trendLevel is shown in Figure 13.1, which shows the variation in NOx concentrations by year and hour of the day. By default the function will use “month” for the x-axis and “hour” for the y-axis. library(openair) library(tidyverse) trendLevel(mydata, pollutant = &quot;nox&quot;) Figure 13.1: Example output from trendLevel trendLevel(mydata, pollutant = &quot;nox&quot;, y = &quot;wd&quot;, border = &quot;white&quot;, cols = &quot;jet&quot;) Figure 13.2: trendLevel output with wind direction as y. Figure 13.3 indicates that the highest NOx concentrations most strongly associate with wind sectors about 200 degrees, appear to be decreasing over the years, but do not appear to associate with an SO2 rich NOx source. Using type = \"so2\" would have conditioned by absolute SO2 concentration. As both a moderate contribution from an SO2 rich source and a high contribution from an SO2 poor source could generate similar SO2 concentrations, such conditioning can sometimes blur interpretations. The use of this type of ‘over pollutant’ ratio reduces this blurring by focusing conditioning on cases when NOx concentrations (be they high or low) associate with relatively high or low SO2 concentrations. ## new field: so2/nox ratio mydata &lt;- mutate(mydata, ratio = so2 / nox) ## condition by mydata$new trendLevel(mydata, &quot;nox&quot;, x = &quot;year&quot;, y = &quot;wd&quot;, type = &quot;ratio&quot;, cols = &quot;inferno&quot;) Figure 13.3: trendLevel output with SO2 : NOx ratio type conditioning. The plot can be used in much more flexible ways. Here are some examples (not plotted): A plot of mean O3 concentration shown by season and by daylight/nighttime hours. trendLevel(mydata, x = &quot;season&quot;, y = &quot;daylight&quot;, pollutant = &quot;o3&quot;) Or by season and hour of the day: trendLevel(mydata, x = &quot;season&quot;, y = &quot;hour&quot;, pollutant = &quot;o3&quot;, cols = &quot;increment&quot;) How about NOx versus NO2 coloured by the concentration of O3? scatterPlot could also be used to produce such a plot. However, one interesting difference with using trendLevel is that the data are split into quantiles where equal numbers of data exist in each interval. This approach can make it a bit easier to see the underlying relationship between variables. A scatter plot may have too much data to be clear and also outliers (or regions with relatively few data) that make it harder to see what is going on. The plot generated by the command below makes it a bit easier to see that it is the higher quantiles of NO2 that are associated with higher O3 concentration (as well as low NOx and NO2 concentrations). trendLevel(mydata, x = &quot;nox&quot;, y = &quot;no2&quot;, pollutant = &quot;o3&quot;, border = &quot;white&quot;, n.levels = 30, statistic = &quot;max&quot;, limits = c(0, 50)) Figure 13.4: trendLevel showing NOx against NO2, coloured by the concentration of O3. The plot can also be shown by wind direction sector, this time showing how O3 varies by weekday, wind direction sector and NOx quantile. trendLevel(mydata, x = &quot;nox&quot;, y = &quot;weekday&quot;, pollutant = &quot;o3&quot;, border = &quot;white&quot;, n.levels = 10, statistic = &quot;max&quot;, limits = c(0, 50), type = &quot;wd&quot;) By default trendLevel subsamples the plotted pollutant data by the supplied x, y and type parameters and in each case calculates the mean. The option statistic has always let you apply other statistics. For example, trendLevel also calculated the maximum via the option statistic = \"max\". The user may also use their own statistic function. As a simple example, consider the above plot which summarises by mean. This tells us about average concentrations. It might also be useful to consider a particular percentile of concentrations. This can be done by defining one’s own function as shown in Figure 13.5. ## function to estimate 95th percentile percentile &lt;- function(x) quantile(x, probs = 0.95, na.rm = TRUE) ## apply to present plot trendLevel(mydata, &quot;nox&quot;, x = &quot;year&quot;, y = &quot;wd&quot;, type = &quot;ratio&quot;, cols = &quot;viridis&quot;, statistic = percentile) Figure 13.5: trendLevel using locally defined statistic. This type of flexibility really opens up the potential of the function as a screening tool for the early stages of data analysis. Increased control of x, y, type and statistic allow you to very quick explore your data and develop an understanding of how different parameters interact. Patterns in trendLevel plots can also help to direct your openair analysis. For example, possible trends in data conditioned by year would suggest that functions like smoothTrend or TheilSen could provide further insight. Likewise, windRose or polarPlot could be useful next steps if wind speed and direct conditioning produces interesting features. However, perhaps most interestingly, novel conditioning or the incorporation of novel parameters in this type of highly flexible function provides a means of developing new data visualisation and analysis methods. trendLevel can also be used with user defined discrete colour scales as shown in Figure 13.6. In this case the default \\(x\\) and \\(y\\) variables are chosen (month and hour) split by type (year). trendLevel(mydata, pollutant = &quot;no2&quot;, x = &quot;week&quot;, border = &quot;white&quot;, statistic = &quot;max&quot;, breaks = c(0, 50, 100, 500), labels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), cols = c(&quot;forestgreen&quot;, &quot;yellow&quot;, &quot;red&quot;), key.position = &quot;top&quot;) Figure 13.6: trendLevel plot for maximum NO2 concentrations using a user-defined discrete colour scale. "],
["sec-scatterPlot.html", "Section14 Scatter plots 14.1 Purpose 14.2 Examples", " Section14 Scatter plots 14.1 Purpose Scatter plots are extremely useful and a very commonly used analysis technique for considering how variables relate to one another. R does of course have many capabilities for plotting data in this way. However, it can be tricky to add linear relationships, or split scatter plots by levels of other variables etc. The purpose of the scatterPlot function is to make it straightforward to consider how variables are related to one another in a way consistent with other openair functions. We have added several capabilities that can be used just by setting different options, some of which are shown below. There is less need for this function now that ggplot2 is available, but it still has some benefits for intercative use. A smooth fit is automatically added to help reveal the underlying relationship between two variables together with the estimated 95% confidence intervals of the fit. This is in general an extremely useful thing to do because it helps to show the (possibly) non-linear relationship between variables in a very robust way — or indeed whether the relationship is linear. It is easy to add a linear regression line. The resulting equation is shown on the plot together with the R\\(^2\\) value. For large data sets there is the possibility to `bin’ the data using hexagonal binning or kernel density estimates. This approach is very useful when there is considerable over-plotting. It is easy to show how two variables are related to one another dependent on levels of a third variable. This capability is very useful for exploring how different variables depend on one another and can help reveal the underlying important relationships. A plot of two variables can be colour-coded by a continuous colour scale of a third variable. It can handle date/time x-axis formats to provide an alternative way of showing time series, which again can be colour-coded by a third variable. The scatterPlot function isn’t really specific to atmospheric sciences, in the same way as other plots. It is more a function for convenience, written in a style that is consistent with other openair functions. Nevertheless, along with the timePlot function they do form an important part of openair because of the usefulness of understanding show variables relate to one another. Furthermore, there are many options to make it easy to explore data in an interactive way without worrying about processing data or formatting plots. 14.2 Examples We provide a few examples of use and as usual, users are directed towards the help pages (type ?scatterPlot) for more extensive examples. First we select a subset of data (2003) using the openair selectByDate function and plot NOx vs. NO2 Figure (14.1). data2003 &lt;- selectByDate(mydata, year = 2003) scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;) Figure 14.1: Scatter plot of hourly NOx vs. NO2 at Marylebone Road for 2003. Often with several years of data, points are over-plotted and it can be very difficult to see what the underlying relationship looks like. One very effective method to use in these situations is to ‘bin’ the data and to colour the intervals by the number of counts of occurrences in each bin. There are various ways of doing this, but `hexagonal binning’ is particularly effective because of the way hexagons can be placed next to one another.4 To use hexagonal binning it will be necessary to install the hexbin package: 14.2.1 Hexaganol binning Now it should be possible to make the plot by setting the method option to method = \"hexbin\", as shown in Figure 14.2. The benefit of hexagonal binning is that it works equally well with enormous data sets e.g. several million records. In this case Figure 14.2 provides a clearer indication of the relationship between NOx and NO2 than Figure 14.1 because it reveals where most of the points lie, which is not apparent from Figure 14.1. Note that For method = \"hexbin\" it can be useful to transform the scale if it is dominated by a few very high values. This is possible by supplying two functions: one that that applies the transformation and the other that inverses it. For log scaling for example (the default), trans = function(x) log(x) and inv = function(x) exp(x). For a square root transform use trans = sqrt and inv = function(x) x\\^2. To not apply any transformation trans = NULL and inv = NULL should be used. scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;, method = &quot;hexbin&quot;, col= &quot;jet&quot;) Figure 14.2: Scatter plot of hourly NOx vs. NO2 at Marylebone Road using hexagonal binning. The number of occurrences in each bin is colour-coded (not on a linear scale). It is now possible to see where most of the data lie and a better indication of the relationship between NOx and NO2 is revealed. Note that when method = \"hexbin\" there are various options that are useful e.g. a border around each bin and the number of bins. For example, to place a grey border around each bin and set the bin size try: scatterPlot(mydata, x = &quot;nox&quot;, y = &quot;no2&quot;, method = &quot;hexbin&quot;, col = &quot;jet&quot;, border = &quot;grey&quot;, xbin = 15) The hexagonal binning and other binning methods are useful but often the choice of bin size is somewhat arbitrary. Another useful approach is to use a kernel density estimate to show where most points lie. This is possible in scatterPlot with the method = \"density\" option. Such a plot is shown in Figure 14.3. scatterPlot(selectByDate(mydata, year = 2003), x = &quot;nox&quot;, y = &quot;no2&quot;, method = &quot;density&quot;, cols = &quot;jet&quot;) ## (loaded the KernSmooth namespace) Figure 14.3: Scatter plot of hourly NOx vs. NO2 at Marylebone Road using a kernel density estimate to show where most of the points lie. The intensity is a measure of how many points lie in a unit area of NOx and NO2 concentration. Sometimes it is useful to consider how the relationship between two variables varies by levels of a third. In openair this approach is possible by setting the option type. When type is another numeric variables, four plots are produced for different quantiles of that variable. We illustrate this point by considering how the relationship between NOx and NO2 varies with different levels of O3. We also take the opportunity to not plot the smooth line, but plot a linear fit instead and force the layout to be a 2 by 2 grid. scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;, type = &quot;o3&quot;, smooth = FALSE, linear = TRUE, layout = c(2, 2)) Figure 14.4: Scatter plot of hourly NOx vs. NO2 at Marylebone Road by different levels of O3. Below is an extended example that brings together data manipulation, refined plot options and linear fitting of two variables with NOx. The aim is to plot the weekly concentration of NOx against PM10 and PM2.5 and fit linear equations to both relationships. To do this we need the \\(x\\) variable as NOx and the \\(y\\) variable as PM10 or PM2.5, which means we also need a column that will act as a grouping column i.e. identifies whether the \\(y\\) is PM10 or PM2.5. # load the packages we need library(tidyverse) # select the variables of interest subdat &lt;- select(mydata, date, nox, pm10, pm25) # calculate weekly averages subdat &lt;- timeAverage(subdat, avg.time = &quot;week&quot;) # reshape so we have two variable columns subdat &lt;- pivot_longer(subdat, cols = c(pm10, pm25), names_to = &quot;pollutant&quot;) head(subdat) ## # A tibble: 6 x 4 ## date nox pollutant value ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1997-12-29 00:00:00 128. pm10 21.8 ## 2 1997-12-29 00:00:00 128. pm25 NaN ## 3 1998-01-05 00:00:00 189. pm10 33.6 ## 4 1998-01-05 00:00:00 189. pm25 NaN ## 5 1998-01-12 00:00:00 203. pm10 29.1 ## 6 1998-01-12 00:00:00 203. pm25 NaN Now we will plot weekly NOx versus PM10 and PM2.5 and fit a linear equation to both — and adjust some of the symbols (shown in Figure 14.5. scatterPlot(subdat, x = &quot;nox&quot;, y = &quot;value&quot;, group = &quot;pollutant&quot;, pch = 21:22, cex = 1.6, fill = c(&quot;dodgerblue&quot;, &quot;tomato&quot;), col = &quot;white&quot;, linear = TRUE, xlab = &quot;nox (ppb)&quot;, ylab = &quot;PM concentration (ug/m3)&quot;) Figure 14.5: Scatter plot of weekly NOx vs. PM10 and PM2.5 at Marylebone Road with linear equations shown and plot symbols modified. To gain a better idea of where the data lie and the linear fits, adding some transparency helps: scatterPlot(subdat, x = &quot;nox&quot;, y = &quot;value&quot;, group = &quot;variable&quot;, pch = 21:22, cex = 1.6, fill = c(&quot;dodgerblue&quot;, &quot;tomato&quot;), col = &quot;white&quot;, linear = TRUE, xlab = &quot;nox (ppb)&quot;, ylab = &quot;PM concentration (ug/m3)&quot;, alpha = 0.2) The above example will also work with type. For example, to consider how NOx againts PM10 and PM2.5 varies by season: scatterPlot(subdat, x = &quot;nox&quot;, y = &quot;value&quot;, group = &quot;variable&quot;, pch = 21:22, cex = 2, fill = c(&quot;dodgerblue&quot;, &quot;tomato&quot;), col = &quot;white&quot;, linear = TRUE, xlab = &quot;nox (ppb)&quot;, ylab = &quot;PM concentration (ug/m3)&quot;, type = &quot;season&quot;) Finally, we show how to plot a continuous colour scale for a third numeric variable setting the value of z to the third variable. Figure 14.6 shows again the relationship between NOx and NO2 but this time colour-coded by the concentration of O3. We also take the opportunity to split the data into seasons and weekday/weekend by setting type = c(\"season\", \"weekend\"). There is an enormous amount of information that can be gained from plots such as this. Differences between weekdays and the weekend can highlight changes in emission sources, splitting by seasons can show seasonal influences in meteorology and background O3 and colouring the data by the concentration of O3 helps to show how O3 concentrations affect NO2 concentrations. For example, consider the summertime-weekday panel where it clearly shows that the higher NO2 concentrations are associated with high O3 concentrations. Indeed there are some hours where NO2 is $&gt;\\(100~ppb at quite low concentrations of NO~x~ (\\)$200 ppb). It would also be interesting instead of using O3 concentrations from Marylebone Road to use O3 from a background site. Figure 14.6 was very easily produced but contains a huge amount of useful information showing the relationship between NOx and NO2 dependent upon the concentration of O3, the season and the day of the week. There are of course numerous other plots that are equally easily produced. scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;, z = &quot;o3&quot;, type = c(&quot;season&quot;, &quot;weekend&quot;), limits = c(0, 30)) Figure 14.6: Scatter plot of hourly NOx vs. NO2 at Marylebone Road by different levels of O3 split by season and weekday-weekend. Figure 14.7 shows that scatterPlot can also handles dates on the x-axis; in this case shown for SO2 concentrations coloured by wind direction for August 2003. scatterPlot(selectByDate(data2003, month = 8), x = &quot;date&quot;, y = &quot;so2&quot;, z = &quot;wd&quot;) Figure 14.7: Scatter plot of date vs. SO2- at Marylebone Road by different levels of wind direction for August 2003. Similar to Section 10, scatterPlot can also plot wind vector arrows if wind speed and wind direction are available in the data frame. Figure 14.8 shows an example of using the windflow option. The Figure also sets many other options including showing the concentration of O3 as a colour, setting the colour scale used and selecting a few days of interest using the selectByDate function. Figure 14.8 shows that when the wind direction changes to northerly, the concentration of NO2 decreases and that of O3 increases. scatterPlot(selectByDate(mydata, start = &quot;1/6/2001&quot;, end = &quot;5/6/2001&quot;), x = &quot;date&quot;, y = &quot;no2&quot;, z = &quot;o3&quot;, col = &quot;increment&quot;, windflow = list(scale = 0.15), key.footer = &quot;o3\\n (ppb)&quot;, main = NULL, ylab = &quot;no2 (ppb)&quot;) Figure 14.8: Scatter plot of date vs. NO2 with the colour scale representing O3. The wind flow vectors are also shown. In fact it is not possible to have a shape with more than 6 sides that can be used to forma a lattice without gaps.↩︎ "],
["sec-calendarPlot.html", "Section15 Calendar plots 15.1 Purpose 15.2 Calendar examples", " Section15 Calendar plots 15.1 Purpose Sometimes it is useful to visualise data in a familiar way. Calendars are the obvious way to represent data for data on the time scale of days or months. The calendarPlot function provides an effective way to visualise data in this way by showing daily concentrations laid out in a calendar format. The concentration of a species is shown by its colour. The data can be shown in different ways. By default, calendarPlot overlays the day of the month. However, if wind speed and wind direction are available then an arrow can be shown for each day giving the vector-averaged wind direction. In addition, the arrow can be scaled according to the wind speed to highlight both the direction and strength of the wind on a particular day, which can help show the influence of meteorology on pollutant concentrations. calendarPlot can also show the daily mean concentration as a number on each day and can be extended to highlight those conditions where daily mean (or maximum etc.) concentrations are above a particular threshold. This approach is useful for highlighting daily air quality limits e.g. when the daily mean concentration is greater than 50 \\(\\mu\\)g m-3. The calendarPlot function can also be used to plot categorical scales. This is useful for plotting concentrations expressed as an air quality index i.e. intervals of concentrations that are expressed in ways like ‘very good’, ‘good’, ‘poor’ and so on. 15.2 Calendar examples The function is called in the usual way. As a minimum, a data frame, pollutant and year is required. So to show O3 concentrations for each day in 2003 (Figure 15.1). Note that if year is not supplied the full data set will be used. library(openair) calendarPlot(mydata, pollutant = &quot;o3&quot;, year = 2003) Figure 15.1: calendarPlot for O3 concentrations in 2003. It is sometimes useful to annotate the plots with other information. It is possible to show the daily mean wind angle, which can also be scaled to wind speed. The idea here being to provide some information on meteorological conditions on each day. Another useful option is to set annotate = \"value\" in which case the daily concentration will be shown on each day. Furthermore, it is sometimes useful to highlight particular values more clearly. For example, to highlight daily mean PM10 concentrations above 50 \\(\\mu\\)g m-3. This is where setting lim (a concentration limit) is useful. In setting lim the user can then differentiate the values below and above lim by colour of text, size of text and type of text e.g. plain and bold. Figure 15.2 highlights those days where PM10 concentrations exceed 50 \\(\\mu\\)g m-3 by making the annotation for those days bigger, bold and orange. Plotting the data in this way clearly shows the days where PM10 &gt; 50 \\(\\mu\\)g m-3. Other openair functions can be used to plot other statistics. For example, rollingMean could be used to calculate rolling 8-hour mean O3 concentrations. Then, calendarPlot could be used with statistic = \"max\" to show days where the maximum daily rolling 8-hour mean O3 concentration is greater than a certain threshold e.g. 100 or 120~\\(\\mu\\)g m-3. calendarPlot(mydata, pollutant = &quot;pm10&quot;, year = 2003, annotate = &quot;value&quot;, lim =50, cols = &quot;Purples&quot;, col.lim = c(&quot;black&quot;, &quot;orange&quot;), layout = c(4, 3)) Figure 15.2: calendarPlot for PM10 concentrations in 2003 with annotations highlighting those days where the concentration of PM10 &gt;50~\\(\\mu\\)g m-3. The numbers show the PM10 concentration in \\(\\mu\\)g m-3. To show wind angle, scaled to wind speed (Figure 15.3). calendarPlot(mydata, pollutant = &quot;o3&quot;, year = 2003, annotate = &quot;ws&quot;) Figure 15.3: calendarPlot for O3 concentrations in 2003 with annotations showing wind angle scaled to wind speed i.e. the longer the arrow, the higher the wind speed. It shows for example high O3 concentrations on the 17 and 18th of April were associated with strong north-easterly winds Note again that selectByDate can be useful. For example, to plot select months: calendarPlot(selectByDate(mydata, year = 2003, month = c(&quot;jun&quot;, &quot;jul&quot;, &quot;aug&quot;)), pollutant = &quot;o3&quot;, year = 2003) Figure 15.4 shows an example of plotting data with a categorical scale. In this case the options labels and breaks have been used to define concentration intervals and their descriptions. Note that breaks needs to be one longer than labels. In the example in Figure 15.4 the first interval (‘Very low’) is defined as concentrations from 0 to 50 (ppb), ‘Low’ is 50 to 100 and so on. Note that the upper value of breaks should be a number greater than the maximum value contained in the data to ensure that it is encompassed. In the example given in Figure 15.4 the maximum daily concentration is plotted. These types of plots are very useful for considering national or international air quality indexes. calendarPlot(mydata, pollutant = &quot;no2&quot;, year = 2003, breaks = c(0, 50, 100, 150, 1000), labels = c(&quot;Very low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Very High&quot;), cols = &quot;increment&quot;, statistic = &quot;max&quot;) Figure 15.4: calendarPlot for NO2 concentrations in 2003 with a user-defined categorical scale. The user can explicitly set each colour interval: calendarPlot(mydata, pollutant = &quot;no2&quot;, year = 2003, breaks = c(0, 50, 100, 150, 1000), labels = c(&quot;Very low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Very High&quot;), cols = c(&quot;lightblue&quot;, &quot;forestgreen&quot;, &quot;yellow&quot;, &quot;red&quot;), statistic = &quot;max&quot;) Note that in the case of categorical scales it is possible to define the breaks and labels first and then make the plot. For example: breaks &lt;- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) labels &lt;- c(&quot;Low.1&quot;, &quot;Low.2&quot;, &quot;Low.3&quot;, &quot;Moderate.4&quot;, &quot;Moderate.5&quot;, &quot;Moderate.6&quot;, &quot;High.7&quot;, &quot;High.8&quot;, &quot;High.9&quot;, &quot;Very High.10&quot;) calendarPlot(mydata, pollutant = &quot;no2&quot;, year = 2003, breaks = breaks, labels = labels, cols = &quot;jet&quot;, statistic = &quot;max&quot;) It is also possible to first use rollingMean to calculate statistics. For example, if one was interested in plotting the maximum daily rolling 8-hour mean concentration, the data could be prepared and plotted as follows. ## makes a new field &#39;rolling8o3&#39; dat &lt;- rollingMean(mydata, pollutant = &quot;o3&quot;, hours = 8) breaks &lt;- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) labels &lt;- c(&quot;Low.1&quot;, &quot;Low.2&quot;, &quot;Low.3&quot;, &quot;Moderate.4&quot;, &quot;Moderate.5&quot;, &quot;Moderate.6&quot;, &quot;High.7&quot;, &quot;High.8&quot;, &quot;High.9&quot;, &quot;Very High.10&quot;) calendarPlot(dat, pollutant = &quot;rolling8o3&quot;, year = 2003, breaks = breaks, labels = labels, cols = &quot;jet&quot;, statistic = &quot;max&quot;) The UK has an air quality index for O3, NO2, PM10 and described in detail at (http://uk-air.defra.gov.uk/air-pollution/daqi) and COMEAP (2011). The index is most relevant to air quality forecasting, but is used widely for public information. Most other countries have similar indexes. Note that the indexes are calculated for different averaging times dependent on the pollutant: rolling 8-hour mean for O3, hourly means for NO2 and a fixed 24-hour mean for PM10 and PM2.5. In the code below the labels and breaks are defined for each pollutant to make it easier to use the index in the calendarPlot function. ## the labels - same for all species labels &lt;- c(&quot;1 - Low&quot;, &quot;2 - Low&quot;, &quot;3 - Low&quot;, &quot;4 - Moderate&quot;, &quot;5 - Moderate&quot;, &quot;6 - Moderate&quot;, &quot;7 - High&quot;, &quot;8 - High&quot;, &quot;9 - High&quot;, &quot;10 - Very High&quot;) o3.breaks &lt;-c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) no2.breaks &lt;- c(0, 67, 134, 200, 268, 335, 400, 468, 535, 600, 1000) pm10.breaks &lt;- c(0, 17, 34, 50, 59, 67, 75, 84, 92, 100, 1000) pm25.breaks &lt;- c(0, 12, 24, 35, 42, 47, 53, 59, 65, 70, 1000) Remember it is necessary to use the correct averaging time. Assuming data are imported using importAURN or similar, then the units will be in \\(\\mu\\)g m-3 — if not the user should ensure this is the case. Note that rather than showing the day of the month (the default), annotate = \"value\" can be used to show the actual numeric value on each day. In this way, the colours represent the categorical interval the concentration on a day corresponds to and the actual value itself is shown. ## import test data dat &lt;- importAURN(site = &quot;kc1&quot;, year = 2010) ## no2 index example calendarPlot(dat, year = 2010, pollutant = &quot;no2&quot;, labels = labels, breaks = no2.breaks, statistic = &quot;max&quot;, cols = &quot;jet&quot;) ## for PM10 or PM2.5 we need the daily mean concentration calendarPlot(dat, year = 2010, pollutant = &quot;pm10&quot;, labels = labels, breaks = pm10.breaks, statistic = &quot;mean&quot;, cols = &quot;jet&quot;) ## for ozone, need the rolling 8-hour mean dat &lt;- rollingMean(dat, pollutant = &quot;o3&quot;, hours = 8) calendarPlot(dat, year = 2010, pollutant = &quot;rolling8o3&quot;, labels = labels, breaks = o3.breaks, statistic = &quot;max&quot;, cols = &quot;jet&quot;) References "],
["sec-TheilSen.html", "Section16 Theil-Sen trends 16.1 Trend estimates 16.2 Example trend analysis", " Section16 Theil-Sen trends 16.1 Trend estimates Calculating trends for air pollutants is one of the most important and common tasks that can be undertaken. Trends are calculated for all sorts of reasons. Sometimes it is useful to have a general idea about how concentrations might have changed. On other occasions a more definitive analysis is required; for example, to establish statistically whether a trend is significant or not. The whole area of trend calculation is a complex one and frequently trends are calculated with little consideration as to their validity. Perhaps the most common approach is to apply linear regression and not think twice about it. However, there can be many pitfalls when using ordinary linear regression, such as the assumption of normality, autocorrelation etc. One commonly used approach for trend calculation in studies of air pollution is the non-parametric Mann-Kendall approach (Hirsch, Slack, and Smith 1982). Wilcox (2010) provides an excellent case for using ‘modern methods’ for regression including the benefits of non-parametric approaches and bootstrap simulations. Note also that the all the regression parameters are estimated through bootstrap resampling. The Theil-Sen method dates back to 1950, but the basic idea pre-dates 1950 (Theil 1950; Sen 1968). It is one of those methods that required the invention of fast computers to be practical. The basic idea is as follows. Given a set of \\(n\\) \\(x\\), \\(y\\) pairs, the slopes between all pairs of points are calculated. Note, the number of slopes can increase by \\(\\approx\\) \\(n^2\\) so that the number of slopes can increase rapidly as the length of the data set increases. The Theil-Sen estimate of the slope is the median of all these slopes. The advantage of the using the Theil-Sen estimator is that it tends to yield accurate confidence intervals even with non-normal data and heteroscedasticity (non-constant error variance). It is also resistant to outliers — both characteristics can be important in air pollution. As previously mentioned, the estimates of these parameters can be made more robust through bootstrap-resampling, which further adds to the computational burden, but is not an issue for most time series which are expressed either as monthly or annual means. Bootstrap resampling also provides the estimate of \\(p\\) for the slope. An issue that can be very important for time series is dependence or autocorrelation in the data. Normal (in the statistical sense) statistics assume that data are independent, but in time series this is rarely the case. The issue is that neighbouring data points are similar to one another (correlated) and therefore not independent. Ignoring this dependence would tend to give an overly optimistic impression of uncertainties. However, taking account of it is far from simple. A discussion of these issues is beyond the aims of this report and readers are referred to standard statistical texts on the issue. In openair we follow the suggestion of Kunsch (1989) of setting the block length to \\(n^{1/3}\\) where n is the length of the time series. There is a temptation when considering trends to use all the available data. Why? Often it is useful to consider specific periods. For example, is there any evidence that concentrations of have decreased since 2000? Clearly, the time period used depends on both the data and the questions, but it is good to be aware that considering subsets of data can be very insightful. Another aspect is that almost all trends are shown as mean concentration versus time; typically by year. Such analyses are very useful for understanding how concentrations have changed through time and for comparison with air quality limits and regulations. However, if one is interested in understanding why trends are as they are, it can be helpful to consider how concentrations vary in other ways. The trend functions in openair do just this. Trends can be plotted by day of the week, month, hour of the day, by wind direction sector and by different wind speed ranges. All these capabilities are easy to use and their effectiveness will depend on the situation in question. One of the reasons that trends are not considered in these many different ways is that there can be a considerable overhead in carrying out the analysis, which is avoided by using these functions. Few, for example, would consider a detailed trend analysis by hour of the day, ensuring that robust statistical methods were used and uncertainties calculated. However, it can be useful to consider how concentrations vary in this way. It may be, for example, that the hours around midday are dominated by heavy vehicle emissions rather than by cars — so is the trend for a pollutant different for those hours compared with say, hours dominated by other vehicle types? Similarly, a much more focussed trend analysis can be done by considering different wind direction, as this can help isolate different source influences. The TheilSen function is typically used to determine trends in pollutant concentrations over several years. However, it can be used to calculate the trend in any numeric variable. It calculates monthly mean values from daily, hourly or higher time resolution data, as well as working directly with monthly means. Whether it is meaningful to calculate trends over shorter periods of time (e.g. 2 years) depends very much on the data. It may well be that statistically significant trends can be detected over relatively short periods but it is another matter whether it matters. Because seasonal effects can be important for monthly data, there is the option to deseasonalise the data first. The timeVariation function are both useful to determine whether there is a seasonal cycle that should be removed. Note also that the symbols shown next to each trend estimate relate to how statistically significant the trend estimate is: \\(p\\) \\(&lt;\\) 0.001 = \\(\\ast\\ast\\ast\\), \\(p\\) \\(&lt;\\) 0.01 = \\(\\ast\\ast\\), \\(p\\) \\(&lt;\\) 0.05 = \\(\\ast\\) and \\(p\\) \\(&lt;\\) 0.1 = \\(+\\). 16.2 Example trend analysis We first show the use of the TheilSen function by applying it to concentrations of O3. The function is called as shown in Figure 16.1. library(openair) TheilSen(mydata, pollutant = &quot;o3&quot;, ylab = &quot;ozone (ppb)&quot;, deseason = TRUE, date.format = &quot;%Y&quot;) ## [1] &quot;Taking bootstrap samples. Please wait.&quot; Figure 16.1: Trends in ozone at Marylebone Road. The plot shows the deseasonalised monthly mean concentrations of O3. The solid red line shows the trend estimate and the dashed red lines show the 95% confidence intervals for the trend based on resampling methods. The overall trend is shown at the top-left as 0.38 (ppb) per year and the 95% confidence intervals in the slope from 0.21–0.51 ppb/year. The \\(\\ast\\ast\\ast\\) show that the trend is significant to the 0.001 level. Because the function runs simulations to estimate the uncertainty in the slope, it can take a little time for all the calculations to finish. These printed results show that in this case the trend in O3 was +0.38 units (i.e. ppb) per year as an average over the entire period. It also shows the 95% confidence intervals in the trend ranged between 0.21 to 0.51 ppb/year. Finally, the significance level in this case is very high; providing very strong evidence that concentrations of O3 increased over the period. The plot together with the summary results is shown in Figure 16.1. Note that if one wanted to display the confidence intervals in the slope at the 99% confidence intervals, the code would be Figure 16.2. TheilSen(mydata, pollutant = &quot;o3&quot;, ylab = &quot;ozone (ppb)&quot;, alpha = 0.01) Sometimes it is useful to consider a subset of data, perhaps by excluding some years. This is easy with the filter function. The following code calculates trends for years greater than 1999 i.e. from 2000 onward. TheilSen(filter(mydata, format(date, &quot;%Y&quot;) &gt; 1999), pollutant = &quot;o3&quot;, ylab = &quot;ozone (ppb)&quot;) It is also possible to calculate trends in many other ways e.g. by wind direction. Considering how trends vary by wind direction can be extremely useful because the influence of different sources invariably depends on the direction of the wind. The TheilSen function splits the wind direction into 8 sectors i.e. N, NE, E etc. The Theil-Sen slopes are then calculated for each direction in turn. This function takes rather longer to run because the simulations need to be run eight times in total. Considering concentrations of O3 again, the output is shown in Figure 16.2. Note that this plot is specifically laid out to assist interpretation, with each panel located at the correct point on the compass. This makes it easy to see immediately that there is essentially no trend in O3 for southerly winds i.e. where the road itself has the strongest influence. On the other hand the strongest evidence of increasing O3 are for northerly winds, where the influence of the road is much less. The reason that there is no trend in O3 for southerly winds is that there is always a great excess of NO, which reacts with O3 to form NO2. At this particular location it will probably take many more years before O3 concentrations start to increase when the wind direction is southerly. Nevertheless, there will always be some hours that do not have such high concentrations of NO. TheilSen(mydata, pollutant = &quot;o3&quot;, type = &quot;wd&quot;, deseason = TRUE, date.format = &quot;%Y&quot;, ylab = &quot;ozone (ppb)&quot;) ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; Figure 16.2: Trends in ozone at Marylebone Road split by eight wind sectors. The TheilSen function will automatically organise the separate panels by the different compass directions. The option slope.percent can be set to express slope estimates as a percentage change per year. This is useful for comparing slopes for sites with very different concentration levels and for comparison with emission inventories. The percentage change uses the concentration at the beginning and end months to express the mean slope. The trend, \\(T\\) is defined as: \\[ T [\\%.yr^{-1}] = 100.\\left(\\frac{C_{End}}{C_{Start}} - 1\\right)\\Bigg /N_{years} \\tag{16.1} \\] where \\(C_{End}\\) and \\(C_{Start}\\) are the mean concentrations for the end and start date, respectfully. \\(N_{years}\\) is the number of years (or fractions of) the time series spans. TheilSen(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, slope.percent = TRUE) The TheilSen function was written to work with hourly data, which is then averaged into monthly or annual data. However, it is realised that users may already have data that is monthly or annual. The function can therefore accept as input monthly or annual data directly. However, it is necessary to ensure the date field is in the correct format. Assuming data in an Excel file in the format dd/mm/YYYY (e.g. 23/11/2008), it is necessary to convert this to a date format understood by R, as shown below. Similarly, if annual data were available, get the dates in formats like ‘2005-01-01’, ‘2006-01-01’ … and make sure the date is again formatted using as.Date. Note that if dates are pre-formatted as YYYY-mm-dd, then it is sufficient to use as.Date without providing any format information because it is already in the correct format. mydata$date = as.Date(mydata$date, format = &quot;%d/%m/%Y&quot;) Finally, the TheilSen function can consider trends at different sites, provided the input data are correctly formatted. For input, a data frame with three columns is required: date, pollutant and site. The call would then be, for example: TheilSen(mydata, pollutant = &quot;no2&quot;, type = &quot;site&quot;) 16.2.1 Output The TheilSen function provides lots of output data for further analysis or adding to a report. To obtain it, it is necessary to read it into a variable: MKresults &lt;- TheilSen(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, type = &quot;wd&quot;) ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; This returns a list of two data frames containing all the monthly mean values and trend statistics and an aggregated summary. The first 6 lines are shown next: head(MKresults$data[[1]]) ## wd date conc a b upper.a upper.b lower.a ## 1 E 1998-01-01 5.552253 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 2 E 1998-02-01 2.919639 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 3 E 1998-03-01 3.849363 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 4 E 1998-04-01 4.051668 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 5 E 1998-05-01 2.304686 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 6 E 1998-06-01 -1.560438 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## lower.b p p.stars slope intercept intercept.lower ## 1 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 2 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 3 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 4 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 5 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 6 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## intercept.upper lower upper slope.percent lower.percent ## 1 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 2 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 3 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 4 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 5 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 6 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## upper.percent ## 1 11.9614 ## 2 11.9614 ## 3 11.9614 ## 4 11.9614 ## 5 11.9614 ## 6 11.9614 Often only the trend statistics are required and not all the monthly values. These can be obtained by: MKresults$data[[2]] ## wd p.stars date conc a b upper.a ## 1 E * 2001-09-15 5.989974 -2.825001 7.185124e-04 -8.8266663 ## 2 N *** 2001-09-15 9.786267 -19.142102 2.485907e-03 -28.7157702 ## 3 NE *** 2001-09-15 9.728994 -9.728741 1.624970e-03 -24.8702270 ## 4 NW *** 2001-09-15 9.786755 -12.940875 1.937119e-03 -22.4192235 ## 5 S 2001-09-15 5.052728 4.472516 2.509968e-05 1.0744367 ## 6 SE 2001-09-15 5.780645 4.822713 7.357549e-05 0.8230425 ## 7 SW 2001-09-15 4.761100 1.818359 2.444766e-04 -1.9192744 ## 8 W ** 2001-09-15 5.618727 -1.178793 5.736058e-04 -5.6090971 ## upper.b lower.a lower.b p slope intercept ## 1 0.0012301086 3.848278 1.449688e-04 0.016694491 0.262257022 -2.825001 ## 2 0.0032893251 -9.725509 1.649012e-03 0.000000000 0.907355964 -19.142102 ## 3 0.0029640572 3.185458 5.003924e-04 0.000000000 0.593114074 -9.728741 ## 4 0.0027690442 -3.396506 1.122414e-03 0.000000000 0.707048447 -12.940875 ## 5 0.0003281521 7.156572 -2.081018e-04 0.868113523 0.009161382 4.472516 ## 6 0.0004318379 8.727162 -2.572925e-04 0.684474124 0.026855053 4.822713 ## 7 0.0005716588 5.538235 -8.999974e-05 0.110183639 0.089233973 1.818359 ## 8 0.0009552189 3.041723 2.044787e-04 0.003338898 0.209366110 -1.178793 ## intercept.lower intercept.upper lower upper slope.percent ## 1 3.848278 -8.8266663 0.05291362 0.4489896 5.7980099 ## 2 -9.725509 -28.7157702 0.60188926 1.2006037 14.4454302 ## 3 3.185458 -24.8702270 0.18264323 1.0818809 8.6085473 ## 4 -3.396506 -22.4192235 0.40968128 1.0107011 10.2917646 ## 5 7.156572 1.0744367 -0.07595715 0.1197755 0.1937191 ## 6 8.727162 0.8230425 -0.09391177 0.1576208 0.4816904 ## 7 5.538235 -1.9192744 -0.03284991 0.2086555 2.0662607 ## 8 3.041723 -5.6090971 0.07463472 0.3486549 4.4665024 ## lower.percent upper.percent ## 1 0.9925881 11.961401 ## 2 8.4310805 24.381910 ## 3 2.1997335 19.875875 ## 4 5.0687904 17.131133 ## 5 -1.5105886 2.703463 ## 6 -1.5405900 3.008348 ## 7 -0.7113745 5.313247 ## 8 1.4540385 8.381275 In the results above the lower and upper fields provide the 95% (or chosen confidence interval using the alpha option) of the trend and slope is the trend estimate expressed in units/year. References "],
["sec-smoothTrend.html", "Section17 Smooth trends 17.1 Background 17.2 Examples of trend analysis 17.3 Seasonal averages", " Section17 Smooth trends 17.1 Background The smoothTrend function calculates smooth trends in the monthly mean concentrations of pollutants. In its basic use it will generate a plot of monthly concentrations and fit a smooth line to the data and show the 95% confidence intervals of the fit. The smooth line is essentially determined using Generalized Additive Modelling using the mgcv package. This package provides a comprehensive and powerful set of methods for modelling data. In this case, however, the model is a relationship between time and pollutant concentration i.e. a trend. One of the principal advantages of this approach is that the amount of smoothness in the trend is optimised in the sense that it is neither too smooth (therefore missing important features) nor too variable (perhaps fitting ‘noise’ rather than real effects). Some background information on the use of this approach in an air quality setting can be found in Carslaw, Beevers, and Tate (2007). Section ?? considers smooth trends in more detail and considers how different models can be developed that can be quite sophisticated. Readers should consider this section if they are considering trend analysis in more depth. The user can select to deseasonalise the data first to provide a clearer indication of the overall trend on a monthly basis. The data are deseasonalised using the stl function. The user may also select to use bootstrap simulations to provide an alternative method of estimating the uncertainties in the trend. In addition, the simulated estimates of uncertainty can account for autocorrelation in the residuals using a block bootstrap approach. 17.2 Examples of trend analysis We apply the function to concentrations of O3 and NO2 using the code below. The first plot shows the smooth trend in raw O3 concentrations, which shows a very clear seasonal cycle. By removing the seasonal cycle of O3, a better indication of the trend is given, shown in the second plot. Removing the seasonal cycle is more effective for pollutants (or locations) where the seasonal cycle is stronger e.g. for ozone and background sites. Figure #ref(fig:smoothTrends) shows the results of the simulations for NO2 without the seasonal cycle removed. It is clear from this plot that there is little evidence of a seasonal cycle. The principal advantage of the smoothing approach compared with the Theil-Sen method is also clearly shown in this plot. Concentrations of NO2 first decrease, then increase strongly. The trend is therefore not monotonic, violating the Theil-Sen assumptions. Finally, the last plot shows the effects of first deaseasonalising the data: in this case with little effect. smoothTrend(mydata, pollutant = &quot;o3&quot;, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean o3&quot;) smoothTrend(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean deseasonalised o3&quot;) smoothTrend(mydata, pollutant = &quot;no2&quot;, simulate = TRUE, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean no2 (bootstrap uncertainties)&quot;) ## [1] &quot;Taking bootstrap samples. Please wait...&quot; smoothTrend(mydata, pollutant = &quot;no2&quot;, deseason = TRUE, simulate =TRUE, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean deseasonalised no2 (bootstrap uncertainties)&quot;) ## [1] &quot;Taking bootstrap samples. Please wait...&quot; Figure 17.1: Examples of the smoothTrend function applied to Marylebone Road The smoothTrend function share many of the functionalities of the TheilSen function. Figure 17.2 shows the result of applying this function to O3 concentrations. The code that produced Figure 17.2 was: smoothTrend(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, type = &quot;wd&quot;) Figure 17.2: Trends in O3 using the smoothTrend function applied to Marylebone Road. The shading shows the estimated 95% confidence intervals. The smoothTrend function can easily be used to gain a large amount of information on trends easily. For example, how do trends in NO2, O3 and PM10 vary by season and wind sector. There are 8 wind sectors and four seasons i.e. 32 plots. In Figure 17.3 all three pollutants are chosen and two types (season and wind direction). We also reduce the number of axis labels and the line to improve clarity. There are numerous combinations of analyses that could be produced here and it is very easy to explore the data in a wide number of ways. smoothTrend(mydata, pollutant = c(&quot;no2&quot;, &quot;pm10&quot;, &quot;o3&quot;), type = c(&quot;wd&quot;, &quot;season&quot;), date.breaks = 3, lty = 0) Figure 17.3: The smoothTrend function applied to three pollutants, split by wind sector and season. 17.3 Seasonal averages If the interest is in considering seasonal trends, it makes sense to set the averaging time to ‘season’ and also show each season in a separate panel. In Figure 17.4 some data is imported for O3 from Lullington Heath on the south coast of England. This plot shows there is strongest evidence for an increase in O3 concentrations during springtime from about 2012 onwards. A probable reason for this increase is a general reduction in NOx concentrations. lh &lt;- importAURN(site = &quot;lh&quot;, year = 2000:2019) smoothTrend(lh, pollutant = &quot;o3&quot;, avg.time = &quot;season&quot;, type = &quot;season&quot;, date.breaks = 4) Figure 17.4: Trends in O3 by season at the Lullington Heath site with a seasonal averaging time and panel for each season. References "],
["sec-trajPlot.html", "Section18 Trajectory analysis 18.1 Plotting trajectories 18.2 Trajectory gridded frequencies 18.3 Trajectory source contribution functions 18.4 Potential Source Contribution Function (PSCF) 18.5 Concentration Weighted Trajectory (CWT) 18.6 Trajectory clustering", " Section18 Trajectory analysis Back trajectories are extremely useful in air pollution and can provide important information on air mass origins. Despite the clear usefulness of back trajectories, their use tends to be restricted to the research community. Back trajectories are used for many purposes from understanding the origins of air masses over a few days to undertaking longer term analyses. They are often used to filter air mass origins to allow for more refined analyses of air pollution — for example trends in concentration by air mass origin. They are often also combined with more sophisticated analyses such as cluster analysis to help group similar type of air mass by origin. Perhaps one of the reasons why back trajectory analysis is not carried out more often is that it can be time consuming to do. This is particularly so if one wants to consider several years at several sites. It can also be difficult to access back trajectory data. In an attempt to overcome some of these issues and expand the possibilities for data analysis, openair makes several functions available to access and analyse pre-calculated back trajectories. Currently these functions allow for the import of pre-calculated back trajectories are several pre-define locations and some trajectory plotting functions. In time all of these functions will be developed to allow more sophisticated analyses to be undertaken. Also it should be recognised that these functions are in their early stages of development and will may continue to change and be refined. The importTraj function imports pre-calculated back trajectories using the Hysplit trajectory model Hybrid Single Particle Lagrangian Integrated Trajectory Model. Trajectories are run at 3-hour intervals and stored in yearly files (see below). The trajectories are started at ground-level (10m) and propagated backwards in time. The data are stored on web-servers at Ricardo Energy &amp; Environment similar to that for importAURN, which makes it very easy to import pre-processed trajectory data for a range of locations and years.5 Users may for various reasons wish to run Hysplit themselves e.g. for different starting heights, longer periods or more locations. Code and instructions have been provided in Section B for users wishing to do this. Users can also use different means of calculating back trajectories e.g. ECMWF and plot them in openair provided a few basic fields are present: date (POSIXct), lat (decimal latitude), lon (decimal longitude) and hour.inc the hour offset from the arrival date (i.e. from zero decreasing to the length of the back trajectories). See ?importTraj for more details. These trajectories have been calculated using the Global NOAA-NCEP/NCAR reanalysis data archives. The global data are on a latitude-longitude grid (2.5 degree). Note that there are many meteorological data sets that can be used to run Hysplit e.g. including ECMWF data. However, in order to make it practicable to run and store trajectories for many years and sites, the NOAA-NCEP/NCAR reanalysis data is most useful. In addition, these archives are available for use widely, which is not the case for many other data sets e.g. ECMWF. Hysplit calculated trajectories based on archive data may be distributed without permission (see https://ready.arl.noaa.gov/HYSPLIT_agreement.php). For those wanting, for example, to consider higher resolution meteorological data sets it may be better to run the trajectories separately. openair uses the mapproj package to allow users to user different map projections. By default, the projection used is Lambert conformal, which is a conic projection best used for mid-latitude areas. The Hysplit model itself will use any one of three different projections depending on the latitude of the origin. If the latitude greater than 55.0 (or less than -55.0) then a polar stereographic projection is used, if the latitude greater than -25.0 and less than 25.0 the mercator projection is used and elsewhere (the mid-latitudes) the Lambert projection. All these projections (and many others) are available in the mapproj package. Users should see the help file for importTraj to get an up to date list of receptors where back trajectories have been calculated. First, the packages are loaded that are needed. library(openair) library(tidyverse) library(lubridate) As an example, we will import trajectories for London in 2010. Importing them is easy: traj &lt;- importTraj(site = &quot;london&quot;, year = 2010) The file itself contains lots of information that is of use for plotting back trajectories: head(traj) ## receptor year month day hour hour.inc lat lon height pressure ## 1 1 2010 1 1 9 0 51.500 -0.100 10.0 994.7 ## 2 1 2010 1 1 8 -1 51.766 0.057 10.3 994.9 ## 3 1 2010 1 1 7 -2 52.030 0.250 10.5 995.0 ## 4 1 2010 1 1 6 -3 52.295 0.488 10.8 995.0 ## 5 1 2010 1 1 5 -4 52.554 0.767 11.0 995.4 ## 6 1 2010 1 1 4 -5 52.797 1.065 11.3 995.6 ## date2 date ## 1 2010-01-01 09:00:00 2010-01-01 09:00:00 ## 2 2010-01-01 08:00:00 2010-01-01 09:00:00 ## 3 2010-01-01 07:00:00 2010-01-01 09:00:00 ## 4 2010-01-01 06:00:00 2010-01-01 09:00:00 ## 5 2010-01-01 05:00:00 2010-01-01 09:00:00 ## 6 2010-01-01 04:00:00 2010-01-01 09:00:00 The traj data frame contains among other things the latitude and longitude of the back trajectory, the height (m) and pressure (Pa) of the trajectory. The date field is the arrival time of the air-mass and is useful for linking with ambient measurement data. The trajPlot function is used for plotting back trajectory lines and density plots and has the following options: 18.1 Plotting trajectories Next, we consider how to plot back trajectories with a few simple examples. The first example will consider a potentially interesting period when the Icelandic volcano, Eyjafjallajokull erupted in April 2010. The eruption of Eyjafjallajokull resulted in a flight-ban that lasted six days across many European airports. In Figure 18.1 selectByDate is used to consider the 7 days of interest and we choose to plot the back trajectories as lines rather than points (the default). Figure 18.1 does indeed show that many of the back trajectories originated from Iceland over this period. Note also the plot automatically includes a world base map. The base map itself is not at very high resolution by default but is useful for the sorts of spatial scales that back trajectories exist over. The base map is also global, so provided that there are pre-calculated back trajectories, these maps can be generated anywhere in the world. By default the function uses the ‘world’ map from the maps package. If map.res = \"hires\" then the (much) more detailed base map worldHires from the mapdata package is used.6 trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end =&quot;21/4/2010&quot;), map.cols = openColours(&quot;hue&quot;, 10), col = &quot;grey30&quot;) ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map Figure 18.1: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010. Note the additional option to vary the country colours using map.cols. By default the map colours for all countries are grey. Note that trajPlot will only plot full length trajectories. This can be important when plotting something like a single month e.g. by using selectByDate when on partial sections of some trajectories may be selected. There are a few other ways of representing the data shown in Figure 18.1. For example, it might be useful to plot the trajectories for each day. To do this we need to make a new column day which can be used in the plotting. The first example considers plotting the back trajectories in separate panels Figure (18.2. ## make a day column traj$day &lt;- as.Date(traj$date) ## plot it choosing a specfic layout trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), type = &quot;day&quot;, layout = c(7, 1)) Figure 18.2: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, shown separately for each day. Another way of plotting the data is to group the trajectories by day and colour them. This time we also set a few other options to get the layout we want — shown in Figure 18.3. trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), group = &quot;day&quot;, col = &quot;jet&quot;, lwd = 2, key.pos = &quot;top&quot;, key.col = 4) Figure 18.3: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, shown grouped for each day and coloured accordingly. So far the plots have provided information on where the back trajectories come from, grouped or split by day. It is also possible, in common with most other openair functions to split the trajectories by many other variables e.g. month, season and so on. However, perhaps one of the most useful approaches is to link the back trajectories with the concentrations of a pollutant. As mentioned previously, the back trajectory data has a column date representing the arrival time of the air mass that can be used to link with concentration measurements. A couple of steps are required to do this using the left_join function. ## import data for North Kensington kc1 &lt;- importAURN(&quot;kc1&quot;, year = 2010) # now merge with trajectory data by &#39;date&#39; traj &lt;- left_join(traj, kc1, by = &quot;date&quot;) ## look at first few lines head(traj) ## receptor year month day hour hour.inc lat lon height pressure ## 1 1 2010 1 2010-01-01 9 0 51.500 -0.100 10.0 994.7 ## 2 1 2010 1 2010-01-01 8 -1 51.766 0.057 10.3 994.9 ## 3 1 2010 1 2010-01-01 7 -2 52.030 0.250 10.5 995.0 ## 4 1 2010 1 2010-01-01 6 -3 52.295 0.488 10.8 995.0 ## 5 1 2010 1 2010-01-01 5 -4 52.554 0.767 11.0 995.4 ## 6 1 2010 1 2010-01-01 4 -5 52.797 1.065 11.3 995.6 ## date2 date site code co nox no2 ## 1 2010-01-01 09:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 2 2010-01-01 08:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 3 2010-01-01 07:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 4 2010-01-01 06:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 5 2010-01-01 05:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 6 2010-01-01 04:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## no o3 so2 pm10 pm2.5 v10 v2.5 nv10 nv2.5 ws wd air_temp ## 1 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 2 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 3 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 4 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 5 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 6 6 46 0 8 NA 0 NA 8 NA NA NA NA This time we can use the option pollutant in the function trajPlot, which will plot the back trajectories coloured by the concentration of a pollutant. Figure 18.4 does seem to show elevated PM10 concentrations originating from Iceland over the period of interest. In fact, these elevated concentrations occur on two days as shown in Figure 18.2. However, care is needed when interpreting such data because other analysis would need to rule out other reasons why PM10 could be elevated; in particular due to local sources of PM10. There are lots of openair functions that can help here e.g. timeVariation or timePlot to see if NOx concentrations were also elevated (which they seem to be). It would also be worth considering other sites for back trajectories that could be less influenced by local emissions. trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), pollutant = &quot;pm10&quot;, col = &quot;jet&quot;, lwd =2) Figure 18.4: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, coloured by the concentration of PM10 in \\(\\mu\\)g m-3. However, it is possible to account for the PM that is local to some extent by considering the relationship between NOx and PM10 (or PM2.5). For example, using scatterPlot (not shown): scatterPlot(kc1, x = &quot;nox&quot;, y = &quot;pm2.5&quot;, avg = &quot;day&quot;, linear = TRUE) which suggests a gradient of 0.084. Therefore, we can remove the PM10 that is associated NOx in kc1 data, making a new column pm.new: kc1 &lt;- mutate(kc1, pm.new = pm10 - 0.084 * nox) We have already merged kc1 with traj, so to keep things simple we import traj again and merge it with kc1. Note that if we had thought of this initially, pm.new would have been calculated first before merging with traj. traj &lt;- importTraj(site = &quot;london&quot;, year = 2010) traj &lt;- left_join(traj, kc1, by = &quot;date&quot;) Now it is possible to plot the trajectories: trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), pollutant = &quot;pm.new&quot;, col = &quot;jet&quot;, lwd = 2) Which, interestingly still clearly shows elevated PM10 concentrations for those two days that cross Iceland. The same is also true for PM2.5. However, as mentioned previously, checking other sites in more rural areas would be a good idea. 18.2 Trajectory gridded frequencies The Hysplit model itself contains various analysis options for gridding trajectory data. Similar capabilities are also available in openair where the analyses can be extended using other openair capabilities. It is useful to gain an idea of where trajectories come from. Over the course of a year representing trajectories as lines or points results in a lot of over-plotting. Therefore it is useful to grid the trajectory data and calculate various statistics by considering latitude-longitude intervals. The first analysis considers the number of unique trajectories in a particular grid square. This is achieved by using the trajLevel function and setting the statistic option to “frequency”. Figure 18.5 shows the frequency of back trajectory crossings for the North Kensington data. In this case it highlights that most trajectory origins are from the west and north for 2010 at this site. Note that in this case, pollutant can just be the trajectory height (or another numeric field) rather than an actual pollutant because only the frequencies are considered. trajLevel(traj, statistic = &quot;frequency&quot;) Figure 18.5: Gridded back trajectory frequencies. The border = NA option removes the border around each grid cell. It is also possible to use hexagonal binning to gain an idea about trajectory frequencies. In this case each 3-hour point along each trajectory is used in the counting. The code below focuses more on Europe and uses the hexagonal binning method. Note that the effect of the very few high number of points at the origin has been diminished by plotting the data on a log scale — see Section 14.2.1 for details. trajLevel(subset(traj, lat &gt; 30 &amp; lat &lt; 70 &amp; lon &gt; -30 &amp; lon &lt; 20), method = &quot;hexbin&quot;, col = &quot;jet&quot;, xbin = 40) Figure 18.6: Gridded back trajectory frequencies with hexagonal binning. 18.3 Trajectory source contribution functions Back trajectories offer the possibility to undertake receptor modelling to identify the location of major emission sources. When many back trajectories (over months to years) are analysed in specific ways they begin to show the geographic origin most associated with elevated concentrations. With enough (dissimilar) trajectories those locations leading to the highest concentrations begin to be revealed. When a whole year of back trajectory data is plotted the individual back trajectories can extend 1000s of km. There are many approaches using back trajectories in this way and Fleming, Monks, and Manning (2012) provide a good overview of the methods available. openair has implemented a few of these techniques and over time these will be refined and extended. 18.3.1 Identifying the contribution of high concentration back trajectories A useful analysis to undertake is to consider the pattern of frequencies for two different conditions. In particular, there is often interest in the origin of high concentrations for different pollutants. For example, compared with data over a whole year, how do the frequencies of occurrence differ? Figure 18.7 shows an example of such an analysis for PM10 concentrations. By default the function will compare concentrations $&gt;$90th percentile with the full year. The percentile level is controlled by the option percentile. Note also there is an option min.bin that will exclude grid cells where there are fewer than min.bin data points. The analysis compares the percentage of time the air masses are in particular grid squares for all data and a subset of data where the concentrations are greater than the given percentile. The graph shows the absolute percentage difference between the two cases i.e. high minus base. Figure 18.7 shows that compared with the whole year, high PM10 concentrations ($&gt;$90th percentile) are more prevalent when the trajectories originate from the east, which is seen by the positive values in the plot. Similarly there are relatively fewer occurrences of these high concentration back trajectories when they originate from the west. This analysis is in keeping with the highest PM10 concentrations being largely controlled by secondary aerosol formation from air-masses originating during anticyclonic conditions from mainland Europe. trajLevel(traj, pollutant = &quot;pm10&quot;, statistic = &quot;difference&quot;, col = c(&quot;skyblue&quot;, &quot;white&quot;, &quot;tomato&quot;), min.bin = 50, border = NA, xlim = c(-20, 20), ylim = c(40, 70)) Figure 18.7: Gridded back trajectory frequencies showing the percentage difference in occurrence for high PM10 concentrations (90th percentile) compared with conditions over the full year. Note that it is also possible to use conditioning with these plots. For example to split the frequency results by season: trajLevel(traj, pollutant = &quot;pm10&quot;, statistic = &quot;frequency&quot;, col = &quot;heat&quot;, type = &quot;season&quot;) 18.3.2 Allocating trajectories to different wind sectors One of the key aspects of trajectory analysis is knowing something about where air masses have come from. Cluster analysis can be used to group trajectories based on their origins and this is discussed in Section 18.6. A simple approach is to consider different wind sectors e.g. N, NE, E and calculate the proportion of time a particular back trajectory resides in a specific sector. It is then possible to allocate a particular trajectory to a sector based on some assumption about the proportion of time it is in that sector — for example, assume a trajectory is from the west sector if it spends at least 50% of its time in that sector or otherwise record the allocation as ‘unallocated’. The code below can be used as the basis of such an approach. First we import the trajectories, which in this case are for London in 2010: traj &lt;- importTraj(site = &quot;london&quot;, year = 2010) alloc &lt;- traj id &lt;- which(alloc$hour.inc == 0) y0 &lt;- alloc$lat[id[1]] x0 &lt;- alloc$lon[id[1]] ## calculate angle and then assign sector alloc &lt;- mutate( alloc, angle = atan2(lon - x0, lat - y0) * 360 / 2 / pi, angle = ifelse(angle &lt; 0, angle + 360 , angle), sector = as.character(cut(angle, breaks = seq(22.5, 382.5, 45), labels = c(&quot;NE&quot;, &quot;E&quot;, &quot;SE&quot;, &quot;S&quot;, &quot;SW&quot;, &quot;W&quot;, &quot;NW&quot;, &quot;N&quot;))), sector = ifelse(is.na(sector), &quot;N&quot;, sector) ) alloc &lt;- group_by(alloc, date, sector) %&gt;% mutate(n = n()) %&gt;% group_by(date) %&gt;% arrange(date, n) %&gt;% slice_tail(1) %&gt;% mutate(sector = ifelse(n &gt; 50, sector, &quot;unallocated&quot;)) %&gt;% select(date, sector, n) # combine with trajectories traj &lt;- left_join(traj, alloc, by = &quot;date&quot;) Now it is possible to post-process the data. traj now has the angle, sector and allocation (sector). head(traj) ## date receptor year month day hour hour.inc lat lon ## 1 2010-01-01 09:00:00 1 2010 1 1 9 0 51.500 -0.100 ## 2 2010-01-01 09:00:00 1 2010 1 1 8 -1 51.766 0.057 ## 3 2010-01-01 09:00:00 1 2010 1 1 7 -2 52.030 0.250 ## 4 2010-01-01 09:00:00 1 2010 1 1 6 -3 52.295 0.488 ## 5 2010-01-01 09:00:00 1 2010 1 1 5 -4 52.554 0.767 ## 6 2010-01-01 09:00:00 1 2010 1 1 4 -5 52.797 1.065 ## height pressure date2 angle sector sec ## 1 10.0 994.7 2010-01-01 09:00:00 0.00000 N unallocated ## 2 10.3 994.9 2010-01-01 08:00:00 30.55019 NE unallocated ## 3 10.5 995.0 2010-01-01 07:00:00 33.43987 NE unallocated ## 4 10.8 995.0 2010-01-01 06:00:00 36.48747 NE unallocated ## 5 11.0 995.4 2010-01-01 05:00:00 39.44005 NE unallocated ## 6 11.3 995.6 2010-01-01 04:00:00 41.93103 NE unallocated First, merge the air quality data from North Kensington: traj &lt;- left_join(traj, kc1, by = &quot;date&quot;) We can work out the mean concentration by allocation, which shows the clear importance for the east and south-east sectors. group_by(traj, sector) %&gt;% summarise(PM2.5 = mean(pm2.5, na.rm = TRUE)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 2 ## sector PM2.5 ## &lt;fct&gt; &lt;dbl&gt; ## 1 NE 14.0 ## 2 E 20.8 ## 3 SE 22.8 ## 4 S 14.9 ## 5 SW 11.9 ## 6 W 12.1 ## 7 NW 13.8 ## 8 N 13.6 Finally, the percentage of the year in each sector can be calculated as follows: group_by(traj, sector) %&gt;% summarise(n = n()) %&gt;% mutate(percent = 100 * n / nrow(traj)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 3 ## sector n percent ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 NE 40700 14.5 ## 2 E 24947 8.86 ## 3 SE 10900 3.87 ## 4 S 12599 4.47 ## 5 SW 36183 12.9 ## 6 W 83535 29.7 ## 7 NW 40308 14.3 ## 8 N 32375 11.5 18.4 Potential Source Contribution Function (PSCF) If statistic = \"pscf\" then the Potential Source Contribution Function (PSCF) is plotted. The PSCF calculates the probability that a source is located at latitude \\(i\\) and longitude \\(j\\) (Fleming, Monks, and Manning 2012; Pekney et al. 2006). The PSCF is somewhat analogous to the CPF function described on Section 7.3 that considers local wind direction probabilities. In fact, the two approaches have been shown to work well together (Pekney et al. 2006). The PSCF approach has been widely used in the analysis of air mass back trajectories. Ara Begum et al. (2005) for example assessed the method against the known locations of wildfires and found it performed well for PM2.5, EC (elemental carbon) and OC (organic carbon) and that other (non-fire related) species such as sulphate had different source origins. The basis of PSCF is that if a source is located at (\\(i\\), \\(j\\)), an air parcel back trajectory passing through that location indicates that material from the source can be collected and transported along the trajectory to the receptor site. PSCF solves \\[\\begin{equation} PSCF = \\frac{m_{ij}}{n_{ij}} \\tag{18.1} \\end{equation}\\] where \\(n_{ij}\\) is the number of times that the trajectories passed through the cell (\\(i\\), \\(j\\)) and \\(m_{ij}\\) is the number of times that a source concentration was high when the trajectories passed through the cell (\\(i\\), \\(j\\)). The criterion for determining \\(m_{ij}\\) is controlled by `percentile}, which by default is 90. Note also that cells with few data have a weighting factor applied to reduce their effect. An example of a PSCF plot is shown in Figure 18.8 for PM2.5 for concentrations &gt;90th percentile. This Figure gives a very clear indication that the principal (high) sources are dominated by source origins in mainland Europe — particularly around the Benelux countries. trajLevel(filter(traj, lon &gt; -20, lon &lt; 20, lat &gt; 45, lat &lt; 60), pollutant = &quot;pm2.5&quot;, statistic = &quot;pscf&quot;, col = &quot;increment&quot;, border = NA) Figure 18.8: PSCF probabilities for PM2.5 concentrations (90th percentile). 18.5 Concentration Weighted Trajectory (CWT) A limitation of the PSCF method is that grid cells can have the same PSCF value when sample concentrations are either only slightly higher or much higher than the criterion (Hsu, Holsen, and Hopke 2003). As a result, it can be difficult to distinguish moderate sources from strong ones. Seibert et al. (1994) computed concentration fields to identify source areas of pollutants. This approach is sometimes referred to as the CWT or CF (concentration field). A grid domain was used as in the PSCF method. For each grid cell, the mean (CWT) or logarithmic mean (used in the Residence Time Weighted Concentration (RTWC) method) concentration of a pollutant species was calculated as follows: \\[\\begin{equation} ln(\\overline{C}_{ij}) = \\frac{1}{\\sum_{k=1}^{N}\\tau_{ijk}}\\sum_{k=1}^{N}ln(c_k)\\tau_{ijk} \\tag{18.2} \\end{equation}\\] where \\(i\\) and \\(j\\) are the indices of grid, \\(k\\) the index of trajectory, \\(N\\) the total number of trajectories used in analysis, \\(c_k\\) the pollutant concentration measured upon arrival of trajectory \\(k\\), and \\(\\tau_{ijk}\\) the residence time of trajectory \\(k\\) in grid cell (\\(i\\), \\(j\\)). A high value of \\(\\overline{C}_{ij}\\) means that, air parcels passing over cell (\\(i\\), \\(j\\)) would, on average, cause high concentrations at the receptor site. Figure 18.9 shows the situation for PM2.5 concentrations. It was calculated by recording the associated PM2.5 concentration for each point on the back trajectory based on the arrival time concentration using 2010 data. The plot shows the geographic areas most strongly associated with high PM2.5 concentrations i.e. to the east in continental Europe. Both the CWT and PSCF methods have been shown to give similar results and each have their advantages and disadvantages (Lupu and Maenhaut 2002; Hsu, Holsen, and Hopke 2003). Figure 18.9 can be compared with Figure 18.8 to compare the overall identification of source regions using the CWT and PSCF techniques. Overall the agreement is good in that similar geographic locations are identified as being important for PM2.5. trajLevel(filter(traj,lon &gt; -20, lon &lt; 20, lat &gt; 45, lat &lt; 60), pollutant = &quot;pm2.5&quot;, statistic=&quot;cwt&quot;, col = &quot;increment&quot;, border = &quot;white&quot;) Figure 18.9: Gridded back trajectory concentrations showing mean PM2.5 concentrations using the CWT approach. Figure 18.9 is useful, but it can be clearer if the trajectory surface is smoothed, which has been done for PM2.5 concentrations shown in Figure 18.10. trajLevel(subset(traj, lat &gt; 45 &amp; lat &lt; 60 &amp; lon &gt;-20 &amp; lon &lt;20), pollutant =&quot;pm2.5&quot;, statistic = &quot;cwt&quot;, smooth = TRUE, col = &quot;increment&quot;) Figure 18.10: Gridded and smoothed back trajectory concentrations showing mean PM2.5 concentrations using the CWT approach. In common with most other openair functions, the flexible type option can be used to split the data in different ways. For example, to plot the smoothed back trajectories for PM2.5 concentrations by season. trajLevel(subset(traj, lat &gt; 40 &amp; lat &lt; 70 &amp; lon &gt;-20 &amp; lon &lt;20), pollutant = &quot;pm2.5&quot;, type = &quot;season&quot;, statistic = &quot;pscf&quot;, layout = c(4, 1)) It should be noted that it makes sense to analyse back trajectories for pollutants that have a large regional component — such as particles or . It makes little sense to analyse pollutants that are known to have local impacts e.g. . However, a species such as can be helpful to exclude `fresh’ emissions from the analysis. 18.6 Trajectory clustering Often it is useful to use cluster analysis on back trajectories to group similar air mass origins together. The principal purpose of clustering back trajectories is to post-process data according to cluster origin. By grouping data with similar geographic origins it is possible to gain information on pollutant species with similar chemical histories. There are several ways in which clustering can be carried out and several measures of the similarity of different clusters. A key issue is how the distance matrix is calculated, which determines the similarity (or dissimilarity) of different back trajectories. The simplest measure is the Euclidean distance. However, an angle-based measure is also often used. The two distance measures are defined below. In openair the distance matrices are calculated using C\\(++\\) code because their calculation is computationally intensive. Note that these calculations can also be performed directly in the Hysplit model itself. The Euclidean distance between two trajectories is given by Equation (18.3). Where \\(X_1\\), \\(Y_1\\) and \\(X_2\\), \\(Y_2\\) are the latitude and longitude coordinates of back trajectories \\(1\\) and \\(2\\), respectively. \\(n\\) is the number of back trajectory points (96 hours in this case). \\[\\begin{equation} d_{1, 2} = \\left({\\sum_{i=1}^{n} ((X_{1i} - X_{2i}) ^ 2 + (Y_{1i} - Y_{2i})) ^ 2}\\right)^{1/2} \\tag{18.3} \\end{equation}\\] The angle distance matrix is a measure of how similar two back trajectory points are in terms of their angle from the origin i.e. the starting location of the back trajectories. The angle-based measure will often capture some of the important circulatory features in the atmosphere e.g. situations where there is a high pressure located to the east of the UK. However, the most appropriate distance measure will be application dependent and is probably best tested by the extent to which they are able to differentiate different air-mass characteristics, which can be tested through post-processing. The angle-based distance measure is defined as: \\[\\begin{equation} d_{1, 2} = \\frac{1}{n}\\sum_{i=1}^{n}cos^{-1} \\left(0.5\\frac{A_i + B_i + C_i}{\\sqrt{A_iB_i}}\\right) \\tag{18.4} \\end{equation}\\] where \\[\\begin{equation} \\tag{18.5} A_i = (X_1(i) - X_0)^2 + (Y_1(i) - Y_0)^2 \\end{equation}\\] \\[\\begin{equation} \\tag{18.6} B_i = (X_2(i) - X_0)^2 + (Y_2(i) - Y_0)^2 \\end{equation}\\] \\[\\begin{equation} \\tag{18.7} C_i = (X_2(i) - X_1(i))^2 + (Y_2(i) - Y_1(i))^2 \\end{equation}\\] where \\(X_0\\) and \\(Y_0\\) are the coordinates of the location being studied i.e. the starting location of the trajectories. As an example we will consider back trajectories for London in 2011. First, the back trajectory data for London is imported together with the air pollution data for the North Kensington site (KC1). traj &lt;- importTraj(site = &quot;london&quot;, year = 2011) kc1 &lt;- importAURN(site = &quot;kc1&quot;, year = 2011) The clusters are straightforward to calculate. In this case the back trajectory data (traj) is supplied and the angle-based distance matrix is used. Furthermore, we choose to calculate 6 clusters and choose a specific colour scheme. In this case we read the output from trajCluster into a variable clust so that the results can be post-processed. clust &lt;- trajCluster(traj, method = &quot;Angle&quot;, n.cluster = 6, col = &quot;Set2&quot;, map.cols = openColours(&quot;Paired&quot;, 10)) Figure 18.11: The 6-cluster solution to back trajectories calculated for the London North Kensington site for 2011 showing the mean trajectory for each cluster. clust returns all the back trajectory information together with the cluster (as a character). This data can now be used together with other data to analyse results further. However, first it is possible to show all trajectories coloured by cluster, although for a year of data there is significant overlap and it is difficult to tell them apart. trajPlot(clust$data, group = &quot;cluster&quot;) A useful way in which to see where these air masses come from by trajectory is to produce a frequency plot by cluster. Such a plot (not shown, but code below) provides a good indication of the spread of the different trajectory clusters as well as providing an indication of where air masses spend most of their time. For the London 2011 data it can be seen cluster 1 is dominated by air from the European mainland to the south. trajLevel(clust$data, type = &quot;cluster&quot;, col = &quot;increment&quot;, border = NA) Perhaps more useful is to merge the cluster data with measurement data. In this case the data at North Kensington site are used. Note that in merging these two data frames it is not necessary to retain all 96 back trajectory hours and for this reason we extract only the first hour. # use inner join - so only where we have data in each kc1 &lt;- inner_join(kc1, filter(clust$data, hour.inc == 0), by = &quot;date&quot;) Now kc1 contains air pollution data identified by cluster. The size of this data frame is about a third of the original size because back trajectories are only run every 3~hours. The numbers of each cluster are given by: table(kc1[[&quot;cluster&quot;]]) ## ## C1 C2 C3 C4 C5 C6 ## 347 661 989 277 280 333 i.e. is dominated by clusters 3 and 2 from west and south-west (Atlantic). Now it is possible to analyse the concentration data according to the cluster. There are numerous types of analysis that can be carried out with these results, which will depend on what the aims of the analysis are in the first place. However, perhaps one of the first things to consider is how the concentrations vary by cluster. As the summary results below show, there are distinctly different mean concentrations of most pollutants by cluster. For example, clusters 1 and 6 are associated with much higher concentrations of PM10 — approximately double that of other clusters. Both of these clusters originate from continental Europe. Cluster 5 is also relatively high, which tends to come from the rest of the UK. Other clues concerning the types of air-mass can be gained from the mean pressure. For example, cluster~5 is associated with the highest pressure (1014~kPa), and as is seen in Figure 18.11 the shape of the line for cluster~5 is consistent with air-masses associated with a high pressure system (a clockwise-type sweep). group_by(kc1, cluster) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## # A tibble: 7 x 28 ## cluster co nox no2 no o3 so2 pm10 pm2.5 v10 v2.5 nv10 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 C1 0.346 89.9 51.4 25.3 32.1 3.12 38.3 31.3 8.79 8.02 29.5 ## 2 C2 0.205 40.9 30.9 6.55 39.4 1.46 17.2 11.6 4.18 3.13 13.1 ## 3 C3 0.198 47.2 32.3 9.78 39.7 1.84 18.1 11.3 3.73 2.80 14.3 ## 4 C4 0.189 44.1 30.6 8.89 39.7 1.56 17.8 11.0 3.41 2.17 14.4 ## 5 C5 0.202 57.0 39.2 11.7 41.4 2.31 24.3 16.5 5.06 4.50 19.2 ## 6 C6 0.267 64.9 42.4 14.8 46.3 2.93 36.6 29.7 7.78 7.57 28.8 ## 7 &lt;NA&gt; 0.225 53.8 36.2 11.6 39.3 2.06 23.7 16.3 5.20 4.14 18.5 ## # … with 16 more variables: nv2.5 &lt;dbl&gt;, ws &lt;dbl&gt;, wd &lt;dbl&gt;, air_temp &lt;dbl&gt;, ## # receptor &lt;dbl&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, day &lt;dbl&gt;, hour &lt;dbl&gt;, ## # hour.inc &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, height &lt;dbl&gt;, pressure &lt;dbl&gt;, ## # traj_len &lt;dbl&gt;, len &lt;dbl&gt; Simple plots can be generated from these results too. For example, it is easy to consider the temporal nature of the volatile component of PM2.5 concentrations (v2.5 in the kc1 data frame). Figure 18.12 for example shows how the concentration of the volatile component of PM2.5 concentrations varies by cluster by plotting the hour of day-month variation. It is clear from Figure 18.12 that the clusters associated with the highest volatile PM2.5 concentrations are clusters 1 and 6 (European origin) and that these concentrations peak during spring. There is less data to see clearly what is going on with cluster~5. Nevertheless, the cluster analysis has clearly separated different air mass characteristics which allows for more refined analysis of different air-mass types. trendLevel(kc1, pollutant = &quot;v2.5&quot;, type = &quot;cluster&quot;, layout = c(6, 1), cols = &quot;increment&quot;) Figure 18.12: Some of the temporal characteristics of the volatile PM2.5 component plotted by month and hour of the day and by cluster for the London North Kensington site for 2011. Similarly, as considered in Figure 8.6, the timeVariation function can also be used to consider the temporal components. Another useful plot to consider is timeProp (see Section 12), which can show how the concentration of a pollutant is comprised. In this case it is useful to plot the time series of PM2.5 and show how much of the concentration is contributed to by each cluster. Such a plot is shown in Figure 18.13. It is now easy to see for example that during the spring months many of the high concentration events were due to clusters 1 and 6, which correspond to European origin air-masses as shown in Figure 18.11. timeProp(kc1, pollutant = &quot;pm2.5&quot;, avg.time = &quot;day&quot;, proportion = &quot;cluster&quot;, cols = &quot;Set2&quot;, key.position = &quot;top&quot;, key.columns = 6) Figure 18.13: Temporal variation in daily PM2.5 concentrations at the North Kensington site show by contribution of each cluster. References "],
["sec-conditionalQuantile.html", "Section19 Conditional quantiles 19.1 Background 19.2 Conditional evaluation", " Section19 Conditional quantiles 19.1 Background Conditional quantiles are a very useful way of considering model performance against observations for continuous measurements (Wilks 2005). The conditional quantile plot splits the data into evenly spaced bins. For each predicted value bin e.g. from 0 to 10 ppb the corresponding values of the observations are identified and the median, 25/75th and 10/90 percentile (quantile) calculated for that bin. The data are plotted to show how these values vary across all bins. For a time series of observations and predictions that agree precisely the median value of the predictions will equal that for the observations for each bin. The conditional quantile plot differs from the quantile-quantile plot (Q-Q plot) that is often used to compare observations and predictions. A Q-Q~plot separately considers the distributions of observations and predictions, whereas the conditional quantile uses the corresponding observations for a particular interval in the predictions. Take as an example two time series, the first a series of real observations and the second a lagged time series of the same observations representing the predictions. These two time series will have identical (or very nearly identical) distributions (e.g. same median, minimum and maximum). A Q-Q plot would show a straight line showing perfect agreement, whereas the conditional quantile will not. This is because in any interval of the predictions the corresponding observations now have different values. Plotting the data in this way shows how well predictions agree with observations and can help reveal many useful characteristics of how well model predictions agree with observations — across the full distribution of values. A single plot can therefore convey a considerable amount of information concerning model performance. The basic function is considerably enhanced by allowing flexible conditioning easily e.g. to evaluate model performance by season, day of the week and so on, as in other openair functions. To make things more interesting we will use data from a model evaluation exercise organised by Defra in 2010/2011. Many models were evaluated but we only consider hourly ozone predictions from the CMAQ model being used at King’s College London. First the data are loaded: load(&quot;~/My Drive/openair/Data/CMAQozone.RData&quot;) class(CMAQ.KCL$date) &lt;- c(&quot;POSIXct&quot;, &quot;POSIXt&quot;) head(CMAQ.KCL) ## site date o3 rollingO3Meas mod rollingO3Mod group ## 1 Aston.Hill 2006-01-01 00:00:00 NA NA 92.80 NA CMAQ.KCL ## 2 Aston.Hill 2006-01-01 01:00:00 74 NA 92.18 NA CMAQ.KCL ## 3 Aston.Hill 2006-01-01 02:00:00 72 NA 92.14 NA CMAQ.KCL ## 4 Aston.Hill 2006-01-01 03:00:00 72 NA 91.72 NA CMAQ.KCL ## 5 Aston.Hill 2006-01-01 04:00:00 70 NA 91.50 NA CMAQ.KCL ## 6 Aston.Hill 2006-01-01 05:00:00 66 NA 92.28 NA CMAQ.KCL The data consists of hourly observations of O3 in \\(\\mu\\)g m-3 at 15 rural O3 sites in the UK together with predicted values.7 First, we consider O3 predictions across all sites to help illustrate the purpose of the function. The results are shown in Figure 19.1. An explanation of the Figure is given in its caption. conditionalQuantile(CMAQ.KCL, obs = &quot;o3&quot;, mod = &quot;mod&quot;) Figure 19.1: Example of the use of conditional quantiles applied to the KCL CMAQ model for 15 rural O3 monitoring sites in 2006, for hourly data. The blue line shows the results for a perfect model. In this case the observations cover a range from 0 to 270 \\(\\mu\\)g m-3. The red line shows the median values of the predictions and corresponding observations. The maximum predicted value is 125 \\(\\mu\\)g m-3, somewhat less than the maximum observed value. The shading shows the predicted quantile intervals i.e. the 25/75th and the 10/90th. A perfect model would lie on the blue line and have a very narrow spread. There is still some spread because even for a perfect model a specific quantile interval will contain a range of values. However, for the number of bins used in this plot the spread will be very narrow. Finally, the histogram shows the counts of predicted values. A more informative analysis can be undertaken by considering conditional quantiles separately by site, which is easily done using the type option. The results are shown in Figure 19.2. It is now easier to see where the model performs best and how it varies by site type. For example, at a remote site in Scotland like Strath Vaich it is clear that the model does not capture either the lowest or highest O3 concentrations very well. conditionalQuantile(CMAQ.KCL, obs = &quot;o3&quot;, mod = &quot;mod&quot;, type = &quot;site&quot;) Figure 19.2: Conditional quantiles by site for 15 O3 monitoring sites in the UK. As with other openair functions, the ability to consider conditioning can really help with interpretation. For example, what do the conditional quantiles at Lullington Heath (in south-east England) look like by season? This is easily done by subsetting the data to select that site and setting type = \"season\", as shown in Figure 19.3. These results show that winter predictions have good coverage i.e. with width of the blue `perfect model’ line is the same as the observations. However, the predictions tend to be somewhat lower than observations for most concentrations (the median line is below the blue line) — and the width of the 10/75th and 10/90th percentiles is quite broad. However, the area where the model is less good is in summer and autumn because the predictions have low coverage (the red line only covers less than half of the observation line and the width of the percentiles is wide). Of course, it is also easy to plot by hour of the day, day of the week, by daylight/nighttime and so on — easily. All these approaches can help better understand why a model does not perform very well rather than just quantifying its performance. Also, these types of analysis are particularly useful when more than one model is involved in a comparison as in the recent Defra model evaluation exercise, which we will come back to later when some results are published. conditionalQuantile(subset(CMAQ.KCL, site == &quot;Lullington.Heath&quot;), obs = &quot;o3&quot;, mod = &quot;mod&quot;, type = &quot;season&quot;) Figure 19.3: Conditional quantiles at Lullington Heath conditioned by season. 19.2 Conditional evaluation There are numerous ways in which model performance can be assessed, including the use of common statistical measures described in Section 20. These approaches are very useful for comparing models against observations and other models. However, model developers would generally like to know why a model may have poor performance under some situations. This is a much more challenging issue to address. However, useful information can be gained by considering how other variables vary simultaneously. The conditionalEval function provides information on how other variables vary across the same intervals as shown on the conditional quantile plot. There are two types of variable that can be considered by setting the value of statistic. First, statistic can be another variable in the data frame. In this case the plot will show the different proportions of statistic across the range of predictions. For example statistic = \"season\" will show for each interval of mod the proportion of predictions that were spring, summer, autumn or winter. This is useful because if model performance is worse for example at high concentrations of mod then knowing that these tend to occur during a particular season etc. can be very helpful when trying to understand why a model fails. See Section 22.2 for more details on the types of variable that can be statistic. Another example would be statistic = \"ws\" (if wind speed were available in the data frame), which would then split wind speed into four quantiles and plot the proportions of each. Again, this would help show whether model performance in predicting concentrations of O3 for example is related to low to high wind speed conditions. conditionalEval can also simultaneously plot the model performance of other observed/predicted variable pairs according to different model evaluation statistics. These statistics derive from the Section 20 function and include MB, NMB, r, COE, MGE, NMGE, RMSE and FAC2. More than one statistic can be supplied e.g. statistic = c(\"NMB\", \"COE\"). Bootstrap samples are taken from the corresponding values of other variables to be plotted and their statistics with 95% confidence intervals calculated. In this case, the model performance of other variables is shown across the same intervals of mod, rather than just the values of single variables. In this second case the model would need to provide observed/predicted pairs of other variables. For example, a model may provide predictions of NOx and wind speed (for which there are also observations available). The conditionalEval function will show how well these other variables are predicted for the same prediction intervals of the main variable assessed in the conditional quantile plot e.g. ozone. In this case, values are supplied to var.obs (observed values for other variables) and var.mod (modelled values for other variables). For example, to consider how well the model predicts NOx and wind speed var.obs = c(\"nox.obs\", \"ws.obs\") and var.mod = c(\"nox.mod\", \"ws.mod\") would be supplied (assuming nox.obs, nox.mod, ws.obs, ws.mod are present in the data frame). The analysis could show for example, when ozone concentrations are under-predicted, the model may also be shown to over-predict concentrations of NOx at the same time, or under-predict wind speeds. Such information can thus help identify the underlying causes of poor model performance. For example, an under-prediction in wind speed could result in higher surface NOx concentrations and lower ozone concentrations. Similarly if wind speed predictions were good and NOx was over predicted it might suggest an over-estimate of NOx emissions. One or more additional variables can be plotted. A special case is statistic = \"cluster\". In this case a data frame is provided that contains the cluster calculated by trajCluster and importTraj. Alternatively users could supply their own pre-calculated clusters. These calculations can be very useful in showing whether certain back trajectory clusters are associated with poor (or good) model performance. Note that in the case of statistic = \"cluster\" there will be fewer data points used in the analysis compared with the ordinary statistics above because the trajectories are available for every three hours. Also note that statistic = \"cluster\" cannot be used together with the ordinary model evaluation statistics such as MB. The output will be a bar chart showing the proportion of each interval of mod by cluster number. Far more insight can be gained into model performance through conditioning using type. For example, type = \"season\" will plot conditional quantiles and the associated model performance statistics of other variables by each season. type can also be a factor or character field e.g. representing different models used. As an example, similar data to that described above from CMAQ have been used as an example. A subset of the data for the North Kensington site can be imported as shown below. condDat &lt;- readRDS(url(&quot;https://davidcarslaw.github.io/data/openair/condDat.rds&quot;)) The file contains observed and modelled hourly values for O3, NOx, wind speed, wind direction, temperature and relative humidity. head(condDat) ## date O3.obs NOx.obs ws.obs wd.obs temp.obs rh.obs O3.mod ## 5 2006-01-01 00:00:00 10 29.43665 4.6296 190 4.9 89 14.80 ## 10 2006-01-01 01:00:00 15 17.55393 NA 210 5.1 90 17.46 ## 15 2006-01-01 02:00:00 11 19.64817 2.5720 220 4.9 94 18.31 ## 20 2006-01-01 03:00:00 11 19.15393 3.6008 270 5.7 91 18.25 ## 25 2006-01-01 04:00:00 11 17.03037 3.0864 270 5.0 94 18.08 ## 30 2006-01-01 05:00:00 12 15.98325 3.6008 260 5.8 94 14.87 ## NOx.mod ws.mod wd.mod temp.mod rh.mod ## 5 24.00 2.78 224 3.85 93.16 ## 10 19.91 2.63 226 3.85 92.77 ## 15 18.25 2.52 236 2.85 99.40 ## 20 18.33 2.48 253 2.85 99.19 ## 25 18.09 2.24 275 3.85 97.49 ## 30 21.38 2.43 285 4.85 94.41 The conditionalEval function can be used straightforwardly to provide information on how predictions depend on another variable in general. In this case the option statistic can refer to another variable in the data frame to see how the quality of predictions depend on values of that variable. For example, in Figure 19.4 it can be seen how wind speed varies across the O3 prediction intervals. At low predicted concentrations of O3 there is a high proportion of low wind speed conditions (0 to 2.57 m s-1). When O3 is predicted to be around 40 ppb the wind speed tends to be higher — and finally at higher predicted concentrations of O3 the wind speed tends to decrease again. The important aspect of plotting data in this way is that it can directly relate the prediction performance to values of other variables, which should help develop a much better idea of the conditions that matter most. The user can therefore develop a good feel for the types of conditions where a model performs well or poorly and this might provide clues as to the underlying reasons for specific model behaviour. conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, statistic = &quot;ws.obs&quot;, col.var = &quot;Set3&quot;) Figure 19.4: Conditional quantiles at for O3 concentrations (left plot). On the right is a plot showing how the wind speed varies across the O3 prediction intervals. In an extension to Figure19.4 it is possible to derive information on the simultaneous model performance of other variables. Figure 19.5 shows the conditional quantile plot for hourly O3 predictions. This shows among other things that concentrations of O3 tend to be under-predicted for concentrations less than about 20 ppb. The Figure on the right shows the simultaneous model performance for wind speed and NOx for the same prediction intervals as shown in the conditional quantile plot. The plot on the right shows that for low concentrations of predicted O3 there is a tendency for NOx concentrations to be overestimated (NMB \\(\\approx\\) 0.2 to 0.4) and wind speeds to be underestimated (NMB \\(\\approx\\)-0.2 to -0.3). One possible explanation for this behaviour is that the meteorological model tends to produce wind speeds that are too low, which would result in higher concentrations of NOx, which in turn would result in lower concentrations of O3. Note that it is possible to include more than one statistic, which would be plotted in a new panel e.g. statistic = c(\"NMB\", \"r\"). In essence the conditionalEval function provides more information on model performance that can help better diagnose potential problems. Clearly, there are many other ways in which the results can be analysed, which will depend on the data available. conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, var.obs = c(&quot;NOx.obs&quot;, &quot;ws.obs&quot;), var.mod = c(&quot;NOx.mod&quot;, &quot;ws.mod&quot;), statistic = &quot;NMB&quot;, var.names = c(&quot;nox&quot;, &quot;wind speed&quot;)) Figure 19.5: Conditional quantiles at for O3 concentrations (left plot). On the right is the model performance for wind speed and NOx predictions, which in this case is for the Normalised Mean Bias. A plot using temperature predictions shows that for most of the range in O3 predictions there is very little bias in temperature (although there is some negative bias in temperature for very low concentration O3 predictions): conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, var.obs = c(&quot;temp.obs&quot;, &quot;ws.obs&quot;), var.mod = c(&quot;temp.mod&quot;, &quot;ws.mod&quot;), statistic = &quot;NMB&quot;, var.names = c(&quot;temperature&quot;, &quot;wind speed&quot;)) Finally, (but not shown) it can be very useful to consider model performance in terms of air mass origin. In the example below, trajectories are imported, a cluster analysis undertaken and then evaluated using conditionalEval. ## import trajectories for 2006 traj &lt;- importTraj(&quot;london&quot;, 2006) ## carry out a cluster analysis cl &lt;- trajCluster(traj, method = &quot;Angle&quot;, n.cluster = 5) ## merge with orginal model eval data condDat &lt;- merge(condDat, cl, by = &quot;date&quot;) ## plot it conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, statistic = &quot;cluster&quot;, col.var = &quot;Set3&quot;) References "],
["sec-modStats.html", "Section20 Model evaluation 20.1 Background 20.2 Example of use", " Section20 Model evaluation 20.1 Background The modStats function provides key model evaluation statistics for comparing models against measurements and models against other models. There are a very wide range of evaluation statistics that can be used to assess model performance. There is, however, no single statistic that encapsulates all aspects of interest. For this reason it is useful to consider several performance statistics and also to understand the sort of information or insight they might provide. In the following definitions, \\(O_i\\) represents the \\(i\\)th observed value and \\(M_i\\) represents the \\(i\\)th modelled value for a total of \\(n\\) observations. Fraction of predictions within a factor or two, FAC2 The fraction of modelled values within a factor of two of the observed values are the fraction of model predictions that satisfy: \\[\\begin{equation} 0.5 \\leq \\frac{M_i}{O_i} \\leq 2.0 \\tag{20.1} \\end{equation}\\] Mean bias, MB The mean bias provides a good indication of the mean over or under estimate of predictions. Mean bias in the same units as the quantities being considered. \\[\\begin{equation} MB = \\frac{1}{n}\\sum_{i=1}^{N} M_i - O_i \\tag{20.2} \\end{equation}\\] Mean Gross Error, MGE The mean gross error provides a good indication of the mean error regardless of whether it is an over or under estimate. Mean gross error is in the same units as the quantities being considered. \\[\\begin{equation} MGE = \\frac{1}{n}\\sum_{i=1}^{N} |M_i - O_i| \\tag{20.3} \\end{equation}\\] Normalised mean bias, NMB The normalised mean bias is useful for comparing pollutants that cover different concentration scales and the mean bias is normalised by dividing by the observed concentration. \\[\\begin{equation} NMB = \\frac{\\sum\\limits_{i=1}^{n} M_i - O_i}{\\sum\\limits_{i=1}^{n} O_i} \\tag{20.4} \\end{equation}\\] Normalised mean gross error, NMGE The normalised mean gross error further ignores whether a prediction is an over or under estimate. \\[\\begin{equation} NMGE = \\frac{\\sum\\limits_{i=1}^{n} |M_i - O_i|}{\\sum\\limits_{i=1}^{n} O_i} \\tag{20.5} \\end{equation}\\] Root mean squared error, RMSE The RMSE is a commonly used statistic that provides a good overall measure of how close modelled values are to predicted values. \\[\\begin{equation} RMSE = \\left({\\frac{\\sum\\limits_{i=1}^{n} (M_i - O_i)^2}{n}}\\right)^{1/2} \\tag{20.6} \\end{equation}\\] Correlation coefficient, r The (Pearson) correlation coefficient is a measure of the strength of the linear relationship between two variables. If there is perfect linear relationship with positive slope between the two variables, \\(r\\) = 1. If there is a perfect linear relationship with negative slope between the two variables \\(r\\) = -1. A correlation coefficient of 0 means that there is no linear relationship between the variables. Note that modStats accepts an option method, which can be set to “kendall” and “spearman” for alternative calculations of \\(r\\). \\[\\begin{equation} r = \\frac{1}{(n-1)}\\sum\\limits_{i=1}^{n}\\left(\\frac{M_i - \\overline{M}}{\\sigma_M}\\right)\\left(\\frac{O_i - \\overline{O}}{\\sigma_O}\\right) \\tag{20.7} \\end{equation}\\] Coefficient of Efficiency, COE The Coefficient of Efficiency based on Legates and McCabe (2012) and Legates and McCabe Jr (1999). There have been many suggestions for measuring model performance over the years, but the \\(COE\\) is a simple formulation which is easy to interpret. A perfect model has a \\(COE\\) = 1. As noted by Legates and McCabe although the \\(COE\\) has no lower bound, a value of \\(COE\\) = 0.0 has a fundamental meaning. It implies that the model is no more able to predict the observed values than does the observed mean. Therefore, since the model can explain no more of the variation in the observed values than can the observed mean, such a model can have no predictive advantage. For negative values of \\(COE\\), the model is less effective than the observed mean in predicting the variation in the observations. \\[\\begin{equation} COE = 1.0 - \\frac{\\sum\\limits_{i=1}^{n} |M_i -O_i|}{\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|} \\tag{20.8} \\end{equation}\\] Index of Agreement, IOA The Index of Agreement, \\(IOA\\) is commonly used in model evaluation (Willmott, Robeson, and Matsuura 2011). It spans between -1 and +1 with values approaching +1 representing better model performance. An \\(IOA\\) of 0.5, for example, indicates that the sum of the error-magnitudes is one half of the sum of the observed-deviation magnitudes. When \\(IOA\\) = 0.0, it signifies that the sum of the magnitudes of the errors and the sum of the observed-deviation magnitudes are equivalent. When \\(IOA\\) = -0.5, it indicates that the sum of the error-magnitudes is twice the sum of the perfect model-deviation and observed-deviation magnitudes. Values of \\(IOA\\) near -1.0 can mean that the model-estimated deviations about \\(O\\) are poor estimates of the observed deviations; but, they also can mean that there simply is little observed variability — so some caution is needed when the \\(IOA\\) approaches -1. It is defined as (with \\(c = 2\\)): \\[\\begin{equation} IOA = \\left\\{ \\begin{array}{l} 1.0 - \\frac{\\displaystyle \\sum\\limits_{i=1}^{n} |M_i-O_i|} {\\displaystyle c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|}, when \\\\ \\sum\\limits_{i=1}^{n} |M_i-O_i| \\le c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}| \\\\ \\frac{\\displaystyle c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|} {\\displaystyle \\sum\\limits_{i=1}^{n} |M_i-O_i|} - 1.0, when \\\\ \\sum\\limits_{i=1}^{n} |M_i-O_i| &gt; c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}| \\end{array} \\tag{20.9} \\right. \\end{equation}\\] 20.2 Example of use The function can be called very simply and only requires two numeric fields to compare. To show how the function works, some synthetic data will be generated for 5 models. ## observations; 100 random numbers set.seed(10) obs &lt;- 100 * runif(100) mod1 &lt;- data.frame(obs, mod = obs + 10, model = &quot;model 1&quot;) mod2 &lt;- data.frame(obs, mod = obs + 20 * rnorm(100), model = &quot;model 2&quot;) mod3 &lt;- data.frame(obs, mod = obs - 10 * rnorm(100), model = &quot;model 3&quot;) mod4 &lt;- data.frame(obs, mod = obs / 2 + 10 * rnorm(100), model = &quot;model 4&quot;) mod5 &lt;- data.frame(obs, mod = obs * 1.5 + 3 * rnorm(100), model = &quot;model 5&quot;) modData &lt;- rbind(mod1, mod2, mod3, mod4, mod5) head(modData) ## obs mod model ## 1 50.747820 60.74782 model 1 ## 2 30.676851 40.67685 model 1 ## 3 42.690767 52.69077 model 1 ## 4 69.310208 79.31021 model 1 ## 5 8.513597 18.51360 model 1 ## 6 22.543662 32.54366 model 1 We now have a data frame with observations and predictions for 5 models. The evaluation of the statistics is given by: modStats(modData, obs = &quot;obs&quot;, mod = &quot;mod&quot;, type = &quot;model&quot;) ## model n FAC2 MB MGE NMB NMGE RMSE ## 1 model 1 100 0.89 10.000000 10.000000 0.22455508 0.2245551 10.000000 ## 2 model 2 100 0.79 0.922391 16.592192 0.02071276 0.3725861 19.318041 ## 3 model 3 100 0.88 1.013647 7.887325 0.02276196 0.1771139 9.451354 ## 4 model 4 100 0.56 -20.603684 21.860655 -0.46266619 0.4908921 25.758520 ## 5 model 5 100 0.96 22.521685 22.569229 0.50573588 0.5068035 26.132866 ## r COE IOA ## 1 1.0000000 0.538289594 0.7691448 ## 2 0.8258202 0.233921216 0.6169606 ## 3 0.9371237 0.635833987 0.8179170 ## 4 0.8142735 -0.009329173 0.4953354 ## 5 0.9963753 -0.042044773 0.4789776 It is possible to rank the statistics based on the , which is a good general indicator of model performance. modStats(modData, obs = &quot;obs&quot;, mod = &quot;mod&quot;, type = &quot;model&quot;, rank.name = &quot;model&quot;) ## model n FAC2 MB MGE NMB NMGE RMSE ## 3 model 3 100 0.88 1.013647 7.887325 0.02276196 0.1771139 9.451354 ## 1 model 1 100 0.89 10.000000 10.000000 0.22455508 0.2245551 10.000000 ## 2 model 2 100 0.79 0.922391 16.592192 0.02071276 0.3725861 19.318041 ## 4 model 4 100 0.56 -20.603684 21.860655 -0.46266619 0.4908921 25.758520 ## 5 model 5 100 0.96 22.521685 22.569229 0.50573588 0.5068035 26.132866 ## r COE IOA ## 3 0.9371237 0.635833987 0.8179170 ## 1 1.0000000 0.538289594 0.7691448 ## 2 0.8258202 0.233921216 0.6169606 ## 4 0.8142735 -0.009329173 0.4953354 ## 5 0.9963753 -0.042044773 0.4789776 The modStats function is however much more flexible than indicated above. While it is useful to calculate model evaluation statistics in a straightforward way it can be much more informative to consider the statistics split by different periods. Data have been assembled from a Defra model evaluation exercise which consists of hourly O3 predictions at 15 receptor points around the UK for 2006. The aim here is not to identify a particular model that is ‘best’ and for this reason the models are simply referred to as model 1, model 2 and so on. We will aim to make the data more widely available. However, data set has this form: load(&quot;~/My Drive/openair/Data/modelData.RData&quot;) head(modTest) ## site date o3 mod group ## 1 Aston.Hill 2006-01-01 00:00:00 NA NA model 1 ## 2 Aston.Hill 2006-01-01 01:00:00 74 65.28 model 1 ## 3 Aston.Hill 2006-01-01 02:00:00 72 64.64 model 1 ## 4 Aston.Hill 2006-01-01 03:00:00 72 64.46 model 1 ## 5 Aston.Hill 2006-01-01 04:00:00 70 64.88 model 1 ## 6 Aston.Hill 2006-01-01 05:00:00 66 65.80 model 1 There are columns representing the receptor location (site), the date, measured values (o3), model predictions (mod) and the model itself (group). There are numerous ways in which the statistics can be calculated. However, of interest here is how the models perform at a single receptor by season. The seasonal nature of O3 is a very important characteristic and it is worth considering in more detail. The statistics are easy enough to calculate as shown below. In this example a subset of the data is selected to consider only the Harwell site. Second, the type option is used to split the calculations by season and model. Finally, the statistics are grouped by the \\(IOA\\) for each season. It is now very easy how model performance changes by season and which models perform best in each season. options(digits = 2) ## don&#39;t display too many decimal places modStats(subset(modTest, site == &quot;Harwell&quot;), obs = &quot;o3&quot;, mod = &quot;mod&quot;, type = c(&quot;season&quot;, &quot;group&quot;), rank = &quot;group&quot;) ## # A tibble: 36 x 12 ## # Groups: season [4] ## season group n FAC2 MB MGE NMB NMGE RMSE r COE ## &lt;ord&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 sprin… mode… 1905 0.875 8.58 16.5 0.137 0.263 21.9 0.576 0.0747 ## 2 sprin… mode… 1905 0.876 -2.04 16.8 -0.0325 0.268 21.8 0.452 0.0584 ## 3 sprin… mode… 1905 0.872 11.9 17.7 0.190 0.282 23.7 0.519 0.00818 ## 4 sprin… mode… 1905 0.885 4.98 18.1 0.0793 0.289 23.9 0.361 -0.0151 ## 5 sprin… mode… 1905 0.870 10.8 19.5 0.172 0.311 24.0 0.588 -0.0940 ## 6 sprin… mode… 1825 0.821 7.79 20.0 0.123 0.317 25.8 0.480 -0.118 ## 7 sprin… mode… 1905 0.786 -13.7 21.4 -0.218 0.340 26.1 0.531 -0.195 ## 8 sprin… mode… 1905 0.732 -13.4 24.3 -0.213 0.388 29.8 0.312 -0.362 ## 9 sprin… mode… 1905 0.743 1.01 24.9 0.0161 0.396 32.8 0.264 -0.392 ## 10 summe… mode… 2002 0.918 3.83 15.5 0.0632 0.256 20.9 0.750 0.317 ## # … with 26 more rows, and 1 more variable: IOA &lt;dbl&gt; Note that it is possible to read the results of the modStats function into a data frame, which then allows the results to be plotted. This is generally a good idea when there is a lot of numeric data to consider and plots will convey the information better. The modStats function is much more flexible than indicated above and can be used in lots of interesting ways. The type option in particular makes it possible to split the statistics in numerous ways. For example, to summarise the performance of models by site, model and day of the week: modStats(modStats, obs = &quot;o3&quot;, mod = &quot;mod&quot;, type = c(&quot;site&quot;, &quot;weekday&quot;, &quot;group&quot;), rank = &quot;group&quot;) Similarly, if other data are available e.g. meteorological data or other pollutant species then these variables can also be used to test models against ranges in their values. This capability is potentially very useful because it allows for a much more probing analysis into model evaluation. For example, with wind speed and direction it is easy to consider how model performance varies by wind speed intervals or wind sectors, both of which could reveal important performance characteristics. References "],
["sec-TaylorDiagram.html", "Section21 Taylor Diagam 21.1 Background 21.2 Examples of Taylor Diagrams", " Section21 Taylor Diagam 21.1 Background The Taylor Diagram is one of the more useful methods for evaluating model performance. Details of the diagram can be found at http://www-pcmdi.llnl.gov/about/staff/Taylor/CV/Taylor_diagram_primer.pdf and in Taylor (2001). The diagram provides a way of showing how three complementary model performance statistics vary simultaneously. These statistics are the correlation coefficient R, the standard deviation (sigma) and the (centred) root-mean-square error. These three statistics can be plotted on one (2D) graph because of the way they are related to one another which can be represented through the Law of Cosines. The openair version of the Taylor Diagram has several enhancements that increase its flexibility. In particular, the straightforward way of producing conditioning plots should prove valuable under many circumstances (using the type option). Many examples of Taylor Diagrams focus on model-observation comparisons for several models using all the available data. However, more insight can be gained into model performance by partitioning the data in various ways e.g. by season, daylight/nighttime, day of the week, by levels of a numeric variable e.g. wind speed or by land-use type etc. We first show a diagram and then pick apart the different components to understand how to interpret it. The diagram can look overly complex but once it is understood how to interpret the three main characteristics it becomes much easier to understand. A typical diagram is shown in Figure 21.1 for nine anonymised models used for predicting hourly O3 concentrations at 15 sites around the UK. library(openair) library(tidyverse) TaylorDiagram(modTest, obs = &quot;o3&quot;, mod = &quot;mod&quot;, group = &quot;group&quot;) Figure 21.1: An example of the use of the TaylorDiagram function. The plots shown in Figure 21.2 break the Taylor Diagrams into three components to aid interpretation. The first plot (top left) highlights the comparison of variability in for each model compared with the measurements. The variability is represented by the standard deviation of the observed and modelled values. The plot shows that the observed variability (given by the standard deviation) is about 27 (\\(\\mu\\)g m-3) and is marked as ‘observed’ on the x-axis. The magnitude of the variability is measured as the radial distance from the origin of the plot (the red line with the arrow shows the standard deviation for model \\(g\\), which is about 25~\\(\\mu\\)g m-3). To aid interpretation the radial dashed line is shown from the ‘observed’ point. Each model is shown in this case by the position of the letters a to i. On this basis it can be seen that models 1, \\(a\\), \\(b\\) have more variability than the measurements (because they extend beyond the dashed line), whereas the others have less variability than the measurements. Models \\(a\\) and \\(b\\) are also closed to the dashed line and therefore have the closest variability compared with the observations. The next statistic to consider is the correlation coefficient, \\(R\\) shown by the top-right Figure in Figure 21.2. This is shown on the arc and points that lie closest to the x-axis have the highest correlation. The grey lines help to show this specific correlation coefficients. The red arc shows \\(R\\)=0.7 for model \\(g\\). The best performing models with the highest \\(R\\) are models \\(b\\) and \\(g\\) with correlation coefficients around 0.7. Two models stand out as having much worse correlations with the observations: models \\(e\\) and \\(i\\) (values of around 0.4). Finally, the lower plot in Figure 21.2 highlights the centred root-mean square error (RMS). It is centred because the mean values of the data (observations and predictions) are subtracted first. The concentric dashed lines emanating from the `observed’ point show the value of the RMS error — so points furthest from the ‘observed’ value are the worst performing models because they have the highest RMS errors. On this basis, model \\(g\\) has the lowest error of about 20~\\(\\mu\\)g m-3, shown again by the red line. Models \\(e\\) and \\(i\\) are considerably worse because they have RMS errors of around 30~\\(\\mu\\)g m-3. So which model is best? Taken as a whole it is probably model \\(g\\) because it has reasonably similar variability compared with the observations, the highest correlation and the least RMS error. However, models \\(f\\) and \\(b\\) also look to be good. Perhaps it is easier to conclude that models \\(e\\) and \\(i\\) are not good . Note that in cases where there is a column site it makes sense to use type = \"site\" to ensure that the statistics are calculated on a per site basis and each panel represents a single site. knitr::include_graphics(c(&#39;images/TaylorExplainSD.png&#39;, &#39;images/TaylorExplainR.png&#39;, &#39;images/TaylorExplainRMS.png&#39;)) Figure 21.2: Taylor Diagrams broken down to highlight how to interpret the three main statistics. The red line/arrow indicate how to read interpret each of the three statistics. Top left: standard deviation, top right: correlation, bottom: centrered RMS error. 21.2 Examples of Taylor Diagrams The example used here carries on from the previous section using data from a Defra model evaluation exercise. As mentioned previously, the use of the type option offers enormous flexibility for comparing models. However, we will only focus on the seasonal evaluation of the models. In the call below, group is the column that identified the model and type is the conditioning variable that produces in this case four panels — one for each season. Note that in this case we focus on a single site. ## select a single site LH &lt;- filter(modTest, site == &quot;Lullington.Heath&quot;) TaylorDiagram(LH, obs = &quot;o3&quot;, mod = &quot;mod&quot;, group = &quot;group&quot;, type = &quot;season&quot;) Figure 21.3: Use of the TaylorDiagam function to show model performance for 9 models used to predict O3 concentrations at the Lullington Heath site. Figure 21.3 contains a lot of useful information. Consider the summer comparison first. All models tend to underestimate the variability of O3 concentrations because they all lies within the black dashed line. However, models 7 and 9 are close to the observed variability. The general underestimate of the variability for summer conditions might reflect that the models do not adequately capture regional O3 episodes when concentrations are high. Models 7 and 8 do best in terms of high correlation with the measurements (around 0.8) and lowest RMS error (around 20–22~\\(\\mu\\)g m-3). Models 3, 5 and 6 tend to do worse on all three statistics during the summer. By contrast, during wintertime conditions models 1 and 3 are clearly best. From an evaluation perspective it would be useful to understand why some models are better for wintertime conditions and others better in summer and this is clearly something that could be investigated further. There are many other useful comparisons that can be undertaken easily. A few of these are shown below, but not plotted. ## by receptor comparison TaylorDiagram(modTest, obs = &quot;o3&quot;, mod = &quot;mod&quot;, group = &quot;group&quot;, type = &quot;site&quot;) ## by month comparison for a SINGLE site TaylorDiagram(subset(modTest, site == &quot;Harwell&quot;), obs = &quot;o3&quot;, mod = &quot;mod&quot;, group = &quot;group&quot;, type = &quot;month&quot;) ## By season AND daylight/nighttime TaylorDiagram(subset(modTest, site == &quot;Harwell&quot;), obs = &quot;o3&quot;, mod = &quot;mod&quot;, group = &quot;group&quot;, type = c(&quot;season&quot;, &quot;daylight&quot;)) It is also possible to combine different groups of model results. For example, rather than plot how the models perform at a single site it can be useful to show how they compare at all sites. To do this it is necessary to normalise the data because there will be different values of the observed variable across different sites. In this case we can supply the option group = c(\"group\", \"site\"). This will show the variation by model for all sites. The results are shown in Figure 21.4. These results show that in general models tend to predict in similalry good (or bad) ways across all sites as shown by the grouping of points on Figure 21.4. TaylorDiagram(modTest, obs = &quot;o3&quot;, mod = &quot;mod&quot;, group = c(&quot;group&quot;, &quot;site&quot;), normalise = TRUE, cex = 1, pch = c(15:19, 15:18)) Figure 21.4: Use of the TaylorDiagam function to show model performance for 9 models used for all sites. References "],
["sec-utility.html", "Section22 Utility functions 22.1 Selecting data by date 22.2 Making intervals — cutData 22.3 Selecting run lengths of values above a threshold — pollution episodes 22.4 Calculating rolling means 22.5 Aggregating data by different time intervals 22.6 Calculating percentiles 22.7 Correlation matrices", " Section22 Utility functions 22.1 Selecting data by date Selecting by date/time in R can be intimidating for new users—and time-consuming for all users. The selectByDate function aims to make this easier by allowing users to select data based on the British way of expressing date i.e. d/m/y. This function should be very useful in circumstances where it is necessary to select only part of a data frame. First load the packages we need. library(openair) library(tidyverse) ## select all of 1999 data.1999 &lt;- selectByDate(mydata, start = &quot;1/1/1999&quot;, end = &quot;31/12/1999&quot;) head(data.1999) ## # A tibble: 6 x 15 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1999-01-01 00:00:00 5.04 140 88 35 4 21 3.84 1.02 18 ## 2 1999-01-01 01:00:00 4.08 160 132 41 3 17 5.24 2.7 11 ## 3 1999-01-01 02:00:00 4.8 160 168 40 4 17 6.51 2.87 8 ## 4 1999-01-01 03:00:00 4.92 150 85 36 3 15 4.18 1.62 10 ## 5 1999-01-01 04:00:00 4.68 150 93 37 3 16 4.25 1.02 11 ## 6 1999-01-01 05:00:00 3.96 160 74 29 5 14 3.88 0.725 NA ## # … with 5 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt; tail(data.1999) ## # A tibble: 6 x 15 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1999-12-31 18:00:00 4.68 190 226 39 NA 29 5.46 2.38 23 ## 2 1999-12-31 19:00:00 3.96 180 202 37 NA 27 4.78 2.15 23 ## 3 1999-12-31 20:00:00 3.36 190 246 44 NA 30 5.88 2.45 23 ## 4 1999-12-31 21:00:00 3.72 220 231 35 NA 28 5.28 2.22 23 ## 5 1999-12-31 22:00:00 4.08 200 217 41 NA 31 4.79 2.17 26 ## 6 1999-12-31 23:00:00 3.24 200 181 37 NA 28 3.48 1.78 22 ## # … with 5 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt; ## easier way data.1999 &lt;- selectByDate(mydata, year = 1999) ## more complex use: select weekdays between the hours of 7 am to 7 pm sub.data &lt;- selectByDate(mydata, day = &quot;weekday&quot;, hour = 7:19) ## select weekends between the hours of 7 am to 7 pm in winter (Dec, Jan, Feb) sub.data &lt;- selectByDate(mydata, day = &quot;weekend&quot;, hour = 7:19, month = c(&quot;dec&quot;, &quot;jan&quot;, &quot;feb&quot;)) The function can be used directly in other functions. For example, to make a polar plot using year 2000 data: polarPlot(selectByDate(mydata, year = 2000), pollutant = &quot;so2&quot;) 22.2 Making intervals — cutData The cutData function is a utility function that is called by most other functions but is useful in its own right. Its main use is to partition data in many ways, many of which are built-in to openair Note that all the date-based types e.g. month/year are derived from a column date. If a user already has a column with a name of one of the date-based types it will not be used. For example, to cut data into seasons: mydata &lt;- cutData(mydata, type = &quot;season&quot;) head(mydata) ## # A tibble: 6 x 16 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1998-01-01 00:00:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 2 1998-01-01 01:00:00 2.16 230 NA NA NA 37 NA NA NA ## 3 1998-01-01 02:00:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 4 1998-01-01 03:00:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 5 1998-01-01 04:00:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 6 1998-01-01 05:00:00 3 190 264 42 0 16 5.50 3.05 NA ## # … with 6 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt;, season &lt;ord&gt; This adds a new field season that is split into four seasons. There is an option hemisphere that can be used to use southern hemisphere seasons when set as hemisphere = \"southern\". The type can also be another field in a data frame e.g. mydata &lt;- cutData(mydata, type = &quot;pm10&quot;) head(mydata) ## # A tibble: 6 x 16 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1998-01-01 00:00:00 0.6 280 285 39 1 pm10… 4.72 3.37 NA ## 2 1998-01-01 01:00:00 2.16 230 NA NA NA pm10… NA NA NA ## 3 1998-01-01 02:00:00 2.76 190 NA NA 3 pm10… 6.83 9.60 NA ## 4 1998-01-01 03:00:00 2.16 170 493 52 3 pm10… 7.66 10.2 NA ## 5 1998-01-01 04:00:00 2.4 180 468 78 2 pm10… 8.07 8.91 NA ## 6 1998-01-01 05:00:00 3 190 264 42 0 pm10… 5.50 3.05 NA ## # … with 6 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt;, season &lt;ord&gt; data(mydata) ## re-load mydata fresh This divides PM10 concentrations into four quantiles — roughly equal numbers of PM10 concentrations in four levels. Most of the time users do not have to call directly because most functions have a option that is used to call cutData directly e.g. polarPlot(mydata, pollutant = &quot;so2&quot;, type = &quot;season&quot;) However, it can be useful to call cutData before supplying the data to a function in a few cases. First, if one wants to set seasons to the southern hemisphere as above. Second, it is possible to override the division of a numeric variable into four quantiles by using the option n.levels. More details can be found in the help file. 22.3 Selecting run lengths of values above a threshold — pollution episodes A seemingly easy thing to do that has relevance to air pollution episodes is to select run lengths of contiguous values of a pollutant above a certain threshold. For example, one might be interested in selecting O3 concentrations where there are at least 8 consecutive hours above 90~ppb. In other words, a selection that combines both a threshold and persistence. These periods can be very important from a health perspective and it can be useful to study the conditions under which they occur. But how do you select such periods easily? The selectRunning utility function has been written to do this. It could be useful for all sorts of situations e.g. Selecting hours when primary pollutant concentrations are persistently high — and then applying other openair functions to analyse the data in more depth. In the study of particle suspension or deposition etc. it might be useful to select hours when wind speeds remain high or rainfall persists for several hours to see how these conditions affect particle concentrations. It could be useful in health impact studies to select blocks of data where pollutant concentrations remain above a certain threshold. As an example we are going to consider O3 concentrations at a semi-rural site in south-west London (Teddington). The data can be downloaded as follows: ted &lt;- importKCL(site = &quot;td0&quot;, year = 2005:2009, met = TRUE) ## NOTE - mass units are used ## ug/m3 for NOx, NO2, SO2, O3; mg/m3 for CO ## PM10_raw is raw data multiplied by 1.3 ## see how many rows there are nrow(ted) ## [1] 43824 We are going to contrast two polar plots of O3 concentration. The first uses all hours in the data set, and the second uses a subset of hours. The subset of hours is defined by O3 concentrations above 90~ppb for periods of at least 8-hours i.e. what might be considered as ozone episode conditions. episode &lt;- selectRunning(ted, pollutant = &quot;o3&quot;, threshold = 90, run.len = 8) ## see how many rows there are nrow(episode) ## [1] 1399 Now we are going to produce two bivariate polar plots shown in Figure 22.1. polarPlot(ted, pollutant = &quot;o3&quot;, min.bin = 2) polarPlot(episode, pollutant = &quot;o3&quot;, min.bin = 2) Figure 22.1: Example of using the selectRunning function to select episode hours to produce bivariate polar plots of O3 concentration. The results are shown in Figure 22.1. The polar plot for all data (left plot of Figure 22.1) shows that the highest O3 concentrations tend to occur for high wind speed conditions from almost every direction. Lower concentrations are observed for low wind speeds because concentrations of NOx are higher, resulting in O3 destruction. By contrast, a polar plot of the episode conditions (right plot of Figure 22.1) is very different. In this case there is a clear set of conditions where these criteria are met i.e. lengths of at least 8-hours where the O3 concentration is at least 90~ppb. It is clear the highest concentrations are dominated by south-easterly conditions i.e. corresponding to easterly flow from continental Europe where there has been time to the O3 chemistry to take place. Another interesting test plot is to consider NOx concentrations at Marylebone Road — see Figure 8.1, which shows that high concentrations are dominated by a swathe of south-westerly wind conditions (even for high wind speeds). However, if a selection is made of episode conditions (defined here as NOx concentrations &gt;500 ppb for at least 5-hours), then it can be seen that it is actually the low wind speed conditions that dominate. These conditions correspond to low in-canyon wind speeds and low wind speeds across London, which tend to elevate local and background NOx concentrations. Even though high concentrations of NOx are observed at high wind speeds, it does not seem that these conditions are as important for episode conditions. Users can run the code below to verify these observations. episode &lt;- selectRunning(mydata, pollutant = &quot;nox&quot;, threshold = 800, run.len = 5) polarPlot(episode, pollutant = &quot;nox&quot;, min.bin = 2) 22.4 Calculating rolling means Some air pollution statistics such as for O3 and particulate matter are expressed as rolling means and it is useful to be able to calculate these. It can also be useful to help smooth-out data for clearer plotting. The rollingMean function makes these calculations. One detail that can be important is that for some statistics a mean is only considered valid if there are a sufficient number of valid readings over the averaging period. Often there is a requirement for at least 75% data capture. For example, with an averaging period of 8 hours and a data capture threshold of 75%, at least 6 hours are required to calculate the mean. The function is called as follows; in this case to calculate 8-hour rolling mean concentrations of O3. mydata &lt;- rollingMean(mydata, pollutant = &quot;o3&quot;, hours = 8, new.name = &quot;rollingo3&quot;, data.thresh = 75) tail(mydata) ## # A tibble: 6 x 16 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2005-06-23 07:00:00 1.5 250 404 156 4 49 NA 1.81 28 ## 2 2005-06-23 08:00:00 1.5 260 388 145 6 48 NA 1.64 26 ## 3 2005-06-23 09:00:00 1.5 210 404 168 7 58 NA 1.29 34 ## 4 2005-06-23 10:00:00 2.6 240 387 175 10 55 NA 1.29 34 ## 5 2005-06-23 11:00:00 3.1 220 312 125 15 52 NA 1.29 33 ## 6 2005-06-23 12:00:00 3.1 220 287 119 17 55 NA 1.29 35 ## # … with 6 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt;, rollingo3 &lt;dbl&gt; Note that calculating rolling means shortens the length of the data set. In the case of O3, no calculations are made for the last 7 hours. Type ?rollingMean into R for more details. Note that the function currently only works with a single site. 22.5 Aggregating data by different time intervals Aggregating data by different averaging periods is a common and important task. There are many reasons for aggregating data in this way: Data sets may have different averaging periods and there is a need to combine them. For example, the task of combining an hourly air quality data set with a 15-minute average meteorological data set. The need here would be to aggregate the 15-minute data to 1-hour before merging. It is extremely useful to consider data with different averaging times straightforwardly. Plotting a very long time series of hourly or higher resolution data can hide the main features and it would be useful to apply a specific (but flexible) averaging period to the data for plotting. Those who make measurements during field campaigns (particularly for academic research) may have many instruments with a range of different time resolutions. It can be useful to re-calculate time series with a common averaging period; or maybe help reduce noise. It is useful to calculate statistics other than means when aggregating e.g. percentile values, maximums etc. For statistical analysis there can be short-term autocorrelation present. Being able to choose a longer averaging period is sometimes a useful strategy for minimising autocorrelation. In aggregating data in this way, there are a couple of other issues that can be useful to deal with at the same time. First, the calculation of proper vector-averaged wind direction is essential. Second, sometimes it is useful to set a minimum number of data points that must be present before the averaging is done. For example, in calculating monthly averages, it may be unwise to not account for data capture if some months only have a few valid points. When a data capture threshold is set through data.thresh it is necessary for timeAverage to know what the original time interval of the input time series is. The function will try and calculate this interval based on the most common time gap (and will print the assumed time gap to the screen). This works fine most of the time but there are occasions where it may not e.g. when very few data exist in a data frame. In this case the user can explicitly specify the interval through interval in the same format as avg.time e.g. interval = \"month\". It may also be useful to set start.date and end.date if the time series do not span the entire period of interest. For example, if a time series ended in October and annual means are required, setting end.date to the end of the year will ensure that the whole period is covered and that data.thresh is correctly calculated. The same also goes for a time series that starts later in the year where start.date should be set to the beginning of the year. All these issues are (hopefully) dealt with by the timeAverage function. The options are shown below, but as ever it is best to check the help that comes with the openair package. To calculate daily means from hourly (or higher resolution) data: daily &lt;- timeAverage(mydata, avg.time = &quot;day&quot;) daily ## # A tibble: 2,731 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998-01-01 00:00:00 6.84 188. 154. 39.4 6.87 18.2 3.15 2.70 NaN ## 2 1998-01-02 00:00:00 7.07 223. 132. 39.5 6.48 27.8 3.94 1.77 NaN ## 3 1998-01-03 00:00:00 11.0 226. 120. 38.0 8.41 20.2 3.20 1.74 NaN ## 4 1998-01-04 00:00:00 11.5 223. 105. 35.3 9.61 21.0 2.96 1.62 NaN ## 5 1998-01-05 00:00:00 6.61 237. 175. 46.0 4.96 24.2 4.52 2.13 NaN ## 6 1998-01-06 00:00:00 4.38 197. 214. 45.3 1.35 34.6 5.70 2.53 NaN ## 7 1998-01-07 00:00:00 7.61 219. 193. 44.9 4.42 31.0 5.67 2.48 NaN ## 8 1998-01-08 00:00:00 8.58 216. 161. 43.1 4.96 36 4.68 2.10 NaN ## 9 1998-01-09 00:00:00 6.7 206. 163. 38 3.62 38.0 5.13 2.36 NaN ## 10 1998-01-10 00:00:00 2.98 167. 219. 44.9 0.375 37.0 4.91 2.23 NaN ## # … with 2,721 more rows, and 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ## # ratio &lt;dbl&gt;, rollingo3 &lt;dbl&gt; Monthly 95th percentile values: monthly &lt;- timeAverage(mydata, avg.time = &quot;month&quot;, statistic = &quot;percentile&quot;, percentile = 95) monthly ## # A tibble: 90 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998-01-01 00:00:00 11.2 45. 371. 68.6 14 53 11.1 3.99 NA ## 2 1998-02-01 00:00:00 8.16 16.7 524. 92 7 68.9 17.5 5.63 NA ## 3 1998-03-01 00:00:00 10.6 37.6 417. 85 15 61 18.4 4.85 NA ## 4 1998-04-01 00:00:00 8.16 44.4 384 81.5 20 52 14.6 4.17 NA ## 5 1998-05-01 00:00:00 7.56 40.6 300 80 25 61 12.7 3.55 40 ## 6 1998-06-01 00:00:00 8.47 50.7 377 74.2 15 53 12.2 4.28 33.9 ## 7 1998-07-01 00:00:00 9.22 36.7 386. 80.0 NA 52.4 13.9 4.52 32 ## 8 1998-08-01 00:00:00 7.92 48.4 337. 87.0 16 58.2 13.0 3.78 38 ## 9 1998-09-01 00:00:00 6 66.7 334. 81.3 14 64 18.2 4.25 47 ## 10 1998-10-01 00:00:00 12 33.9 439. 84 15.1 54 12.0 4.81 33 ## # … with 80 more rows, and 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, ## # rollingo3 &lt;dbl&gt; 2-week averages but only calculate if at least 75% of the data are available: twoweek &lt;- timeAverage(mydata, avg.time = &quot;2 week&quot;, data.thresh = 75) twoweek ## # A tibble: 196 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1997-12-29 00:00:00 6.98 212. 167. 41.4 4.63 29.3 4.47 2.17 NA ## 2 1998-01-12 00:00:00 4.91 221. 173. 42.1 4.70 28.8 5.07 1.86 NA ## 3 1998-01-26 00:00:00 2.78 242. 233. 51.4 2.30 34.9 8.07 2.45 NA ## 4 1998-02-09 00:00:00 4.43 215. 276. 57.1 2.63 43.7 8.98 2.94 NA ## 5 1998-02-23 00:00:00 6.89 237. 248. 56.7 4.99 28.8 9.79 2.57 NA ## 6 1998-03-09 00:00:00 2.97 288. 160. 44.8 5.64 32.7 8.65 1.62 NA ## 7 1998-03-23 00:00:00 4.87 192. 224. 53.6 5.52 35.9 10.2 2.34 NA ## 8 1998-04-06 00:00:00 3.24 294. 144. 43.4 10.1 23.8 5.48 1.40 NA ## 9 1998-04-20 00:00:00 4.38 195. 177. 47.6 10.5 31.4 5.54 1.73 NA ## 10 1998-05-04 00:00:00 3.97 285. 134. 45.5 10.2 38.6 5.49 1.41 24.6 ## # … with 186 more rows, and 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ## # ratio &lt;dbl&gt;, rollingo3 &lt;dbl&gt; Note that timeAverage has a type option to allow for the splitting of variables by a grouping variable. The most common use for type is when data are available for different sites and the averaging needs to be done on a per site basis. First, retaining by site averages: # import some data for two sites dat &lt;- importAURN(c(&quot;kc1&quot;, &quot;my1&quot;), year = 2011:2013) # annual averages by site timeAverage(dat, avg.time = &quot;year&quot;, type = &quot;site&quot;) ## # A tibble: 6 x 17 ## # Groups: site [2] ## site date co nox no2 no o3 so2 pm10 pm2.5 ## &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lond… 2011-01-01 00:00:00 0.656 306. 97.2 137. 18.5 6.86 38.4 24.5 ## 2 Lond… 2012-01-01 00:00:00 0.589 313. 94.0 143. 15.0 8.13 30.8 21.5 ## 3 Lond… 2013-01-01 00:00:00 0.506 281. 84.7 128. 17.7 5.98 29.1 20.1 ## 4 Lond… 2011-01-01 00:00:00 0.225 53.8 36.1 11.6 39.4 2.06 23.7 16.3 ## 5 Lond… 2012-01-01 00:00:00 0.266 57.4 36.7 13.3 38.5 2.03 20.2 14.6 ## 6 Lond… 2013-01-01 00:00:00 0.250 57.9 36.9 13.7 38.4 2.01 23.1 14.7 ## # … with 7 more variables: v10 &lt;dbl&gt;, v2.5 &lt;dbl&gt;, nv10 &lt;dbl&gt;, nv2.5 &lt;dbl&gt;, ## # ws &lt;dbl&gt;, wd &lt;dbl&gt;, air_temp &lt;dbl&gt; Retain site name and site code: # can also retain site code timeAverage(dat, avg.time = &quot;year&quot;, type = c(&quot;site&quot;, &quot;code&quot;)) ## # A tibble: 6 x 18 ## # Groups: site, code [2] ## site code date co nox no2 no o3 so2 pm10 ## &lt;fct&gt; &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lond… MY1 2011-01-01 00:00:00 0.656 306. 97.2 137. 18.5 6.86 38.4 ## 2 Lond… MY1 2012-01-01 00:00:00 0.589 313. 94.0 143. 15.0 8.13 30.8 ## 3 Lond… MY1 2013-01-01 00:00:00 0.506 281. 84.7 128. 17.7 5.98 29.1 ## 4 Lond… KC1 2011-01-01 00:00:00 0.225 53.8 36.1 11.6 39.4 2.06 23.7 ## 5 Lond… KC1 2012-01-01 00:00:00 0.266 57.4 36.7 13.3 38.5 2.03 20.2 ## 6 Lond… KC1 2013-01-01 00:00:00 0.250 57.9 36.9 13.7 38.4 2.01 23.1 ## # … with 8 more variables: pm2.5 &lt;dbl&gt;, v10 &lt;dbl&gt;, v2.5 &lt;dbl&gt;, nv10 &lt;dbl&gt;, ## # nv2.5 &lt;dbl&gt;, ws &lt;dbl&gt;, wd &lt;dbl&gt;, air_temp &lt;dbl&gt; Average all data across sites (drops site and code): timeAverage(dat, avg.time = &quot;year&quot;) ## # A tibble: 3 x 16 ## date co nox no2 no o3 so2 pm10 pm2.5 v10 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-01-01 00:00:00 0.439 181. 67.1 74.9 31.5 4.31 31.4 20.5 5.40 ## 2 2012-01-01 00:00:00 0.424 182. 64.7 76.7 26.9 5.08 25.6 18.1 4.23 ## 3 2013-01-01 00:00:00 0.378 169. 60.7 70.7 28.0 3.79 26.8 17.4 4.29 ## # … with 6 more variables: v2.5 &lt;dbl&gt;, nv10 &lt;dbl&gt;, nv2.5 &lt;dbl&gt;, ws &lt;dbl&gt;, ## # wd &lt;dbl&gt;, air_temp &lt;dbl&gt; timeAverage also works the other way in that it can be used to derive higher temporal resolution data e.g. hourly from daily data or 15-minute from hourly data. An example of usage would be the combining of daily mean particle data with hourly meteorological data. There are two ways these two data sets can be combined: either average the meteorological data to daily means or calculate hourly means from the particle data. The timeAverage function when used to ‘expand’ data in this way will repeat the original values the number of times required to fill the new time scale. In the example below we calculate 15-minute data from hourly data. As it can be seen, the first line is repeated four times and so on. data15 &lt;- timeAverage(mydata, avg.time = &quot;15 min&quot;, fill = TRUE) head(data15, 20) ## # A tibble: 20 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1998-01-01 00:00:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 2 1998-01-01 00:15:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 3 1998-01-01 00:30:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 4 1998-01-01 00:45:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 5 1998-01-01 01:00:00 2.16 230 NA NA NA 37 NA NA NA ## 6 1998-01-01 01:15:00 2.16 230 NA NA NA 37 NA NA NA ## 7 1998-01-01 01:30:00 2.16 230 NA NA NA 37 NA NA NA ## 8 1998-01-01 01:45:00 2.16 230 NA NA NA 37 NA NA NA ## 9 1998-01-01 02:00:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 10 1998-01-01 02:15:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 11 1998-01-01 02:30:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 12 1998-01-01 02:45:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 13 1998-01-01 03:00:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 14 1998-01-01 03:15:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 15 1998-01-01 03:30:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 16 1998-01-01 03:45:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 17 1998-01-01 04:00:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 18 1998-01-01 04:15:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 19 1998-01-01 04:30:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 20 1998-01-01 04:45:00 2.4 180 468 78 2 34 8.07 8.91 NA ## # … with 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, rollingo3 &lt;dbl&gt; The timePlot can apply this function directly to make it very easy to plot data with different averaging times and statistics. 22.6 Calculating percentiles calcPercentile makes it straightforward to calculate percentiles for a single pollutant. It can take account of different averaging periods, data capture thresholds — see Section 22.5 for more details. For example, to calculate the 25, 50, 75 and 95th percentiles of O3 concentration by year: calcPercentile(mydata, pollutant = &quot;o3&quot;, percentile = c(25, 50, 75, 95), avg.time = &quot;year&quot;) ## date percentile.25 percentile.50 percentile.75 percentile.95 ## 1 1998-01-01 2 4 7 16 ## 2 1999-01-01 2 4 9 21 ## 3 2000-01-01 2 4 9 22 ## 4 2001-01-01 2 4 10 24 ## 5 2002-01-01 2 4 10 24 ## 6 2003-01-01 2 4 11 24 ## 7 2004-01-01 2 5 11 23 ## 8 2005-01-01 3 7 16 28 22.7 Correlation matrices Understanding how different variables are related to one another is always important. However, it can be difficult to easily develop an understanding of the relationships when many different variables are present. One of the useful techniques used is to plot a correlation matrix, which provides the correlation between all pairs of data. The basic idea of a correlation matrix has been extended to help visualise relationships between variables by Friendly (2002) and Sarkar (2007). The corPlot shows the correlation coded in three ways: by shape (ellipses), colour and the numeric value. The ellipses can be thought of as visual representations of scatter plot. With a perfect positive correlation a line at 45 degrees positive slope is drawn. For zero correlation the shape becomes a circle — imagine a ‘fuzz’ of points with no relationship between them. With many variables it can be difficult to see relationships between variables i.e. which variables tend to behave most like one another. For this reason hierarchical clustering is applied to the correlation matrices to group variables that are most similar to one another (if cluster = TRUE.) It is also possible to use the openair type option to condition the data in many flexible ways, although this may become difficult to visualise with too many panels. An example of the corPlot function is shown in Figure 22.2. In this Figure it can be seen the highest correlation coefficient is between PM10 and PM2.5 (r = 0. 84) and that the correlations between SO2, NO2 and NOx are also high. O3 has a negative correlation with most pollutants, which is expected due to the reaction between NO and O3. It is not that apparent in Figure 22.2 that the order the variables appear is due to their similarity with one another, through hierarchical cluster analysis. In this case we have chosen to also plot a dendrogram that appears on the right of the plot. Dendrograms provide additional information to help with visualising how groups of variables are related to one another. Note that dendrograms can only be plotted for type = \"default\" i.e. for a single panel plot. corPlot(mydata, dendrogram = TRUE) Figure 22.2: Example of a correlation matrix showing the relationships between variables. Note also that the corPlot accepts a type option, so it possible to condition the data in many flexible ways, although this may become difficult to visualise with too many panels. For example: corPlot(mydata, type = &quot;season&quot;) When there are a very large number of variables present, the corPlot is a very effective way of quickly gaining an idea of how variables are related. As an example (not plotted) it is useful to consider the hydrocarbons measured at Marylebone Road. There is a lot of information in the hydrocarbon plot (about 40 species), but due to the hierarchical clustering it is possible to see that isoprene, ethane and propane behave differently to most of the other hydrocarbons. This is because they have different (non-vehicle exhaust) origins. Ethane and propane results from natural gas leakage whereas isoprene is biogenic in origin (although some is from vehicle exhaust too). It is also worth considering how the relationships change between the species over the years as hydrocarbon emissions are increasingly controlled, or maybe the difference between summer and winter blends of fuels and so on. hc &lt;- importAURN(site = &quot;my1&quot;, year = 2005, hc = TRUE) ## now it is possible to see the hydrocarbons that behave most ## similarly to one another corPlot(hc) References "],
["references.html", "References", " References "],
["annotating-openair-plots.html", "A Annotating openair plots A.1 Adding text A.2 Adding text and a shaded area A.3 Adding an arrow A.4 Adding a reference line and text A.5 Highlight a specific point A.6 Add a filled polygon A.7 Add air quality bands as polygons A.8 Polar plot examples A.9 Using grid graphics — identify locations interactively", " A Annotating openair plots A frequently asked question about openair and requested feature is how to annotate plots. While all openair functions could have options to allow annotations to be made, this would make the functions cumbersome and reduce flexibility. Nevertheless, it is useful to be able to annotate plots in lots of different ways. Fortunately there are existing functions in packages such as lattice and latticeExtra that allow for plots to be updated. An example of the sorts of annotation that are possible is shown in Figure A.1, which is an enhanced version of Figure 10.1. These annotations have been subsequently added to Figure 10.1 and built up in layers. This section considers how to annotate openair plots more generally and uses Figure A.1 as an example of the types of annotation possible. Also considered specifically is the annotation of plots that are in polar coordinates, as these can sometimes benefit from different types of annotation. There are several types of objects that can be useful to add to plots including text, shapes, lines and other shading. Given that many openair plots can consist of multiple panels, it is also useful to think about how to annotate specific panels. The examples given in this section will apply to all openair plot, the only difference being the coordinate system used in each case. The basis of openair annotations is through the use of the latticeExtra package, which should already be installed as part of openair. In that package there is a function called layer that effectively allows annotations to be built up ‘layer by layer’. ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union Figure A.1: Annotations added to the timePlot output. A.1 Adding text To add text (or other annotations) it is necessary to know the coordinates on a plot for where the text will go, which will depend on the data plotted. In this extended example using the timePlot function, the y-axis will be in ordinary numerical units, whereas the x-axis will be in a date-time format (POSIXct). There are various ways that annotations can be added, but the method used here is to add to the previous plot using a function called trellis.last.object() to which we want to add a later. This may seem complicated, but once a few examples are considered, the method becomes very powerful, flexible and straightforward. In a multi-panel plot such as Figure A.1 it is also useful to specify which rows/columns should be added to. If they are not specified then the annotation will appear in all panels. First, a plot should be produced to which we wish to add some text. ## make sure latticeExtra is loaded library(openair) library(lubridate) library(latticeExtra) timePlot(selectByDate(mydata, year = 2003, month = &quot;aug&quot;), pollutant = c(&quot;nox&quot;, &quot;o3&quot;, &quot;pm25&quot;, &quot;pm10&quot;, &quot;ws&quot;)) So, considering A.1, this is how the text `some missing data’ was added to the top panel. trellis.last.object() + layer(ltext(x = ymd(&quot;2003-08-04&quot;), y = 200, labels = &quot;some missing data&quot;), rows = 1) So what does this do? First, the trellis.last.object() is simply the last plot that was plotted. Next the layer function is used to add some text. The text itself is added using the ltext lattice function. It is worth having a look at the help for ltext as that gives an overview of all the common annotations and other options. We have chosen to plot the text at position x = ‘2003-08-04’ and y = 200 and the label itself. A useful option to ltext is pos. Values can be 1, 2, 3 and 4, and indicate positions below (the default), to the left of, above and to the right of the specified coordinates A.2 Adding text and a shaded area This time we will highlight an interval in row 2 (O3) and write some text on top. Note that this time we use the lpolygon function and choose to put it under everything else on the plot. For the text, we have chosen a colour (yellow) font type 2 (bold) and made it a bit bigger (cex = 1.5). Note also the y values extend beyond the actual limits shown on the plot — just to make sure they cover the whole region. The polygon could of course be horizontal and more than one producing a series of ‘bands’ e.g. air quality indexes. A more sophisticated approach is shown later for PM2.5 ## add shaded polygon trellis.last.object() + layer(lpolygon(x = c(ymd(&quot;2003-08-07&quot;), ymd(&quot;2003-08-07&quot;), ymd(&quot;2003-08-12&quot;), ymd(&quot;2003-08-12&quot;)), y = c(-20, 600, 600, -20), col = &quot;grey&quot;, border = NA), under = TRUE, rows = 2) ## add text trellis.last.object() + layer(ltext(x = ymd_hm(&quot;2003-08-09 12:00&quot;), y = 50, labels = &quot;!!episode!!&quot;, col = &quot;yellow&quot;, font = 2, cex = 1.5), rows = 2) The small shaded, semi-transparent area shown in the bottom panel was added as follows: ## add shaded polygon plt &lt;- plt + layer(lpolygon(x = c(ymd(&quot;2003-08-21&quot;), ymd(&quot;2003-08-21&quot;), ymd(&quot;2003-08-23&quot;), ymd(&quot;2003-08-23&quot;)), y = c(4, 8, 8, 4), col = &quot;blue&quot;, border = NA, alpha = 0.2), rows = 5) A.3 Adding an arrow The arrow shown on the first panel of Figure A.1 was added as follows. Note the code = 3 placed arrows at both ends. Note that angle is the angle from the shaft of the arrow to the edge of the arrow head. trellis.last.object() + layer(larrows(ymd(&quot;2003-08-01&quot;), 100, ymd_hm(&quot;2003-08-08 14:00&quot;), 100, code = 3, angle = 30), rows = 1) A.4 Adding a reference line and text This code adds a vertical dashed reference line shown in the 4th panel (PM10) along with some text aligned at 90 degrees using the srt option of ltext. trellis.last.object() + layer(panel.abline(v = ymd(&quot;2003-08-25&quot;), lty = 5), rows = 4) trellis.last.object() + layer(ltext(x = ymd_hm(&quot;2003-08-25 08:00&quot;), y = 60, labels = &quot;reference line&quot;, srt = 90), rows = 4) A.5 Highlight a specific point Up until now annotations have been added using arbitrary coordinates in each panel. What if we wanted to highlight a particular point, or more generally work with the actual data that are plotted. Knowing how to refer to existing data greatly extends the power of these functions. It is possible to refer to a specific point in a panel simply by indexing the point of interest i.e. x, y. For example, to mark the 200th PM10 concentration (without knowing the actual date or value): ## add a specfic point trellis.last.object() + layer(lpoints(x[200], y[200], pch = 16, cex = 1.5), rows = 4) What if we wanted to highlight the maximum O3 concentration? It is possible to work out the index first and then use that to refer to that point. Note the } to allow for the code to span multiple commands. ## add a point to the max O3 concentration trellis.last.object() + layer({maxy &lt;- which.max(y); lpoints(x[maxy], y[maxy], col = &quot;black&quot;, pch = 16)}, rows = 2) ## label max ozone trellis.last.object() + layer({maxy &lt;- which.max(y); ltext(x[maxy], y[maxy], paste(y[maxy], &quot;ppb&quot;), pos = 4)}, rows = 2) A.6 Add a filled polygon It can be seen in the top panel of Figure A.1 that some data are highlighted by filling the area below the line. This approach can be useful more generally in plotting. While it is possible to draw polygons easily and refer to the data itself, there needs to be a way for dealing with gaps in data, otherwise these gaps could be filled in perhaps unpredictable ways. A function has been written to draw a polygon taking into account gaps (poly.na). poly.na &lt;- function(x1, y1, x2, y2, col = &quot;black&quot;, alpha = 0.2) { for(i in seq(2, length(x1))) if (!any(is.na(y2[c(i - 1, i)]))) lpolygon(c(x1[i - 1], x1[i], x2[i], x2[i - 1]), c(y1[i - 1], y1[i], y2[i], y2[i - 1]), col = col, border = NA, alpha = alpha) } This time we work out the ids of the data spanning an area of interest. Then the poly.na function is used. Note that the alpha transparency is by default 0.2 but another value can easily be supplied, as shown in the air quality `bands’ example. trellis.last.object() + layer({id &lt;- which(x &gt;= ymd(&quot;2003-08-11&quot;) &amp; x &lt;= ymd(&quot;2003-08-25&quot;)); poly.na(x[id], y[id], x[id], rep(0, length(id)), col = &quot;darkorange&quot;)}, rows = 1) A.7 Add air quality bands as polygons It is a simple extension to go from using a polygon below the data to polygons at certain intervals e.g. air quality indexes. These are shown for PM2.5 and the bands considered are 0–20, 20–30, 30–40 and &gt;40. trellis.last.object() + layer(poly.na(x, y, x, rep(0, length(x)), col = &quot;green&quot;, alpha = 1), rows = 3) trellis.last.object() + layer(poly.na(x, ifelse(y &lt;20, NA, y), x, rep(20, length(x)), col = &quot;yellow&quot;, alpha = 1), rows = 3) trellis.last.object() + layer(poly.na(x, ifelse(y &lt;30, NA, y), x, rep(30, length(x)), col = &quot;orange&quot;, alpha = 1), rows = 3) trellis.last.object() + layer(poly.na(x, ifelse(y &lt;40, NA, y), x, rep(40, length(x)), col = &quot;red&quot;, alpha = 1), rows = 3) A.8 Polar plot examples Many of the examples considered above are relevant to all other functions e.g. how to add text, choosing rows and columns to plot in. Polar coordinate plots are different because of the coordinate system used and this section considers a few examples. One useful approach is to be able to draw an arc, perhaps highlighting an area of interest. A simple, but flexible function has been written to do this. It takes arguments theta1 and theta2 that define the angular area of interest and lower and upper to set the lower and upper wind speed, respectively. It also has additional arguments theta3 and theta4 which optionally set the angles for the ‘upper’ wind speed. arc &lt;- function(theta1 = 30, theta2 = 60, theta3 = theta1, theta4 = theta2, lower = 1, upper = 10){ ## function to work out coordinates for an arc sector if (theta2 &lt; theta1) { ang1 &lt;- seq(theta1, 360, length = abs(theta2 - theta1)) ang2 &lt;- seq(0, theta2, length = abs(theta2 - theta1)) angles.low &lt;- c(ang1, ang2) ## for upper angles ang1 &lt;- seq(theta1, 360, length = abs(theta4 - theta3)) ang2 &lt;- seq(0, theta2, length = abs(theta4 - theta3)) angles.high &lt;- c(ang1, ang2) } else { angles.low &lt;- seq(theta1, theta2, length = abs(theta2 - theta1)) angles.high &lt;- seq(theta3, theta4, length = abs(theta4 - theta3)) } x1 &lt;- lower * sin(pi * angles.low / 180) y1 &lt;- lower * cos(pi * angles.low / 180) x2 &lt;- rev(upper * sin(pi * angles.high / 180)) y2 &lt;- rev(upper * cos(pi * angles.high / 180)) data.frame(x = c(x1, x2), y = c(y1, y2)) } Figure A.2: Annotations on a polar plot. Following on from the previous examples, some annotations have been added to the basic polar plor for SO2 as shown in Figure A.2. Note that in these plots (0, 0) is the middle of the plot and the radial distance will be determined by the wind speed — or whatever the radial variable is. This way of plotting arcs can also be applied to other functions that show directional data. polarPlot(mydata, pollutant = &quot;so2&quot;, col = &quot;jet&quot;) trellis.last.object() + layer(ltext(-12, -12, &quot;A&quot;, cex = 2)) trellis.last.object() + layer(ltext(10, 2, &quot;B&quot;, cex = 2, col = &quot;white&quot;)) trellis.last.object() + layer(lsegments(0, 0, -11.5, -11.5, lty = 5)) ## add and arc to highlight area of interest trellis.last.object() + layer(lpolygon(x = arc(theta1 = 60, theta2 = 120, lower = 2, upper = 15)$x, y = arc(theta1 = 60, theta2 = 120, lower = 2, upper = 15)$y, lty = 1, lwd = 2)) A.9 Using grid graphics — identify locations interactively The examples above provide a precise way of annotating plots for single or multi-panels openair displays. However, these methods won’t work for plots that consist of completely separate plots such as the four plots in timeVariation. There are however other methods that can be used to annotate such plots using the package grid, which forms the basis of lattice graphics. There is enormous capability for annotating plots using the grid package and only a few simple examples are given here. Given a plot such as Figure 11.1, how could texts be added at any location — say in the middle monthly plot? One very useful function for this type of annotation that allows the user to interactively choose a location is the grid.locator() function in the grid package. That function can be called with different coordinate systems — but the one we want defines the bottom-left corner as (0, 0) and the top right as (1, 1). First, make a timeVariation plot like Figure 11.1. timeVariation(mydata) Now let’s choose a location on the plot interactively using the mouse and selecting somewhere in the middle of the monthly plot. library(grid) ## bring up the interactive location chooser grid.locator(unit = &quot;npc&quot;) What should happen is that in the R console the coordinates are given for that point. In my case these were x = 0.503 and y = 0.338. These coordinates can now be used as the basis of adding some text or other annotation. In the example below, the grid.text function is used to add some text for these coordinates making the font bigger (cex = 2), bold (font = 2) and blue (col = \"blue\"). grid.text(x = 0.503, y = 0.338, label = &quot;here!&quot;, gp = gpar(cex = 2, font = 2, col = &quot;blue&quot;)) Even with this basic approach, some sophisticated annotation is possible with any openair plot. There are many other functions that can be used from the grid package that would allow for polygons, segments and other features to be drawn is a similar way to the examples earlier in this section. Continuing with the same example, here is how to add an arrow pointing to the maximum concentration shown on the top plot for Saturday (again using the grid.locator function). grid.lines(x = c(0.736, 0.760), y = c(0.560, 0.778), arrow = arrow()) grid.text(x = 0.736, y = 0.560, label = &quot;maximum&quot;, just = &quot;left&quot;) "],
["sec-prod-hyspl-traj.html", "B Production of HYSPLIT trajectory files", " B Production of HYSPLIT trajectory files As discussed in Section 18, openair can import pre-calculated trajectory data for specified locations. The data are stored on a Ricardo webserver to make it easy to import 96-hour back trajectory data. Several users have requested how they can run HYSPLIT themselves e.g. for different trajectory start heights or for many locations. This section provides the code necessary to run the HYSPLIT model. The code below assumes that full years are run, but it could be adopted for shorter periods. There are three main parts to producing trajectory files: Download and install the NOAA Hysplit model, somewhere with write access (see below). Download the monthly meteorological (.gbl) files also from the NOAA website. Obtain the code to run Hysplit. To run back trajectories it is necessary to download the meteorological data files. The easiest way to download the meteorological files is using the function below. getMet &lt;- function (year = 2013, month = 1, path_met = &quot;~/TrajData/&quot;) { for (i in seq_along(year)) { for (j in seq_along(month)) { download.file(url = paste0(&quot;ftp://arlftp.arlhq.noaa.gov/archives/reanalysis/RP&quot;, year[i], sprintf(&quot;%02d&quot;, month[j]), &quot;.gbl&quot;), destfile = paste0(path_met, &quot;RP&quot;, year[i], sprintf(&quot;%02d&quot;, month[j]), &quot;.gbl&quot;), mode = &quot;wb&quot;) } } } The function will download monthly met files (each about 120 MB) to the chosen directory. Note that the met data files only need be downloaded once. For example, to download files for 2013: getMet(year = 2013, month = 1:12) The original functions have been modified by Stuart Grange (PhD student at the University of York). It is first necessary on ensure that the stringr and devtools packages are installed. The latter is needed to load some R functions stored as a GitHub gist (some code that can be shared publicly). library(devtools) source_gist(&quot;https://gist.github.com/davidcarslaw/c67e33a04ff6e1be0cd7357796e4bdf5&quot;, filename = &quot;run_hysplit.R&quot;) ## Sourcing https://gist.githubusercontent.com/davidcarslaw/c67e33a04ff6e1be0cd7357796e4bdf5/raw/1eb36e223725308934bfa189c30969014e979198/run_hysplit.R ## SHA-1 hash of file is e1d37c75528ac18924fd0c6902318702301fa591 Now there should be several loaded functions, including run_hysplit. To run Hysplit, have a look at the examples here https://gist.github.com/davidcarslaw/c67e33a04ff6e1be0cd7357796e4bdf5. On my Windows machine it is run as follows: data_out &lt;- run_hysplit( latitude = 36.134, longitude = -5.347, runtime = -96, start_height = 10, model_height = 10000, start = 2015, end = &quot;2015-01-10&quot;, hysplit_exec = &quot;~/hysplit4/exec&quot;, hysplit_input = &quot;~/trajData&quot;, hysplit_output = &quot;~/temp&quot;, site = &quot;gibraltar&quot;) The data_out can then be used directly in openair trajectory functions. Most of the options should be self-explanatory but hysplit_exec is the path to the Hysplit executable, hysplit_input is the path to the meteorological files (downloaded as described above) and hysplit_output is the directory where Hysplit will write its temporary files. Once run it is then advisable to store the data somewhere. Save it like: saveRDS(data_out, file = &quot;~/trajProc/myTrajData.rds&quot;) Then it is easy to read in later and use e.g. traj &lt;- readRDS(&quot;~/trajProc/myTrajData.rds&quot;) Applequist, Scott. 2012. “Wind Rose Bias Correction.” Journal of Applied Meteorology and Climatology 51 (7): 1305–9. Ara Begum, Bilkis, Eugene Kim, Cheol-Heon Jeong, Doh-Won Lee, and Philip K. Hopke. 2005. “Evaluation of the potential source contribution function using the 2002 Quebec forest fire episode.” Atmospheric Environment 39 (20): 3719–24. https://doi.org/10.1016/j.atmosenv.2005.03.008. Ashbaugh, Lowell L., William C. Malm, and Willy Z. Sadeh. 1985. “A residence time probability analysis of sulfur concentrations at grand Canyon National Park.” Atmospheric Environment (1967) 19 (8): 1263–70. https://doi.org/10.1016/0004-6981(85)90256-2. Carslaw, David. 2020. Worldmet: Import Surface Meteorological Data from Noaa Integrated Surface Database (Isd). http://github.com/davidcarslaw/worldmet. Carslaw, D. C., and S. D. Beevers. 2013. “Characterising and Understanding Emission Sources Using Bivariate Polar Plots and K-Means Clustering.” Environmental Modelling &amp; Software 40 (0): 325–29. https://doi.org/10.1016/j.envsoft.2012.09.005. Carslaw, D. C., S. D. Beevers, K. Ropkins, and M. C. Bell. 2006. “Detecting and Quantifying Aircraft and Other on-Airport Contributions to Ambient Nitrogen Oxides in the Vicinity of a Large International Airport.” Atmospheric Environment 40 (28): 5424–34. Carslaw, D. C., S. D. Beevers, and J. E. Tate. 2007. “Modelling and Assessing Trends in Traffic-Related Emissions Using a Generalised Additive Modelling Approach.” Atmospheric Environment 41 (26): 5289–99. Carslaw, D. C., and K. Ropkins. 2012. “openair — An R package for air quality data analysis.” Environmental Modelling &amp; Software 27–28 (0): 52–61. https://doi.org/10.1016/j.envsoft.2011.09.008. COMEAP. 2011. “Review of the Uk Air Quality Index: A Report by the Committee on the Medical Effects of Air Pollutants.” http://comeap.org.uk/documents/reports/130-review-of-the-uk-air-quality-index.html. Droppo, James G, and Bruce A Napier. 2008. “Wind Direction Bias in Generating Wind Roses and Conducting Sector-Based Air Dispersion Modeling.” Journal of the Air &amp; Waste Management Association 58 (7): 913–18. Fleming, Z. L., P. S. Monks, and A. J. Manning. 2012. “Review: Untangling the influence of air-mass history in interpreting observed atmospheric composition.” Atmospheric Research 104-105: 1–39. https://doi.org/10.1016/j.atmosres.2011.09.009. Friendly, M. 2002. “Corrgrams: Exploratory Displays for Correlation Matrices.” The American Statistician 56 (4): 316–25. Grange, Stuart K, Alastair C Lewis, and David C Carslaw. 2016. “Source Apportionment Advances Using Polar Plots of Bivariate Correlation and Regression Statistics.” Atmospheric Environment 145: 128–34. Hastie, T. J., and R. J. Tibshirani. 1990. Generalized Additive Models. London: Chapman; Hall. Henry, Ronald, Gary A. Norris, Ram Vedantham, and Jay R. Turner. 2009. “Source Region Identification Using Kernel Smoothing.” Article. Environmental Science &amp; Technology 43 (11): 4090–7. https://doi.org/{10.1021/es8011723}. Hirsch, R. M., J. R. Slack, and R. A. Smith. 1982. “Techniques of Trend Analysis for Monthly Water-Quality Data.” Water Resources Research 18 (1): 107–21. Hsu, Ying-Kuang, Thomas M. Holsen, and Philip K. Hopke. 2003. “Comparison of hybrid receptor models to locate PCB sources in Chicago.” Atmospheric Environment 37 (4): 545–62. https://doi.org/10.1016/S1352-2310(02)00886-5. Kunsch, H. R. 1989. “The Jackknife and the Bootstrap for General Stationary Observations.” Annals of Statistics 17 (3): 1217–41. Legates, D. R., and G. J. McCabe. 2012. “A Refined Index of Model Performance: A Rejoinder.” International Journal of Climatology. Legates, D. R., and G. J. McCabe Jr. 1999. “Evaluating the Use of ‘Goodness-of-Fit’ Measures in Hydrologic and Hydroclimatic Model Validation.” Water Resources Research 35 (1): 233–41. Lupu, Alexandru, and Willy Maenhaut. 2002. “Application and comparison of two statistical trajectory techniques for identification of source regions of atmospheric aerosol species.” Atmospheric Environment 36: 5607–18. Pekney, Natalie J., Cliff I. Davidson, Liming Zhou, and Philip K. Hopke. 2006. “Application of PSCF and CPF to PMF-Modeled Sources of PM 2.5 in Pittsburgh.” Aerosol Science and Technology 40 (10): 952–61. https://doi.org/10.1080/02786820500543324. Sarkar, Deepayan. 2007. Lattice Multivariate Data Visualization with R. New York: Springer. Seibert, P, H Kromp-Kolb, U Baltensperger, and DT Jost. 1994. “Trajectory Analysis of High-Alpine Air Pollution Data.” NATO Challenges of Modern Society 18: 595–95. Sen, P. K. 1968. “Estimates of Regression Coefficient Based on Kendall’s Tau.” Journal of the American Statistical Association 63(324): 1379–89. Taylor, K. E. 2001. “Summarizing Multiple Aspects of Model Performance in a Single Diagram.” Journal of Geophysical Research 106 (D7): 7183–92. Theil, H. 1950. “A Rank Invariant Method of Linear and Polynomial Regression Analysis, I, Ii, Iii.” Proceedings of the Koninklijke Nederlandse Akademie Wetenschappen, Series A – Mathematical Sciences 53: 386–92, 521–25, 1397–1412. Uria-Tellaetxe, I, and D. C. Carslaw. 2014. “Conditional Bivariate Probability Function for Source Identification.” Environmental Modelling &amp; Software 59: 1–9. https://doi.org/10.1016/j.envsoft.2014.05.002. Westmoreland, E. J., N Carslaw, D. C. Carslaw, A. Gillah, and E. Bates. 2007. “Analysis of Air Quality Within a Street Canyon Using Statistical and Dispersion Modelling Techniques.” Atmospheric Environment 41 (39): 9195–9205. Wilcox, Rand R. 2010. Fundamentals of Modern Statistical Methods: Substantially Improving Power and Accuracy. 2nd ed. Springer New York. http://www.springerlink.com/content/978-1-4419-5524-1. Wilks, Daniel S. 2005. Statistical Methods in the Atmospheric Sciences, Volume 91, Second Edition (International Geophysics). 2nd ed. Hardcover; Academic Press. Willmott, Cort J, Scott M Robeson, and Kenji Matsuura. 2011. “A Refined Index of Model Performance.” International Journal of Climatology. Wood, S. N. 2006. Generalized Additive Models: An Introduction with R. Chapman; Hall/CRC. Yu, K. N., Y. P. Cheung, T. Cheung, and R. C. Henry. 2004. “Identifying the Impact of Large Urban Airports on Local Air Quality by Nonparametric Regression.” Atmospheric Environment 38 (27): 4501–7. "]
]
