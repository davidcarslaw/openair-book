[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The openair book",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#hello-and-welcome",
    "href": "index.html#hello-and-welcome",
    "title": "The openair book",
    "section": "Hello and welcome",
    "text": "Hello and welcome\n\nThis document has been a long time coming. The openair project started with funding from the UK Natural Environment Environment Research Council (NERC) over 10 years ago. The main aim was to fill a perceived gap in that there was a lack of a dedicated set of easily accessible, open source tools for analysing air quality data. At that time R was becoming increasingly popular but far, far less than it is today.\nThe book is split into broad sections that cover common aspects of air quality data analysis.\n\nData Import Mostly focused on the easy access of UK air quality data across national and regional networks and accessing global meteorological data.\nDirectional Analysis Many common functions such as wind and pollution roses, bivariate polar plots and back trajectories.\nTime Series and Trends Various ways of considering changes in time and flexible methods for trend assessment.\nModel Evaluation Functions such as Taylor Diagrams, common model evaluation statistics and conditional quantile plots.\nInteractive Maps Considers effective ways of viewing UK air quality networks and plotting directional analyses on interactive plots; particularly useful for source characterisation.\nUtility functions various functions to flexibly carry out time-averaging, correlation analysis and other common tasks of relevance to air pollution.\n\nopenair does not of course cover every conceivable analysis that users might be interested in, but a selection of flexible methods that should have wide appeal. However, by using R as the basis of development, users can greatly extend the types of analysis in many powerful and innovative ways.\nopenair is a product of the time it was started and used the highly capable lattice package for plotting. This was a time before ggplot2 and the ‘tidyverse’. Nevertheless, the package is used extensively around the world (see downloads here) and if anything, growing in popularity and use. The original aims of the project have been met in providing tools for academia, the private and public sectors — all of which continue to use the software.\nAt some point there will need to be a transition to ggplot2; in particular to capitalise on the many extensions available and which openair can benefit from.\nThe reason for writing this book (or manual) in this form i.e. a website rather than a pdf or Word document is convenience for all involved. For me it makes it much easier to keep the information up to date and ensure that the information is reproducible. For the reader it is something that can easily be read and navigated in a browser. Where code is involved — as it is heavily in this book — it is very easy to use the copy icon at the top right of each code block to make it easy to copy into R. Finally, it is increasingly the case that information can be plotted interactively, which is not something that can easily be done in a pdf or Word document.\nTo cite openair please use:\nCarslaw, D. C., and K. Ropkins. 2012. “openair — An R package for air quality data analysis.” Environmental Modelling & Software 27–28 (0): 52–61. https://doi.org/10.1016/j.envsoft.2011.09.008.\nThis document was produced using R version 4.2.2 and openair version 2.15.0.9002."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This document provides information on the use of R to analyse air pollution data. The document supports an initiative to develop and make available a consistent set of tools for analysing and understanding air pollution data in a free, open-source environment.\nThe amount of monitoring data available is substantial and increasing. In the UK alone there are thought to be over 1000 continuous monitoring sites. Much of the data available is only briefly analysed; perhaps with the aim of comparing pollutant concentrations with national and international air quality limits. However, as it will hopefully be seen, the critical analysis of air pollution data can be highly rewarding, perhaps yielding important information on pollutant sources that was previously unknown or unquantified.\nWhile many of the options in these functions allow quite a sophisticated analysis to be undertaken, the defaults generally use the simplest (and fastest) assumptions. A more detailed analysis can refine these assumptions e.g. by accounting for autocorrelation, or fine-control over the appearance of a plot.\nIt should be noted that while the aim is to keep this documentation up to date, the primary source of information related to the different functions is contained within the package itself. Once loaded, type ?openair to see all the help pages associated with the package. The website for openair is https://davidcarslaw.github.io/openair/.\n\n\n\n\n\n\nNote!\n\n\n\nThe next Section contains important information on loading the openair package for the first time and the input data requirements. Users will need to consider the advice in this section to ensure that openair can be used without problems."
  },
  {
    "objectID": "openair-package.html#installation-and-code-access",
    "href": "openair-package.html#installation-and-code-access",
    "title": "2  The openair package",
    "section": "\n2.1 Installation and code access",
    "text": "2.1 Installation and code access\nopenair is avaiable on CRAN (Comprehensive R Archive network) which means it can be installed easily from R. I would recommend re-starting R and then type install.packages(\"openair\"). If you use RStudio (which is highly recommended), you can just choose the ‘packages’ tab on the bottom-right, and then select ‘Install’. Simply start typing openair and you will find the package.\nFor openair all development is carried out using Github for version control. Users can access all code used in openair at (https://github.com/davidcarslaw/openair).\nSometimes it might be useful to install the development version of openair and you can find instructions here."
  },
  {
    "objectID": "openair-package.html#input-data-requirements",
    "href": "openair-package.html#input-data-requirements",
    "title": "2  The openair package",
    "section": "\n2.2 Input data requirements",
    "text": "2.2 Input data requirements\nThe openair package applies certain constraints on input data requirements. It is important to adhere to these requirements to ensure that data are correctly formatted for use in openair. The principal reason for insisting on specific input data format is that there will be less that can go wrong and it is easier to write code for a more limited set of conditions.\n\nData should be in a data frame (or tibble).\nThe date/time field should be called date — note the lower case. No other name is acceptable.\nThe wind speed and wind direction should be named ws and wd, respectively (note again, lower case). Wind directions follow the UK Met Office format and are represented as degrees from north e.g. 90 degrees is east. North is taken to be 360 degrees\nWhere fields should have numeric data e.g. concentrations of NOx, then the user should ensure that no other characters are present in the column, accept maybe something that represents missing data e.g. ‘no data’.\nOther variables names can be upper/lower case but should not start with a number. If column names do have white spaces, R will automatically replace them with a full-stop. While PM2.5 as a field name is perfectly acceptable, it is a pain to type it in—better just to use pm25 (openair will recognise pollutant names like this and automatically format them as PM2.5 in plots)."
  },
  {
    "objectID": "openair-package.html#reading-and-formatting-dates-and-times",
    "href": "openair-package.html#reading-and-formatting-dates-and-times",
    "title": "2  The openair package",
    "section": "\n2.3 Reading and formatting dates and times",
    "text": "2.3 Reading and formatting dates and times\nWhile not specific to openair, dealing with dates and times is likely to be an issue that needs to be dealt with at some point. There is no getting away from the fact that dates and times can be complicated with issues such as time zones and daylight saving time i.e. when the clocks change for summer. This is a potentially big topic to consider and it is only considered in outline here.\nFor a lot of openair functions this issue will not be important. While these issues can often be ignored, it is better to be explicit and set the date-time correctly. Two situations where it becomes important is when wanting to show temporal variations in local time and combining data sets that are in different time zones. The former issue can be important (for example) when considering diurnal variations in a pollutant concentration that follows a human activity (such as rush-hour traffic), which follows local time and not GMT/UTC.\n\n\n\n\n\n\nKnow your data!\n\n\n\nWhen importing data into R it is important to know how the date-time is represented in your original data, especially in terms of time zone.\n\n\nWhen importing data it is important to know how the date-time is represented. In the UK it is easy for us to forget that simply working with data in GMT is not always an option. However, most air quality and meteorological data around the world tends to be in GMT/UTC or a fixed offset from GMT/UTC i.e. not in local time where hours can be missing or duplicated.\nLife is made much easier using the lubridate package, which has been developed for working with dates and times. The lubridate package has a family of functions that will convert common formats of dates and times into a R-formatted version. These functions are useful when importing data and the date-time is in a character format and needs formatting. Here are some examples:\nOriginal date in ‘British’ format (day/month/ year hours-minutes):\n\nlibrary(lubridate) # load package\ndate_time <- \"2/8/2022 11:00\"\n\n# format it\ndmy_hm(date_time)\n\n[1] \"2022-08-02 11:00:00 UTC\"\n\n\nWhen R formats a date-time correctly is will be shown from ‘large to small’ i.e. YYYY-MM-DD HH:MM:SS, which provides a clue that it has indeed been formatted correctly.\nUS date time with seconds (month-day-year):\n\ndate_time <- \"8/2/2022 11:05:12\"\nmdy_hms(date_time)\n\n[1] \"2022-08-02 11:05:12 UTC\"\n\n\nAs you can see, by default, the date-time is formatted in UTC (GMT). It is at this point where you can also set a time zone of the original data if it was not in GMT. Let’s assume the original data were a fixed off-set from GMT of -8 hours (west coast USA perhaps). This can be done by setting the time zone explicitly1:\n\ndate_time <- \"8/2/2022 11:05:12\" # time 8 hours behind GMT\nmdy_hms(date_time, tz = \"Etc/GMT+8\")\n\n[1] \"2022-08-02 11:05:12 -08\"\n\n\nwhich actually shows the GMT offset of -8 hours.\nA common task might be to plot time series and temporal variations of pollutant concentrations in local time. How does one do this if the imported data are in GMT/UTC (or a fixed offset from GMT/UTC)?\nIn this case it is necessary to know how the local time zone with daylight saving time (DST) is represented. Time zone names follow the Olson scheme — you can list them by typing OlsonNames(). Given the scenario where we have imported data in GMT but want to display the data in local time (BST — British Summer Time), we can use the with_tz function in lubridate to do this:\n\ndate_time <- \"2/8/2022 11:00\"\n\n# format it\ndate_time <- dmy_hm(date_time) # GMT\ndate_time\n\n[1] \"2022-08-02 11:00:00 UTC\"\n\n# what is the hour?\nhour(date_time)\n\n[1] 11\n\n# format in local time\ntime_local <- with_tz(date_time, tz = \"Europe/London\")\ntime_local\n\n[1] \"2022-08-02 12:00:00 BST\"\n\n# local hour is +1 from GMT\nhour(time_local)\n\n[1] 12\n\n\nIn the above example, date_time and local_time are the same absolute time — we are just changing how the time is displayed. In practice, given a data frame with a date column in GMT and there interest in making sure openair uses the local time, some formatting such as mydata$date -> with_tz(mydata$date, tz = \"Europe/London\") is what is needed.\nAnother scenario is you import data using a function such as read_csv from the readr package and it recognises a date-time in the data and by default assumes it is GMT/UTC. This might be wrong even though it is now formatted correctly in R. In this case you can force a new time zone using the force_tz function. For example:\n\n# a correctly formatted date-time that is in GMT but should be something else\ndate_time\n\n[1] \"2022-08-02 11:00:00 UTC\"\n\n# force the time zone to be something different\nforce_tz(date_time, tz = \"Etc/GMT+8\")\n\n[1] \"2022-08-02 11:00:00 -08\"\n\n\nFinally, what about combining data sets in different time zones? In Chapter 4 it is shown how it is possible to access meteorological data from around the world (all in GMT). The interest might be in combining this data with air quality data that is in another time zone. So long as the date-times were correctly formatted in the first place, then simply joining the data sets by date is all that is needed, as R works out how the times match internally. An example of joining two data sets is shown in Section 4.2."
  },
  {
    "objectID": "openair-package.html#brief-intr-open",
    "href": "openair-package.html#brief-intr-open",
    "title": "2  The openair package",
    "section": "\n2.4 Brief overview of openair",
    "text": "2.4 Brief overview of openair\nThis section gives a brief overview of the functions in openair. Having read some data into a data frame it is then straightforward to run any function. Almost all functions are run as:\n\nfunctionName(thedata, options, ...)\n\nThe usage is best illustrated through a specific example, in this case the polarPlot function. The details of the function are shown in Chapter 8) and through the help pages (type ?polarPlot). As it can be seen there are numerous options associated with polarPlot — and most other functions and each of these has a default. For example, the default pollutant considered in polarPlot is nox. If the user has a data frame called theData then polarPlot could minimally be called by:\n\npolarPlot(theData)\n\nwhich would plot a nox polar plot if nox was available in the data frame theData.\nNote that the options do not need to be specified in order nor is it always necessary to write the whole word. For example, it is possible to write:\n\npolarPlot(theData, type = \"year\", poll = \"so2\")\n\nIn this case writing poll is sufficient to uniquely identify that the option is pollutant.\nAlso there are many common options available in functions that are not explicitly documented, but are part of lattice graphics. Some common ones are summarised in Table 2.1. The layout option allows the user to control the layout of multi-panel plots e.g. layout = c(4, 1) would ensure a four-panel plot is 4 columns by 1 row.\n\n\n\n\n\n\n\nTable 2.1: Common options used in openair plots that can be set by the user but are generally not explicitly documented.\n\noption\ndescription\n\n\n\nxlab\nx-axis label\n\n\nylab\ny-axis label\n\n\nmain\ntitle of the plot\n\n\npch\nplotting symbol used for points\n\n\ncex\nsize of symbol plotted\n\n\nlty\nline type\n\n\nlwd\nline width\n\n\nlayout\nthe plot layout e.g. c(2, 2)"
  },
  {
    "objectID": "openair-package.html#the-type-option",
    "href": "openair-package.html#the-type-option",
    "title": "2  The openair package",
    "section": "\n2.5 The type option",
    "text": "2.5 The type option\nOne of the central themes in openair is the idea of conditioning. Rather than plot \\(x\\) against \\(y\\), considerably more information can usually be gained by considering a third variable, \\(z\\). In this case, \\(x\\) is plotted against \\(y\\) for many different intervals of \\(z\\). This idea can be further extended. For example, a trend of NOx against time can be conditioned in many ways: NOx vs. time split by wind sector, day of the week, wind speed, temperature, hour of the day … and so on. This type of analysis is rarely carried out when analysing air pollution data, in part because it is time consuming to do. However, thanks to the capabilities of R and packages such as lattice and ggplot2, it becomes easier to work in this way.\nIn most openair functions conditioning is controlled using the type option. type can be any other variable available in a data frame (numeric, character or factor). A simple example of type would be a variable representing a ‘before’ and ‘after’ situation, say a variable called period i.e. the option type = \"period\" is supplied. In this case a plot or analysis would be separately shown for ‘before’ and ‘after’. When type is a numeric variable then the data will be split into four quantiles and labelled accordingly. Note however the user can set the quantile intervals to other values using the option n.levels. For example, the user could choose to plot a variable by different levels of temperature. If n.levels = 3 then the data could be split by ‘low’, ‘medium’ and ‘high’ temperatures, and so on. Some variables are treated in a special way. For example if type = \"wd\" then the data are split into 8 wind sectors (N, NE, E, …) and plots are organised by points of the compass.\nThere are a series of pre-defined values that type can take related to the temporal components of the data as summarised in Table 2.2. To use these there must be a date field so that it can be calculated. These pre-defined values of type are shown below are both useful and convenient. Given a data frame containing several years of data it is easy to analyse the data e.g. plot it, by year by supplying the option type = \"year\". Other useful and straightforward values are “hour” and “month”. When type = \"season\" openair will split the data by the four seasons (winter = Dec/Jan/Feb etc.). Note for southern hemisphere users that the option hemisphere = \"southern\" can be given. When type = \"daylight\" is used the data are split between nighttime and daylight hours. In this case the user can also supply the options latitude and longitude for their location (the default is London).\n\n\n\n\n\n\n\nTable 2.2: Built-in ways of splitting data in openair using the type option that is available for most functions.\n\n\n\n\n\noption\ndescription\n\n\n\n‘year’\nsplits data by year\n\n\n‘month’\nsplits data by month of the year\n\n\n‘week’\nsplits data by week of the year\n\n\n‘monthyear’\nsplits data by year and month\n\n\n‘season’\nsplits data by season. Note in this case the user can also supply a hemisphere option that can be either ‘northern’ (default) or ‘southern’\n\n\n‘weekday’\nsplits data by day of the week\n\n\n‘weekend’\nsplits data by Saturday, Sunday, weekday\n\n\n‘daylight’\nsplits data by nighttime/daytime. Note the user must supply a longitude and latitude\n\n\n\n‘dst’\nsplits data by daylight saving time and non-daylight saving time\n\n\n‘wd’\nif wind direction (wd) is available type = 'wd' will split the data into 8 sectors: N, NE, E, SE, S, SW, W, NW\n\n\n‘seasonyear’\nwill split the data into year-season intervals, keeping the months of a season together. For example, December 2010 is considered as part of winter 2011 (with January and February 2011). This makes it easier to consider contiguous seasons. In contrast, type = 'season' will just split the data into four seasons regardless of the year.\n\n\n\n\n\n\nIf a categorical variable is present in a data frame e.g. site then that variables can be used directly e.g. type = \"site\".\nIn some cases it is useful to categorise numeric variables according to one’s own intervals. One example is air quality bands where concentrations might be described as “good”, “fair”, “bad”. For this situation we can use the cut function. In the example below, concentrations of NO2 are divided into intervals 0-50, 50-100, 100-150 and >150 using the breaks option. Also shown are user-defined labels. Note there is 1 more break than label. There are a couple of thing sto note here. First, include.lowest = TRUE ensures that the lowest value is included in the lowest break (in this case 0). Second, the maximum value (1000) is set to be more than the maximum value in the data to ensure the final break encompasses all the data.\n\n2.5.1 Make your own type\nIn some cases it is useful to categorise numeric variables according to one’s own intervals. One example is air quality bands where concentrations might be described as “good”, “fair”, “bad”. For this situation we can use the cut function. In the example below, concentrations of NO2 are divided into intervals 0-50, 50-100, 100-150 and >150 using the breaks option. Also shown are user-defined labels. Note there is 1 more break than label. There are a couple of things to note here. First, include.lowest = TRUE ensures that the lowest value is included in the lowest break (in this case 0). Second, the maximum value (1000) is set to be more than the maximum value in the data to ensure the final break encompasses all the data.\n\nmydata$intervals <- cut(mydata$no2, \n                        breaks = c(0, 50, 100, 150, 1000), \n                        labels = c(\"Very low\", \"Low\", \"High\",\n                                   \"Very High\"), \n                        include.lowest = TRUE)\n\n# look at the data\nhead(mydata)\n\n# A tibble: 6 × 11\n  date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n1 1998-01-01 00:00:00  0.6    280   285    39     1    29  4.72  3.37    NA\n2 1998-01-01 01:00:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n3 1998-01-01 02:00:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n4 1998-01-01 03:00:00  2.16   170   493    52     3    35  7.66 10.2     NA\n5 1998-01-01 04:00:00  2.4    180   468    78     2    34  8.07  8.91    NA\n6 1998-01-01 05:00:00  3      190   264    42     0    16  5.50  3.05    NA\n# … with 1 more variable: intervals <fct>\n\n\nThen it is possible to use the new intervals variable in most openair functions e.g. windRose(mydata, type = \"intervals\").\nA special case is splitting data by date. In this scenario there might be interest in a ‘before-after’ situation e.g. due to an intervention. The openair function splitByDate should make this easy. Here is an example:\n\nsplitByDate(\n  mydata,\n  dates = \"1/1/2003\",\n  labels = c(\"before\", \"after\"),\n  name = \"scenario\"\n)\n\n# A tibble: 65,533 × 12\n   date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n   <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n 1 1998-01-01 00:00:00  0.6    280   285    39     1    29  4.72  3.37    NA\n 2 1998-01-01 01:00:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n 3 1998-01-01 02:00:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n 4 1998-01-01 03:00:00  2.16   170   493    52     3    35  7.66 10.2     NA\n 5 1998-01-01 04:00:00  2.4    180   468    78     2    34  8.07  8.91    NA\n 6 1998-01-01 05:00:00  3      190   264    42     0    16  5.50  3.05    NA\n 7 1998-01-01 06:00:00  3      140   171    38     0    11  4.23  2.26    NA\n 8 1998-01-01 07:00:00  3      170   195    51     0    12  3.88  2.00    NA\n 9 1998-01-01 08:00:00  3.36   170   137    42     1    12  3.35  1.46    NA\n10 1998-01-01 09:00:00  3.96   170   113    39     2    12  2.92  1.20    NA\n# … with 65,523 more rows, and 2 more variables: intervals <fct>,\n#   scenario <ord>\n\n\nThis code adds a new column scenario that is labelled before and after depending on the date. Note that the dates input by the user is in British format (dd/mm/YYYY) and that several dates (and labels) can be provided."
  },
  {
    "objectID": "openair-package.html#font-size",
    "href": "openair-package.html#font-size",
    "title": "2  The openair package",
    "section": "\n2.6 Controlling font size",
    "text": "2.6 Controlling font size\nAll openair plot functions have an option fontsize. Users can easily vary the size of the font for each plot e.g.\n\npolarPlot(mydata, fontsize = 20)\n\nThe font size will be reset to the default sizes once the plot is complete. Finer control of individual font sizes is currently not easily possible."
  },
  {
    "objectID": "openair-package.html#colours",
    "href": "openair-package.html#colours",
    "title": "2  The openair package",
    "section": "\n2.7 Using colours",
    "text": "2.7 Using colours\nMany of the functions described require that colour scales are used; particularly for plots showing surfaces. It is only necessary to consider using other colours if the user does not wish to use the default scheme, shown at the top of Figure 2.1. The choice of colours does seem to be a vexing issue as well as something that depends on what one is trying to show in the first place. For this reason, the colour schemes used in openair are very flexible: if you don’t like them, you can change them easily. R itself can handle colours in many sophisticated ways; see for example the RColorBrewer package.\nSeveral pre-defined colour schemes are available to make it easy to plot data. In fact, for most situations the default colour schemes should be adequate. The choice of colours can easily be set; either by using one of the pre-defined schemes or through a user-defined scheme. More details can be found in the openair openColours function. Some defined colours are shown in Figure 2.1, together with an example of a user defined scale that provides a smooth transition from yellow to blue.\n\nlibrary(openair)\n## small function for plotting\nprintCols <- function(col, y) {\n    rect((0:200) / 200, y, (1:201) / 200, y + 0.1, col = openColours(col, n = 201),\n         border = NA)\n    text(0.5, y + 0.15, deparse(substitute(col)))\n}\n\n## plot an empty plot\nplot(1, xlim = c(0, 1), ylim = c(0, 1.6), type = \"n\", xlab = \"\", ylab = \"\",\n     axes = FALSE)\nprintCols(\"default\", 0)\nprintCols(\"increment\", 0.2)\nprintCols(\"heat\", 0.4)\nprintCols(\"turbo\", 0.6)\nprintCols(\"viridis\", 0.8)\nprintCols(\"inferno\", 1.0)\nprintCols(\"greyscale\", 1.2)\nprintCols(c(\"tomato\", \"white\", \"forestgreen\" ), 1.4)\n\n\n\nFigure 2.1: elected pre-defined colour scales in. The top colour scheme is a user-defined one.\n\n\n\n\nThe user-defined scheme is very flexible and the following provides examples of its use. In the examples shown next, the polarPlot function is used as a demonstration of their use.\n\n# use default colours - no need to specify\npolarPlot(mydata)\n\n# use pre-defined \"turbo\" colours\npolarPlot(mydata, cols = \"turbo\")\n\n# define own colours going from yellow to green\npolarPlot(mydata, cols = c(\"yellow\", \"green\"))\n\n# define own colours going from red to white to blue\npolarPlot(mydata, cols = c(\"red\", \"white\", \"blue\"))\n\nFor more detailed information on using appropriate colours, have a look at the colorspace package. colorspace provides the definitive, comprehensive approach to using colours effectively. You will need to install the package, install.packages(\"colorspace\"). To use the palettes with openair, you can for example do:\n\nlibrary(colorspace)\nlibrary(openair)\nwindRose(mydata, cols = qualitative_hcl(4, palette = \"Dark 3\"))"
  },
  {
    "objectID": "openair-package.html#quickText",
    "href": "openair-package.html#quickText",
    "title": "2  The openair package",
    "section": "\n2.8 Automatic text formatting",
    "text": "2.8 Automatic text formatting\nopenair tries to automate the process of annotating plots. It can be time-consuming (and tricky) to repetitively type in text to represent μg m-3 or PM10 (μg m-3) etc. in R. For this reason, an attempt is made to automatically detect strings such as nox or NOx and format them correctly. Where a user needs a y-axis label such as NOx (μg m-3) it will only be necessary to type ylab = \"nox (ug/m3)\". The same is also true for plot titles.\nUsers can override this option by setting it to FALSE."
  },
  {
    "objectID": "openair-package.html#sec-multiple-plots-page",
    "href": "openair-package.html#sec-multiple-plots-page",
    "title": "2  The openair package",
    "section": "\n2.9 Multiple plots on a page",
    "text": "2.9 Multiple plots on a page\nWe often get asked how to combine multiple plots on one page. Recent changes to openair makes this a bit easier. Note that because openair uses lattice graphics the base graphics `par} settings will not work.\nIt is possible to arrange plots based on a column \\(\\times\\) row layout. Let’s put two plots side by side (2 columns, 1 row). First it is necessary to assign the plots to a variable:\n\na <- windRose(mydata)\nb <- polarPlot(mydata)\n\nNow we can plot them using the split option:\n\nprint(a, split = c(1, 1, 2, 1))\nprint(b, split = c(2, 1, 2, 1), newpage = FALSE)\n\nIn the code above for the split option, the last two numbers give the overall layout (2, 1) — 2 columns, 1 row. The first two numbers give the column/row index for that particular plot. The last two numbers remain constant across the series of plots being plotted.\nThere is one difficulty with plots that already contain sub-plots such as timeVariation where it is necessary to identify the particular plot of interest (see the timeVariation help for details). However, say we want a polar plot (b above) and a diurnal plot:\n\nc <- timeVariation(mydata)\nprint(b, split = c(1, 1, 2, 1))\nprint(c, split = c(2, 1, 2, 1), subset = \"hour\", newpage = FALSE)\n\nFor more control it is possible to use the position argument. position is a vector of 4 numbers, c(xmin, ymin, xmax, ymax) that give the lower-left and upper-right corners of a rectangle in which the plot is to be positioned. The coordinate system for this rectangle is [0–1] in both the x and y directions.\nAs an example, consider plotting the first plot in the lower left quadrant and the second plot in the upper right quadrant:\n\nprint(a, position = c(0, 0, 0.5, 0.5), more = TRUE)\nprint(b, position = c(0.5, 0.5, 1, 1))\n\nThe position argument gives more fine control over the plot location."
  },
  {
    "objectID": "UK-air-quality-data.html#accessing-data",
    "href": "UK-air-quality-data.html#accessing-data",
    "title": "3  Accessing UK Air Quality Data",
    "section": "\n3.1 Accessing data",
    "text": "3.1 Accessing data\nThe UK has a surprisingly large amount of air quality data that is publicly accessible. The main UK AURN archive and regional (England, Scotland, Wales and Northern Ireland) together with Imperial College London’s London Air Quality Network (LAQN) are important and large databases of information that allow free public access. Storing and managing data in this way has many advantages including consistent data format, and underlying high quality methods to process and store the data.\nopenair has a family of functions that provide users with extensive access to UK air quality data. Ricardo Energy & Environment have provided .RData files (R workspaces) for several important air quality networks in the UK. These files are updated on a daily basis. This approach requires a link to the Internet to work. The work of Trevor Davies at Ricardo Energy & Environment is greatly appreciated in making all the data available. The networks include:\n\nimportAURN For importing data from the UK national network called Automatic Urban and Rural Network}. This is the main UK network.\nimportSAQN For accessing data from Air Quality Scotland network.\nimportWAQN For accessing data from the Air Quality Wales network.\nimportAQE For accessing data from the Air Quality England network of sites.\nimportNI For accessing data from the Northern Ireland network of sites.\nimportLocal Import data from locally managed AQ networks in England. These are sites operated in most cases by Local Authorities but may also include monitoring from other programmes, industry and airports. The location and purpose of these sites differs from the national network which is governed by strict rules of the air quality directives. As a result there is a broad range of site types, equipment and data quality practices. For more information see here. These data represent information from about 15 different local air quality networks.\nimportEurope A simplified version of a function to give basic access to hourly European data based on Stuart Grange’s saqgetr package — see https://github.com/skgrange/saqgetr. The openair function has a similar approach to other openair import functions i.e. requires a site code(s) and year(s) to be supplied.\nimportKCL For accessing data from the sites operated by Imperial College London1, primarily including the The London Air Quality Network.\n\nMany users download hourly data from the air quality archive at https://www.airquality.co.uk. Most commonly, the data are emailed to the user as .csv files and have a fixed format as shown below. This is a useful facility but does have some limitations and frustrations, many of which have been overcome using a new way of storing and downloading the data described below.\nThere are several advantages over the web portal approach where .csv files are downloaded. First, it is quick to select a range of sites, pollutants and periods (see examples below). Second, storing the data as .RData objects is very efficient as they are about four times smaller than .csv files (which are already small) — which means the data downloads quickly and saves bandwidth. Third, the function completely avoids any need for data manipulation or setting time formats, time zones etc. Finally, it is easy to import many years of data. The final point makes it possible to download several long time series in one go.\nThe site codes and pollutant names can be upper or lower case.\nSome examples of usage are shown below. First load the packages we need.\n\nlibrary(openair)\nlibrary(tidyverse)"
  },
  {
    "objectID": "UK-air-quality-data.html#site-meta-data",
    "href": "UK-air-quality-data.html#site-meta-data",
    "title": "3  Accessing UK Air Quality Data",
    "section": "\n3.2 Site Meta Data",
    "text": "3.2 Site Meta Data\n\n3.2.1 National networks\nThe first question is, what sites are available and what do they measure? Users can access the details of air pollution monitoring sites using the importMeta function. The user only needs to provide the network name and (optionally) whether all data should be returned and whether certain periods should be considered. By default only site type, latitude and longitude are returned.\n\naurn_meta <- importMeta(source = \"aurn\")\naurn_meta\n\n# A tibble: 278 × 6\n   site                           code  latitude longitude site_type      source\n   <chr>                          <chr>    <dbl>     <dbl> <chr>          <chr> \n 1 Aberdeen                       ABD       57.2     -2.09 Urban Backgro… aurn  \n 2 Aberdeen Erroll Park           ABD9      57.2     -2.09 Urban Backgro… aurn  \n 3 Aberdeen Union Street Roadside ABD7      57.1     -2.11 Urban Traffic  aurn  \n 4 Aberdeen Wellington Road       ABD8      57.1     -2.09 Urban Traffic  aurn  \n 5 Armagh Roadside                ARM6      54.4     -6.65 Urban Traffic  aurn  \n 6 Aston Hill                     AH        52.5     -3.03 Rural Backgro… aurn  \n 7 Auchencorth Moss               ACTH      55.8     -3.24 Rural Backgro… aurn  \n 8 Ballymena Antrim Road          BAAR      54.9     -6.27 Urban Traffic  aurn  \n 9 Ballymena Ballykeel            BALM      54.9     -6.25 Urban Backgro… aurn  \n10 Barnsley                       BARN      53.6     -1.48 Urban Backgro… aurn  \n# … with 268 more rows\n\n\nOr return much more detailed data that includes which pollutants are measured at each site and site start / end dates. The option all = TRUE should be added.\n\naurn_meta <- importMeta(source = \"aurn\", all = TRUE) \n\n# what comes back?\nglimpse(aurn_meta)\n\nRows: 2,729\nColumns: 14\n$ code            <chr> \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\"…\n$ site            <chr> \"Aberdeen\", \"Aberdeen\", \"Aberdeen\", \"Aberdeen\", \"Aberd…\n$ site_type       <chr> \"Urban Background\", \"Urban Background\", \"Urban Backgro…\n$ latitude        <dbl> 57.15736, 57.15736, 57.15736, 57.15736, 57.15736, 57.1…\n$ longitude       <dbl> -2.094278, -2.094278, -2.094278, -2.094278, -2.094278,…\n$ variable        <chr> \"O3\", \"NO\", \"NO2\", \"NOx\", \"SO2\", \"CO\", \"PM10\", \"NV10\",…\n$ Parameter_name  <chr> \"Ozone\", \"Nitric oxide\", \"Nitrogen dioxide\", \"Nitrogen…\n$ start_date      <dttm> 2003-08-01, 1999-09-18, 1999-09-18, 1999-09-18, 2001-…\n$ end_date        <chr> \"2021-09-20\", \"2021-09-20\", \"2021-09-20\", \"2021-09-20\"…\n$ ratified_to     <dttm> 2021-09-20, 2021-09-20, 2021-09-20, 2021-09-20, 2007-…\n$ zone            <chr> \"North East Scotland\", \"North East Scotland\", \"North E…\n$ agglomeration   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ local_authority <chr> \"Aberdeen City\", \"Aberdeen City\", \"Aberdeen City\", \"Ab…\n$ source          <chr> \"aurn\", \"aurn\", \"aurn\", \"aurn\", \"aurn\", \"aurn\", \"aurn\"…\n\n\nNote that importMeta can import information for several networks at once e.g. source = c(\"aurn\", \"saqn\").\nOften it is useful to consider sites that were open in a particular year or were open for a duration of years. This can be done using the year argument. When year is a range such as year = 2010:2020, only sites that were open across that range of years will be returned. This option is especially useful for trend analysis when there might be an interest in extracting only sites that were measuring over the period of interest. Furthermore, if all = TRUE is used, supplying a year (or years) will select only specific pollutants that were measured during the period of interest.2\nFor example, to check the number of sites that were open from 2010 to 2022 in the AURN and SAQN combined:\n\nsites_2010_2022 <- importMeta(\n  source = c(\"aurn\", \"saqn\"),\n  year = 2010:2022\n)\n\nnrow(sites_2010_2022)\n\n[1] 143\n\n\nThe example below uses sites on the AURN that measure NO2, but can easily be extended to the other data sources.\nTo see how many sites measure NO2 in the AURN that are ‘urban traffic’:\n\naurn_detailed <- importMeta(source = \"aurn\", all = TRUE)\n\nno2_sites <- filter(\n  aurn_detailed,\n  variable == \"NO2\",\n  site_type == \"Urban Traffic\"\n)\n\nnrow(no2_sites)\n\n[1] 90\n\n\n\n\n\n\n\n\nUse importMeta as a way to select sites to import\n\n\n\nOne of the most useful aspects of importMeta is to use it as a basis to identify site codes to then import data. For example, to import data from the AURN for sites that have been in operation from 2005 to 2020:\n\nsites_2005_2020 <- importMeta(\n  source = \"aurn\",\n  year = 2005:2020\n)\n\nall_aq_data <- importAURN(\n  site = sites_2005_2020$code,\n  year = 2005:2020\n)\n\n\n\nTo import data, you can use the different versions of importAURN. Some examples are below.\n\n## import all pollutants from Marylebone Rd from 2000:2005\nmary <- importAURN(site = \"my1\", year = 2000:2005)\n\n## import nox, no2, o3 from Marylebone Road and Nottingham Centre for 2000\nthedata <- importAURN(site = c(\"my1\", \"nott\"), year = 2000,\n                      pollutant = c(\"nox\", \"no2\", \"o3\"))\n\n## import over 30 years of Mace Head O3 data!\no3 <- importAURN(site = \"mh\", year = 1987:2019)\n\n## import hydrocarbon data from Marylebone Road\nhc <- importAURN(site = \"my1\", year = 2008, hc = TRUE)\n\n## Import data from the AQE network (York data in this case)\nyk13 <- importAQE(site = \"yk13\", year = 2018)\n\nAnd to include basic meta data when importing air pollution data:\n\nkc1 <- importAURN(site = \"kc1\", year = 2018, meta = TRUE)\n\nglimpse(kc1)\n\nRows: 8,760\nColumns: 17\n$ site      <chr> \"London N. Kensington\", \"London N. Kensington\", \"London N. K…\n$ code      <chr> \"KC1\", \"KC1\", \"KC1\", \"KC1\", \"KC1\", \"KC1\", \"KC1\", \"KC1\", \"KC1…\n$ date      <dttm> 2018-01-01 00:00:00, 2018-01-01 01:00:00, 2018-01-01 02:00:…\n$ co        <dbl> 0.114872, 0.111043, 0.112000, 0.100512, 0.091897, 0.100512, …\n$ nox       <dbl> 8.32519, 8.89934, 9.41967, 9.36584, 7.21277, 7.64339, 10.173…\n$ no2       <dbl> 8.11153, 8.54325, 8.99235, 8.93852, 6.94570, 7.26948, 10.013…\n$ no        <dbl> 0.13935, 0.23224, 0.27869, 0.27869, 0.17418, 0.24386, 0.1045…\n$ o3        <dbl> 70.98040, 67.52118, 69.69982, 70.49810, 71.74542, 70.49810, …\n$ so2       <dbl> NA, 2.40953, 2.49812, 2.12606, 2.39181, 2.28551, 2.23236, 2.…\n$ pm10      <dbl> 12.425, 7.375, 5.625, 3.200, 3.875, 5.050, 9.400, 12.400, 15…\n$ pm2.5     <dbl> 8.892, 4.363, 3.137, 1.792, 2.146, 2.618, 4.575, 6.109, 7.05…\n$ ws        <dbl> 5.5, 5.0, 4.8, 4.8, 5.3, 5.3, 4.4, 3.0, 2.6, 1.6, 1.6, 1.1, …\n$ wd        <dbl> 263.3, 256.4, 251.0, 246.8, 248.4, 248.0, 245.8, 239.5, 232.…\n$ air_temp  <dbl> 5.5, 5.1, 4.9, 4.7, 4.9, 5.0, 5.0, 4.6, 4.2, 3.7, 5.4, 5.7, …\n$ latitude  <dbl> 51.52105, 51.52105, 51.52105, 51.52105, 51.52105, 51.52105, …\n$ longitude <dbl> -0.213492, -0.213492, -0.213492, -0.213492, -0.213492, -0.21…\n$ site_type <chr> \"Urban Background\", \"Urban Background\", \"Urban Background\", …\n\n\nBy default, the function returns data where each pollutant is in a separate column. However, it is possible to return the data in a tidy format (column for pollutant name, column for value) by using the option to_narrow:\n\nmy1 <- importAURN(\"my1\", year = 2018, to_narrow = TRUE)\n\nIt is also possible to return information on whether the data have been ratified or not using the option ratified (FALSE by default). So, add the option ratified = TRUE if you want this information.\n\n3.2.2 Local networks\nIn the case of locally available data, it is useful to know who the data providers are, which are shown below.\n\n# access local meta data to get provider\nmeta_local <- importMeta(\"local\", all = TRUE)\nunique(meta_local$provider)\n\n [1] \"Sussex Air Quality Network\"                         \n [2] \"Air Quality in North Lincolnshire \"                 \n [3] \"Kent and Medway Air Quality\"                        \n [4] \"Londonair\"                                          \n [5] \"Essex Air Quality Network\"                          \n [6] \"Hertfordshire and Bedfordshire Air Quality Network \"\n [7] \"Wiltshire Air Quality\"                              \n [8] \"UK Air Quality\"                                     \n [9] \"Hampshire Air Quality Network\"                      \n[10] \"Nottingham Air Quality\"                             \n[11] \"Heathrow Airwatch\"                                  \n[12] \"Norfolk Air Quality\"                                \n[13] \"Leicester Council AQ Network\"                       \n[14] \"Wolverhampton Air Quality\"                          \n[15] \"Liverpool Air Quality\""
  },
  {
    "objectID": "UK-air-quality-data.html#plot-sites-on-a-map",
    "href": "UK-air-quality-data.html#plot-sites-on-a-map",
    "title": "3  Accessing UK Air Quality Data",
    "section": "\n3.3 Plot Sites on a Map",
    "text": "3.3 Plot Sites on a Map\nTo easily visualise entire monitoring networks, consider using the openairmaps R package. This package can be installed from CRAN, similar to openair.\n\ninstall.packages(\"openairmaps\")\n\nThis package contains the networkMap function which acts as a wrapper around importMeta and returns a detailed map similar to the one produced above, with many options for customisation. For example, sites can be clustered together to avoid clutter, and an optional “control menu” can be added to filter for certain sites (e.g., different site types, shown below).\nlibrary(openairmaps)\nnetworkMap(source = \"aurn\", control = \"site_type\")\n\n\nFigure 3.1: Plotting the AURN using the openairmaps package.\n\n\nFor more information about using openairmaps to build maps of monitoring networks, please refer to the Network Visualisation Page."
  },
  {
    "objectID": "UK-air-quality-data.html#annual-and-other-statistics",
    "href": "UK-air-quality-data.html#annual-and-other-statistics",
    "title": "3  Accessing UK Air Quality Data",
    "section": "\n3.4 Annual and other statistics",
    "text": "3.4 Annual and other statistics\nBy default, all the functions above return hourly data. However, often there is a need to return data such as annual means of a long period of time. The UK family of functions (but not importKCL) can return data for averaging times: annual, monthly, daily and for SO2 15-minute. The annual and monthly data also provide valuable information on data capture rates. The averaging statistic is selected with the data_type option. The values data_type can take include:\n\n\n“hourly” This is the default and specific site(s) must be provided.\n\n“daily” Daily means returned and specific site(s) must be provided. Note that in the case of PM10 and PM2.5 daily measurements can be available from those derived from hourly measurements (using instruments such as TEOM, BAM and FIDAS) our daily gravimetric measurements such as from a Partisol. In the returned data the gravimetric daily measurements are shown as gr_pm10 and gr_pm2.5, respectively.\n\n“monthly” Monthly means returned. No site code is needed because all data for a particular year are returned. Data capture statistics are also given.\n\n“annual” Annual means returned. No site code is needed because all data for a particular year are returned. Data capture statistics are also given.\n\n“15_min” 15-minute SO2 concentrations returned for a specific site(s).\n\n“8_hour” Rolling 8-hour concentrations returned for a specific site(s) for O3 and CO.\n\n“24_hour” Rolling 24-hour concentrations returned for a specific site(s) for PM10 and PM2.5.\n\n“daily_max_8” Maximum daily rolling 8-hour maximum for O3 and CO.\n\n“daqi” Daily Air Quality Index (DAQI). See here for more details of how the index is defined.\n\nNote that for annual and monthly statistics all network data is returned and the site option has no effect.\nAs an example, to import 5 years of annual mean data from the AURN:\n\nuk_annual <- importAURN(year = 2016:2020, data_type = \"annual\")\n\nBy default, this will return data in “wide” format with a pollutant and its data capture rate in separate columns. Often it is more useful to have “narrow” format data, which is possible to select with the to_narrow option. Furthermore, it is also possible to return site meta data (site type, latitude and longitude) at the same time.\nBelow is an example of obtaining annual mean data for 2020.\n\nuk_2020 <- importAURN(\n  year = 2020,\n  data_type = \"annual\",\n  meta = TRUE,\n  to_narrow = TRUE\n)\n\nuk_2020\n\n# A tibble: 3,268 × 9\n   code  site     date                species      value data_…¹ latit…² longi…³\n   <chr> <chr>    <dttm>              <chr>        <dbl>   <dbl>   <dbl>   <dbl>\n 1 ABD   Aberdeen 2020-01-01 00:00:00 o3           45.5    0.610    57.2   -2.09\n 2 ABD   Aberdeen 2020-01-01 00:00:00 o3.daily.m…  57.5   NA        57.2   -2.09\n 3 ABD   Aberdeen 2020-01-01 00:00:00 o3.aot40v    NA     NA        57.2   -2.09\n 4 ABD   Aberdeen 2020-01-01 00:00:00 o3.aot40f    NA     NA        57.2   -2.09\n 5 ABD   Aberdeen 2020-01-01 00:00:00 somo35      502.     0.607    57.2   -2.09\n 6 ABD   Aberdeen 2020-01-01 00:00:00 no            4.60   0.945    57.2   -2.09\n 7 ABD   Aberdeen 2020-01-01 00:00:00 no2          13.5    0.945    57.2   -2.09\n 8 ABD   Aberdeen 2020-01-01 00:00:00 nox          20.5    0.945    57.2   -2.09\n 9 ABD   Aberdeen 2020-01-01 00:00:00 so2          NA     NA        57.2   -2.09\n10 ABD   Aberdeen 2020-01-01 00:00:00 co           NA     NA        57.2   -2.09\n# … with 3,258 more rows, 1 more variable: site_type <chr>, and abbreviated\n#   variable names ¹​data_capture, ²​latitude, ³​longitude\n\n\nThe pollutants returned include:\n\nunique(uk_2020$species)\n\n [1] \"o3\"                 \"o3.daily.max.8hour\" \"o3.aot40v\"         \n [4] \"o3.aot40f\"          \"somo35\"             \"no\"                \n [7] \"no2\"                \"nox\"                \"so2\"               \n[10] \"co\"                 \"pm10\"               \"nv10\"              \n[13] \"v10\"                \"pm2.5\"              \"nv2.5\"             \n[16] \"v2.5\"               \"gr10\"               \"gr2.5\"             \n[19] \"o3.summer\"         \n\n\nNow it is easy for example, to select annual mean data from 2020 for NO2 with a data capture rate of at least 80%:\n\nuk_2020 %>%\n  filter(species == \"no2\", data_capture >= 0.8)\n\n# A tibble: 144 × 9\n   code  site  date                species value data_…¹ latit…² longi…³ site_…⁴\n   <chr> <chr> <dttm>              <chr>   <dbl>   <dbl>   <dbl>   <dbl> <chr>  \n 1 ABD   Aber… 2020-01-01 00:00:00 no2     13.5    0.945    57.2   -2.09 Urban …\n 2 ABD7  Aber… 2020-01-01 00:00:00 no2     23.6    0.982    57.1   -2.11 Urban …\n 3 ABD8  Aber… 2020-01-01 00:00:00 no2     25.1    0.995    57.1   -2.09 Urban …\n 4 AH    Asto… 2020-01-01 00:00:00 no2      2.81   0.983    52.5   -3.03 Rural …\n 5 ARM6  Arma… 2020-01-01 00:00:00 no2     21.1    0.960    54.4   -6.65 Urban …\n 6 BAAR  Ball… 2020-01-01 00:00:00 no2     15.6    0.893    54.9   -6.27 Urban …\n 7 BALM  Ball… 2020-01-01 00:00:00 no2     10.3    0.993    54.9   -6.25 Urban …\n 8 BAR3  Barn… 2020-01-01 00:00:00 no2     11.9    0.970    53.6   -1.51 Urban …\n 9 BBRD  Birk… 2020-01-01 00:00:00 no2     16.9    0.984    53.4   -3.02 Urban …\n10 BDMA  Brad… 2020-01-01 00:00:00 no2     34.4    0.805    53.8   -1.76 Urban …\n# … with 134 more rows, and abbreviated variable names ¹​data_capture,\n#   ²​latitude, ³​longitude, ⁴​site_type\n\n\nFor the AURN, it is also possible to return the DAQI (Daily Air Quality Index) by pollutant to save deriving it.\n\ndaqi_2020 <- importAURN(\n  year = 2020,\n  data_type = \"daqi\", meta = TRUE\n)\n\ndaqi_2020\n\n# A tibble: 148,513 × 10\n   code  site        pollu…¹ date                conce…² poll_…³ measu…⁴ latit…⁵\n   <chr> <chr>       <chr>   <dttm>                <dbl>   <int> <chr>     <dbl>\n 1 ABD   Aberdeen    no2     2020-01-01 00:00:00    39.2       1 Hourly…    57.2\n 2 ABD   Aberdeen    pm10    2020-01-01 00:00:00    10         1 Daily …    57.2\n 3 ABD   Aberdeen    pm2.5   2020-01-01 00:00:00     9         1 Daily …    57.2\n 4 ABD7  Aberdeen U… no2     2020-01-01 00:00:00    42.4       1 Hourly…    57.1\n 5 ABD8  Aberdeen W… no2     2020-01-01 00:00:00    30.6       1 Hourly…    57.1\n 6 ACTH  Auchencort… o3      2020-01-01 00:00:00    57         2 8hour …    55.8\n 7 ACTH  Auchencort… pm10    2020-01-01 00:00:00    14         1 Daily …    55.8\n 8 ACTH  Auchencort… pm2.5   2020-01-01 00:00:00    12         2 Daily …    55.8\n 9 AGRN  Birmingham… no2     2020-01-01 00:00:00    23.2       1 Hourly…    52.4\n10 AGRN  Birmingham… o3      2020-01-01 00:00:00    35         2 8hour …    52.4\n# … with 148,503 more rows, 2 more variables: longitude <dbl>, site_type <chr>,\n#   and abbreviated variable names ¹​pollutant, ²​concentration, ³​poll_index,\n#   ⁴​measurement_period, ⁵​latitude"
  },
  {
    "objectID": "access-met-data.html#the-worldmet-package",
    "href": "access-met-data.html#the-worldmet-package",
    "title": "4  Access meteorological data",
    "section": "\n4.1 The worldmet package",
    "text": "4.1 The worldmet package\nMost of the import functions described in Chapter 3 return basic modelled hourly meteorological data (wind speed, wind direction and surface temperature). These data are derived from the WRF model that Ricardo runs to provide the data.\nAlternatively it may be advantageous to use surface measurements. worldmet provides an easy way in which to access surface meteorological data from >30,000 sites across the world. The package accesses the NOAA webservers to download hourly data. See https://github.com/davidcarslaw/worldmet and https://www.ncdc.noaa.gov/isd for further information.\nAccess to surface meteorological data is very useful in general but is especially useful when using openair and functions such as polarPlot. To install the package, type:\n\ninstall.packages(\"worldmet\")\n\nThere are two main functions in the package: getMeta and importNOAA. The former helps the user find meteorological sites by name, country and proximity to a location based on the latitude and longitude. getMeta will also return a code that can be supplied to importNOAA, which then imports the data.\nProbably the most common use of getMeta is to search around a location of interest based on its latitude and longitude. First we will load the worldmet (and other packages we use later):\n\nlibrary(worldmet)\nlibrary(openair)\nlibrary(tidyverse)\n\nAs an example, we will search for the 10 nearest sites to Dublin (latitude = 53.3, longitude = -6.3)1:\n\ngetMeta(lat = 53.3, lon = -6.3, returnMap = TRUE)\n\n\nFigure 4.1: Map of returned area of interest. The user can interactively select a site of interest and find its code to import data.\n\n\n\nNote that it is just as easy to access all the site information at once because it is quick to use the map to select the site and its code i.e.\n\ngetMeta()\n\nWe can use the map that is produced to select a site of interest and import the data. For example, to import data for Dublin Airport and look at some of the data:\n\ndublin_met <- importNOAA(code = \"039690-99999\", year = 2019)\n\n# first few lines of data\ndublin_met\n\n# A tibble: 8,760 × 24\n   code    station date                latit…¹ longi…²  elev    ws    wd air_t…³\n   <fct>   <fct>   <dttm>                <dbl>   <dbl> <dbl> <dbl> <dbl>   <dbl>\n 1 039690… DUBLIN… 2019-01-01 00:00:00    53.4   -6.27  73.8  5.07  250     8.93\n 2 039690… DUBLIN… 2019-01-01 01:00:00    53.4   -6.27  73.8  4.73  247.    8.1 \n 3 039690… DUBLIN… 2019-01-01 02:00:00    53.4   -6.27  73.8  4.07  250     8.47\n 4 039690… DUBLIN… 2019-01-01 03:00:00    53.4   -6.27  73.8  4.4   250     8.87\n 5 039690… DUBLIN… 2019-01-01 04:00:00    53.4   -6.27  73.8  5.47  257.    8.47\n 6 039690… DUBLIN… 2019-01-01 05:00:00    53.4   -6.27  73.8  4.9   260     8.8 \n 7 039690… DUBLIN… 2019-01-01 06:00:00    53.4   -6.27  73.8  4.9   260     8.83\n 8 039690… DUBLIN… 2019-01-01 07:00:00    53.4   -6.27  73.8  4.4   254.    8.93\n 9 039690… DUBLIN… 2019-01-01 08:00:00    53.4   -6.27  73.8  5.27  270     8.93\n10 039690… DUBLIN… 2019-01-01 09:00:00    53.4   -6.27  73.8  4.73  263.    8.1 \n# … with 8,750 more rows, 15 more variables: atmos_pres <dbl>,\n#   visibility <dbl>, dew_point <dbl>, RH <dbl>, ceil_hgt <dbl>, cl_1 <dbl>,\n#   cl_2 <dbl>, cl_3 <dbl>, cl <dbl>, cl_1_height <dbl>, cl_2_height <dbl>,\n#   cl_3_height <dbl>, precip_12 <dbl>, precip_6 <dbl>, precip <dbl>, and\n#   abbreviated variable names ¹​latitude, ²​longitude, ³​air_temp\n\n\nPlot a wind rose.\n\nwindRose(dublin_met)\n\n\n\nFigure 4.2: Example wind rose for Dublin Airport data."
  },
  {
    "objectID": "access-met-data.html#sec-link-aq",
    "href": "access-met-data.html#sec-link-aq",
    "title": "4  Access meteorological data",
    "section": "\n4.2 Linking with air quality data",
    "text": "4.2 Linking with air quality data\nHaving imported some meteorological data it will often be necessary to combine it with air quality data. Quite how this is done depends on the air quality data. The important point is there should be a date column in each data set that make the connection between the two. It may also depend on whether the air quality data has existing fields that also exist in the meteorological data e.g. ws, wd and air_temp, as is the case with most data accessible as described in Chapter 3. In this case, the aim to to replace any existing meteorological variables in the air quality data with those in the meteorological data. An example is shown below.\n\n# import some air quality data and check the variables that exist\n\naq_2019 <- importAURN(site = \"kc1\", year = 2019)\nnames(aq_2019) # ws, wd, air_temp already exist from WRF model\n\n [1] \"site\"     \"code\"     \"date\"     \"co\"       \"nox\"      \"no2\"     \n [7] \"no\"       \"o3\"       \"so2\"      \"pm10\"     \"pm2.5\"    \"ws\"      \n[13] \"wd\"       \"air_temp\"\n\n# import some met data - default is London Heathrow\nmet_2019 <- importNOAA(year = 2019)\n\n# merge the two data sets but not ws, wd, air_temp from aq data\naq_met <- left_join(\n  select(aq_2019, -ws, -wd, -air_temp),\n  met_2019,\n  by = \"date\"\n)\n\nnames(aq_met)\n\n [1] \"site\"        \"code.x\"      \"date\"        \"co\"          \"nox\"        \n [6] \"no2\"         \"no\"          \"o3\"          \"so2\"         \"pm10\"       \n[11] \"pm2.5\"       \"code.y\"      \"station\"     \"latitude\"    \"longitude\"  \n[16] \"elev\"        \"ws\"          \"wd\"          \"air_temp\"    \"atmos_pres\" \n[21] \"visibility\"  \"dew_point\"   \"RH\"          \"ceil_hgt\"    \"cl_1\"       \n[26] \"cl_2\"        \"cl_3\"        \"cl\"          \"cl_1_height\" \"cl_2_height\"\n[31] \"cl_3_height\" \"precip_12\"   \"precip_6\"    \"pwc\"         \"precip\""
  },
  {
    "objectID": "wind-roses.html#example-of-use",
    "href": "wind-roses.html#example-of-use",
    "title": "5  Wind and Pollution Roses",
    "section": "\n5.1 Example of use",
    "text": "5.1 Example of use\nFirst we load the packages:\n\nlibrary(openair)\nlibrary(tidyverse)\n\nThe function is very simply called as shown for Figure 5.1.\n\nwindRose(mydata)\n\n\n\nFigure 5.1: Use of windRose function to plot wind speed/direction frequencies. Wind speeds are split into the intervals shown by the scale in each panel. The grey circles show the % frequencies.\n\n\n\n\nFigure 5.2 highlights some interesting differences between the years. In 2000, for example, there were numerous occasions when the wind was from the SSW and 2003 clearly had more occasions when the wind was easterly. It can also be useful to use type = \"month\" to get an idea of how wind speed and direction vary seasonally.\n\nwindRose(mydata, type = \"year\", layout = c(4, 2))\n\n\n\nFigure 5.2: Use of windRose function to plot wind speed/direction frequencies by year. Wind speeds are split into the intervals shown by the scale in each panel. The grey circles show the 10 and 20% frequencies.\n\n\n\n\nThe type option is very flexible in openair and can be used to quickly consider the dependencies between variables. Section 25.2) describes the basis of this option in openair plot. As an example, consider the question: what are the meteorological conditions that control high and low concentrations of PM10? By setting type = \"pm10\", openair will split the PM10 concentrations into four quantiles i.e. roughly equal numbers of points in each level. The plot will then show four different wind roses for each quantile level, although the default number of levels can be set by the user — see ?cutData for more details. Figure 5.3 shows the results of setting type = \"pm10\". For the lowest concentrations of PM10 the wind direction is dominated by northerly winds, and relatively low wind speeds. By contrast, the highest concentrations (plot furthest right) are dominated by relatively strong winds from the south-west. It is therefore very easy to obtain a good idea about the conditions that tend to lead to high (or low) concentrations of a pollutant. Furthermore, the type option is available in almost all openair functions.\n\nwindRose(mydata, type = \"pm10\", layout = c(4, 1))\n\n\n\nFigure 5.3: Wind rose for four different levels of PM10 concentration. The levels are defined as the four quantiles of PM10 concentration and the ranges are shown on each of the plot labels.\n\n\n\n\nA comparison of the effect that bias has can be seen by plotting the following. Note the prominent frequencies for W, E and N in particular that are due to the bias issue discussed by Applequist (2012).\n\nApplequist, Scott. 2012. “Wind Rose Bias Correction.” Journal of Applied Meteorology and Climatology 51 (7): 1305–9.\n\n## no bias correction\nwindRose(mydata, angle = 22.5, bias.corr = FALSE)\n\n## bias correction (the default)\nwindRose(mydata, angle = 22.5)\n\npollutionRose is a variant of windRose that is useful for considering pollutant concentrations by wind direction, or more specifically the percentage time the concentration is in a particular range. This type of approach can be very informative for air pollutant species, as demonstrated by Ronald Henry and co-authors in Henry et al. (2009).\n\nHenry, Ronald, Gary A. Norris, Ram Vedantham, and Jay R. Turner. 2009. “Source Region Identification Using Kernel Smoothing.” {Article}. Environmental Science & Technology 43 (11): 4090–97. https://doi.org/{10.1021/es8011723}.\nYou can produce similar pollution roses using the pollutionRose function in recent versions of openair, e.g. as in Figure 5.4:\n\npollutionRose(mydata, pollutant = \"nox\")\n\n\n\nFigure 5.4: NOx pollution rose produced using pollutionRose and default pollutionRose settings.\n\n\n\n\npollutionRose is wrapper for windRose. It simply replaces the wind speed data series in the supplied data set with another variable using the argument pollutant before passing that on to windRose. It also modifies breaks to estimate a sensible set of break points for that pollutant and uses a slightly different set of default options (key to right, wedge style plot) but otherwise handles arguments just like the parent windRose function.\nWhile Figure 5.4 indicates that higher NOx concentrations are also associated with the SW, conditioning allows you to be much informative. For example, conditioning by SO2 (Figure 5.5) demonstrates that higher NOx concentrations are associated with the SW and much of the higher SO2 concentrations. However, it also highlights a notable NOx contribution from the E, most apparent at highest SO2 concentrations that is obscured in Figure 5.4 by a relatively high NOx background Figure 5.5.\n\npollutionRose(mydata,\n  pollutant = \"nox\",\n  type = \"so2\",\n  layout = c(4, 1),\n  key.position = \"bottom\"\n)\n\n\n\nFigure 5.5: NOx pollution rose conditioned by SO2 concentration.\n\n\n\n\npollutionRose can also usefully be used to show which wind directions dominate the overall concentrations. By supplying the option statistic = \"prop.mean\" (proportion contribution to the mean), a good idea can be gained as to which wind directions contribute most to overall concentrations, as well as providing information on the different concentration levels. A simple plot is shown in Figure 5.6, which clearly shows the dominance of south-westerly winds controlling the overall mean NOx concentrations at this site. Indeed, almost half the overall NOx concentration is contributed by two wind sectors to the south-west. The polarFreq function can also show this sort of information, but the pollution rose is more effective because both length and colour are used to show the contribution. These plots are very useful for understanding which wind directions control the overall mean concentrations.\n\npollutionRose(mydata, pollutant = \"nox\", statistic = \"prop.mean\")\n\n\n\nFigure 5.6: Pollution rose showing which wind directions contribute most to overall mean concentrations.\n\n\n\n\nIt is sometimes useful to more clearly understand the contributions from wind directions that have low frequencies. For example, for a pollution rose of SO2 there are few occurrences of easterly winds making it difficult to see how the concentration intervals are made up. Try:\n\npollutionRose(mydata, pollutant = \"so2\", seg = 1)\n\nHowever, each wind sector can be normalised to give a probability between 0 and 1 to help show the variation within each wind sector more clearly. An example is shown in Figure 5.7 where for easterly winds it is now clearer that a greater proportion of the time the concentration is made up of high SO2 concentrations. In this plot each wind sector is scaled between 0 and 1. Also shown with a black like is an indication of the wind direction frequency to remind us that winds from the east occur with a low frequency.\n\npollutionRose(mydata,\n  pollutant = \"so2\",\n  normalise = TRUE,\n  seg = 1,\n  cols = \"heat\"\n)\n\n\n\nFigure 5.7: SO2 pollution rose produced using pollutionRose normalised by each wind sector."
  },
  {
    "objectID": "wind-roses.html#sec-comp-met",
    "href": "wind-roses.html#sec-comp-met",
    "title": "5  Wind and Pollution Roses",
    "section": "\n5.2 Comparing two meteorological data sets",
    "text": "5.2 Comparing two meteorological data sets\nThe pollutionRose function is also useful for comparing two meteorological data sets. In this case a ‘reference’ data set is compared with a second data set. There are many reasons for doing so e.g. to see how one site compares with another or for meteorological model evaluation (more on that in later sections). In this case, ws and wd are considered to the the reference data sets with which a second set of wind speed and wind directions are to be compared (ws2 and wd2). The first set of values is subtracted from the second and the differences compared. If for example, wd2 was biased positive compared with wd then pollutionRose will show the bias in polar coordinates. In its default use, wind direction bias is colour-coded to show negative bias in one colour and positive bias in another.\nNote that this plot is mostly aimed at showing wind direction biases. It does also show the wind speed bias but only if there is a wind direction bias also. However, in most practical situations the plot should show both wind speed and direction biases together. An example of a situation where no wind speed bias would be shown would be for westerly winds where there was absolutely no bias between two data sets in terms of westerly wind direction but there was a difference in wind speed. Users should be aware of this limitation.\nIn the next example, some artificial wind direction data are generated by adding a positive bias of 30~degrees with some normally distributed scatter. Also, the wind speed data are given a positive bias. The results are shown in Figure 5.8. The Figure clearly shows the mean positive bias in wind direction i.e. the direction is displaced from north (no bias). The colour scale also shows the extent to which wind speeds are biased i.e. there is a higher proportion of positively biased wind speeds shown by the red colour compared with the negatively biased shown in blue. Also shown in Figure 5.8 is the mean wind speed and direction bias as numerical values.\nNote that the type option can be used in Figure 5.8 e.g. type = \"month\" to split the analysis in useful ways. This is useful if one wanted to see whether a site or the output from a model was biased for different periods. For example, type = \"daylight\" would show whether there are biases between nighttime and daytime conditions.\n\n## $example of comparing 2 met sites\n## first we will make some new ws/wd data with a postive bias\nmydata <- mutate(mydata,\n  ws2 = ws + 2 * rnorm(nrow(mydata)) + 1,\n  wd2 = wd + 30 * rnorm(nrow(mydata)) + 30\n)\n\n## need to correct negative wd\nid <- which(mydata$wd2 < 0)\nmydata$wd2[id] <- mydata$wd2[id] + 360\n\n## results show postive bias in wd and ws\npollutionRose(mydata,\n  ws = \"ws\",\n  wd = \"wd\",\n  ws2 = \"ws2\",\n  wd2 = \"wd2\"\n)\n\n\n\nFigure 5.8: Pollution rose showing the difference between two meteorological data sets. The colours are used to show whether data tend to be positively or negatively biased with respect to the reference data set.\n\n\n\n\nAn example of using user-supplied breaks is shown in Figure 5.9. In this case six intervals are chosen including one that spans -0.5 to +0.5 that is useful to show wind speeds that do not change.\n\n## add some wd bias to some nighttime hours\nid <- which(as.numeric(format(mydata$date, \"%H\")) %in% c(23, 1, 2, 3, 4, 5))\nmydata$wd2[id] <- mydata$wd[id] + 30 * rnorm(length(id)) + 120\nid <- which(mydata$wd2 < 0)\nmydata$wd2[id] <- mydata$wd2[id] + 360\n\npollutionRose(mydata,\n  ws = \"ws\", wd = \"wd\", ws2 = \"ws2\", wd2 = \"wd2\",\n  breaks = c(-11, -2, -1, -0.5, 0.5, 1, 2, 11),\n  cols = c(\"dodgerblue4\", \"white\", \"firebrick\"),\n  type = \"daylight\"\n  )\n\n\n\nFigure 5.9: Pollution rose showing the difference between two meteorological data sets. The colours are used to show whether data tend to be positively or negatively biased with respect to the reference data set. In this case the example shows how to use user-defined breaks and split the data by day/night for a latitude assumed to be London."
  },
  {
    "objectID": "polar-freq.html#sec-polarFreqBack",
    "href": "polar-freq.html#sec-polarFreqBack",
    "title": "6  Polar frequencies",
    "section": "\n6.1 Background and examples",
    "text": "6.1 Background and examples\nThis compactly shows the distribution of wind speeds and directions from meteorological measurements. It is similar to the traditional wind rose, but includes a number of enhancements to also show how concentrations of pollutants and other variables vary. It can summarise all available data, or show it by different time periods e.g. by year, month, day of the week. It can also consider a wide range of statistics.\nThis section shows an example output and use, using our data frame mydata. The function is very simply run as shown in Figure 6.1.\n\n# load openair\nlibrary(openair)\npolarFreq(mydata)\n\n\n\nFigure 6.1: Use of polarFreq function to plot wind speed/directions. Each cell gives the total number of hours the wind was from that wind speed/direction in a particular year. The number of hours is coded as a colour scale shown to the right. The scale itself is non-linear to help show the overall distribution. The dashed circular grey lines show the wind speed scale. The date range covered by the data is shown in the strip.\n\n\n\n\nBy setting type = \"year\", the frequencies are shown separately by year as shown in Figure 6.2, which shows that most of the time the wind is from a south-westerly direction with wind speeds most commonly between 2–6 m s-1. In 2000 there seemed to be a lot of conditions where the wind was from the south-west (leading to high pollutant concentrations at this location). The data for 2003 also stand out due to the relatively large number of occasions where the wind was from the east. Note the default colour scale, which has had a square-root transform applied, is used to provide a better visual distribution of the data.\n\npolarFreq(mydata, type = \"year\")\n\n\n\nFigure 6.2: Use of polarFreq function to plot wind speed/directions by year. Each cell gives the total number of hours the wind was from that wind speed/direction in a particular year. The number of hours is coded as a colour scale shown to the right. The scale itself is non-linear to help show the overall distribution. The dashed circular grey lines show the wind speed scale.\n\n\n\n\nThe polarFreq function can also usefully consider pollutant concentrations. Figure 6.3 shows the mean concentration of SO2 by wind speed and wind direction and clearly highlights that SO2 concentrations tend to be highest for easterly winds and for 1998 in particular.\nBy weighting the concentrations by the frequency of occasions the wind is from a certain direction and has a certain speed, gives a better indication of the conditions that dominate the overall mean concentrations. Figure 6.4 shows the weighted mean concentration of SO2 and highlights that annual mean concentrations are dominated by south-westerly winds i.e. contributions from the road itself — and not by the fewer higher hours of concentrations when the wind is easterly. However, 2003 looks interesting because for that year, significant contributions to the overall mean were due to easterly wind conditions.\nThese plots when applied to other locations can reveal some useful features about different sources. For example, it may be that the highest concentrations measured only occur infrequently, and the weighted mean plot can help show this.\nThe code required to make Figure 6.3 and Figure 6.4 is shown below.\n\npolarFreq(mydata, pollutant = \"so2\", \n          type = \"year\",\n           statistic = \"mean\", \n          min.bin = 2)\n\n\n\nFigure 6.3: Use of polarFreq function to plot mean SO2 concentrations (ppb) by wind speed/directions and year.\n\n\n\n\n\n# weighted mean SO2 concentrations\npolarFreq(mydata, pollutant = \"so2\", \n          type = \"year\",\n           statistic = \"weighted.mean\", \n          min.bin = 2)\n\n\n\nFigure 6.4: Use of polarFreq function to plot weighted mean SO2 concentrations (ppb) by wind speed/directions and year.\n\n\n\n\nUsers are encouraged to try out other plot options. However, one potentially useful plot is to select a few specific years of user interest. For example, what if you just wanted to compare two years e.g. 2000 and 2003? This is easy to do by sending a subset of data to the function. Use here can be made of the openair selectByDate function (see Section 25.1).\n\n# wind rose for just 2000 and 2003\npolarFreq(selectByDate(mydata, year = c(2000, 2003)),\n          cols = \"turbo\",\n          type = \"year\")\n\nThe polarFreq function can also be used to gain an idea about the wind directions that contribute most to the overall mean concentrations. As already shown, use of the option statistic =   \"weighted.mean\" will show the percentage contribution by wind direction and wind speed bin. However, often it is unnecessary to consider different wind speed intervals. To make the plot more effective, a few options are set as shown in Figure 6.5. First, the statistic = \"weighted.mean\" is chosen to ensure that the plot shows concentrations weighted by their frequency of occurrence of wind direction. For this plot, we are mostly interested in just the contribution by wind direction and not wind speed. Setting the ws.int to be above the maximum wind speed in the data set ensures that all data are shown in one interval. Rather than having a square-root transform applied to the colour scale, we choose to have a linear scale by setting trans = FALSE. Finally, to show a ‘disk’, the offset is set at 80. Increasing the value of the offset will narrow the disk.\nWhile Figure 6.5 is useful — e.g. it clearly shows that concentrations of NOx at this site are totally dominated by south-westerly winds, the use of pollutionRose for this type of plot is more effective, as shown in Chapter 5.\n\npolarFreq(mydata, pollutant = \"nox\", ws.int = 30, \n          statistic = \"weighted.mean\",\n          offset = 80, trans = FALSE, \n          col = \"heat\")\n\n\n\nFigure 6.5: The percentage contribution to overall mean concentrations of NOx at Marylebone Road."
  },
  {
    "objectID": "percentile-roses.html#introduction",
    "href": "percentile-roses.html#introduction",
    "title": "7  Percentile roses",
    "section": "\n7.1 Introduction",
    "text": "7.1 Introduction\npercentileRose calculates percentile levels of a pollutant and plots them by wind direction. One or more percentile levels can be calculated and these are displayed as either filled areas or as lines.\nBy default, the function plots percentile concentrations in 10 degree segments. Alternatively, the levels by wind direction are calculated using a cyclic smooth cubic spline. The wind directions are rounded to the nearest 10 degrees, consistent with surface data from the UK Met Office before a smooth is fitted.\nThe percentileRose function compliments other similar functions including windRose, pollutionRose, polarFreq or polarPlot. It is most useful for showing the distribution of concentrations by wind direction and often can reveal different sources e.g. those that only affect high percentile concentrations such as a chimney stack.\nSimilar to other functions, flexible conditioning is available through the type option. It is easy for example to consider multiple percentile values for a pollutant by season, year and so on. See examples below."
  },
  {
    "objectID": "percentile-roses.html#examples",
    "href": "percentile-roses.html#examples",
    "title": "7  Percentile roses",
    "section": "\n7.2 Examples",
    "text": "7.2 Examples\nThe first example is a basic plot of percentiles of O3 shown in Figure 7.1.\n\nlibrary(openair)\npercentileRose(mydata, pollutant = \"o3\")\n\n\n\nFigure 7.1: A percentileRose plot of O3 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction. It shows for example that higher concentrations occur for northerly winds, as expected at this location. However, it also shows, for example the actual value of the 95th percentile O3 concentration.\n\n\n\n\nA slightly more interesting plot is shown in Figure 7.2 for SO2 concentrations. We also take the opportunity of changing some default options. In this case it can be clearly seen that the highest concentrations of SO2 are dominated by east and south-easterly winds; likely reflecting the influence of stack emissions in those directions.\n\npercentileRose(mydata, pollutant = \"so2\",\n               percentile = c(25, 50, 75, 90, 95, 99, 99.9),\n               col = \"brewer1\", key.position = \"right\", smooth = TRUE)\n\n\n\nFigure 7.2: A percentileRose plot of SO2 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction. This plot sets some user-defined percentile levels to consider the higher SO2 concentrations, moves the key to the right and uses an alternative colour scheme.\n\n\n\n\nLots more insight can be gained by considering how percentile values vary by other factors i.e. conditioning. For example, what do O3 concentrations look like split by season and whether it is daylight or nighttime hours? We can set the type to consider season and whether it is daylight or nighttime.1 This Figure reveals some interesting features. First, O3 concentrations are higher in the spring and summer and when the wind is from the north. O3 concentrations are higher on average at this site in spring due to the peak of northern hemispheric O3 and to some extent local production. This may also explain why O3 concentrations are somewhat higher at nighttime in spring compared with summer. Second, peak O3 concentrations are higher during daylight hours in summer when the wind is from the south-east. This will be due to more local (UK/European) production that is photochemically driven — and hence more important during daylight hours.\n\npercentileRose(mydata, type = c(\"season\", \"daylight\"), \n               pollutant = \"o3\",\n               col = \"Set3\", mean.col = \"black\")\n\n\n\nFigure 7.3: A percentileRose plot of O3 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction.The plot shows the variation by season and whether it is nighttime or daylight hours."
  },
  {
    "objectID": "percentile-roses.html#sec-CPF",
    "href": "percentile-roses.html#sec-CPF",
    "title": "7  Percentile roses",
    "section": "\n7.3 Condtional probability function",
    "text": "7.3 Condtional probability function\nThe percentileRose function can also plot conditional probability functions (CPF) (Ashbaugh, Malm, and Sadeh 1985). The CPF is defined as CPF = \\(m_\\theta/n_\\theta\\), where \\(m_\\theta\\) is the number of samples in the wind sector \\(\\theta\\) with mixing ratios greater than some `high’ concentration, and \\(n_\\theta\\) is the total number of samples in the same wind sector. CPF analysis is very useful for showing which wind directions are dominated by high concentrations and give the probability of doing so. In openair, a CPF plot can be produced as shown in Figure 7.4. Note that in these plots only one percentile is provided and the method must be supplied. In Figure 7.4 it is clear that the high concentrations (greater than the 95th percentile of all observations) is dominated by easterly wind directions. There are very low conditional probabilities of these concentrations being experienced for other wind directions.\n\nAshbaugh, Lowell L., William C. Malm, and Willy Z. Sadeh. 1985. “A residence time probability analysis of sulfur concentrations at grand Canyon National Park.” Atmospheric Environment (1967) 19 (8): 1263–70. https://doi.org/10.1016/0004-6981(85)90256-2.\n\npercentileRose(mydata, poll=\"so2\", percentile = 95, \n               method = \"cpf\",\n               col = \"darkorange\", smooth = TRUE)\n\n\n\nFigure 7.4: A CPF plot of SO2 concentrations at Marylebone Road.\n\n\n\n\nIt is easy to plot several species on the same plot and this works well because they all have the same probability scale (i.e. 0 to 1). In the example below (not shown) it is easy to see for each pollutant the wind directions that dominate the contributions to the highest (95th percentile) concentrations. For example, the highest CO and NOx concentrations are totally dominated by south/south-westerly winds and the probability of their being such high concentrations from other wind directions is effectively zero.\n\npercentileRose(mydata, \n               pollutant = c(\"nox\", \"so2\", \"o3\", \"co\", \"pm10\", \"pm25\"),\n               percentile = 95, method = \"cpf\", col = \"darkorange\",\n               layout = c(3, 2))\n\n\n\nFigure 7.5: A CPF plot of many pollutants at Marylebone Road."
  },
  {
    "objectID": "polar-plots.html#introduction-to-polar-plots",
    "href": "polar-plots.html#introduction-to-polar-plots",
    "title": "8  Polar plots",
    "section": "\n8.1 Introduction to polar plots",
    "text": "8.1 Introduction to polar plots\nThe polarPlot function plots a bivariate polar plot of concentrations. Concentrations are shown to vary by wind speed and wind direction. In many respects they are similar to the plots shown in Chapter 6 but include some additional enhancements. These enhancements include: plots are shown as a continuous surface and surfaces are calculated through modelling using smoothing techniques. These plots are not entirely new as others have considered the joint wind speed-direction dependence of concentrations (see for example Yu et al. (2004)). However, plotting the data in polar coordinates and for the purposes of source identification is new. Furthermore, the basic polar plot is since been enhanced in many ways as described below. Publications that describe or use the technique are Carslaw et al. (2006) and Westmoreland et al. (2007). These plots have proved to be useful for quickly gaining a graphical impression of potential sources influences at a location.\n\nYu, K. N., Y. P. Cheung, T. Cheung, and R. C. Henry. 2004. “Identifying the Impact of Large Urban Airports on Local Air Quality by Nonparametric Regression.” Atmospheric Environment 38 (27): 4501–7.\n\nWestmoreland, E. J., N Carslaw, D. C. Carslaw, A. Gillah, and E. Bates. 2007. “Analysis of Air Quality Within a Street Canyon Using Statistical and Dispersion Modelling Techniques.” Atmospheric Environment 41 (39): 9195–205.\nThe polarPlot function is described in more detail in Carslaw et al. (2006) where it is used to triangulate sources in an airport setting, Carslaw and Beevers (2013) where it is used with a clustering technique to identify features in a polar plot with similar characteristics and Uria-Tellaetxe and Carslaw (2014) where it is extended to include a conditional probability function to extract more information from the plots.\nFor many, maybe most situations, increasing wind speed generally results in lower concentrations due to increased dilution through advection and increased mechanical turbulence. There are, however, many processes that can lead to interesting concentration-wind speed dependencies, and we will provide a more theoretical treatment of this in due course. However, below are a few reasons why concentrations can change with increasing wind speeds.\n\nBuoyant plumes from tall stacks can be brought down to ground-level resulting in high concentrations under high wind speed conditions.\nParticle suspension increases with increasing wind speeds e.g. PM10 from spoil heaps and the like.\n‘Particle’ suspension can be important close to coastal areas where higher wind speeds generate more sea spray.\nThe wind speed dependence of concentrations in a street canyon can be very complex: higher wind speeds do not always results in lower concentrations due to re-circulation. Bivariate polar plots are very good at revealing these complexities.\nAs Carslaw et al. (2006) showed, aircraft emissions have an unusual wind speed dependence and this can help distinguish them from other sources. If several measurement sites are available, polar plots can be used to triangulate different sources.\nConcentrations of NO2 can increase with increasing wind speed — or at least not decline steeply due to increased mixing. This mixing can result in O3-rich air converting NO to NO2.\n\n\nCarslaw, D. C., S. D. Beevers, K. Ropkins, and M. C. Bell. 2006. “Detecting and Quantifying Aircraft and Other on-Airport Contributions to Ambient Nitrogen Oxides in the Vicinity of a Large International Airport.” Atmospheric Environment 40 (28): 5424–34.\nThe function has been developed to allow variables other than wind speed to be plotted with wind direction in polar coordinates. The key issue is that the other variable plotted against wind direction should be discriminating in some way. For example, temperature can help reveal high-level sources brought down to ground level in unstable atmospheric conditions, or show the effect a source emission dependent on temperature e.g. biogenic isoprene. For research applications where many more variables could be available, discriminating sources by these other variables could be very insightful.\nBivariate polar plots are constructed in the following way. First, wind speed, wind direction and concentration data are partitioned into wind speed-direction bins and the mean concentration calculated for each bin. Testing on a wide range of data suggests that wind direction intervals at 5–10\\(^\\circ\\) and 40 wind speed intervals capture sufficient detail of the concentration distribution. The wind direction data typically available are generally rounded to 10\\(^\\circ\\) and for typical surface measurements. Binning the data in this way is not strictly necessary but acts as an effective data reduction technique without affecting the fidelity of the plot itself. Furthermore, because of the inherent wind direction variability in the atmosphere, data from several weeks, months or years typically used to construct a bivariate polar plot tends to be diffuse and does not vary abruptly with either wind direction or speed and more finely resolved bin sizes or working with the raw data directly does not yield more information.\nThe wind components, \\(u\\) and \\(v\\) are calculated i.e.\n\\[\nu = \\overline{u} . sin\\left(\\frac{2\\pi}{\\theta}\\right), v = \\overline{u} . cos\\left(\\frac{2\\pi}{\\theta}\\right)\n\\tag{8.1}\\]\nwith \\(\\overline{u}\\) is the mean hourly wind speed and \\(\\theta\\) is the mean wind direction in degrees with 90\\(^\\circ\\) as being from the east.\nThe calculations above provides a \\(u\\), \\(v\\), concentration (\\(C\\)) surface. While it would be possible to work with this surface data directly a better approach is to apply a model to the surface to describe the concentration as a function of the wind components \\(u\\) and \\(v\\) to extract real source features rather than noise. A flexible framework for fitting a surface is to use a Generalized Additive Model (GAM) e.g. (Hastie and Tibshirani 1990), (Wood 2006). GAMs are a useful modelling framework with respect to air pollution prediction because typically the relationships between variables are non-linear and variable interactions are important, both of which issues can be addressed in a GAM framework. GAMs can be expressed as shown in Equation 8.2:\n\nHastie, T. J., and R. J. Tibshirani. 1990. Generalized Additive Models. London: Chapman; Hall.\n\nWood, S. N. 2006. Generalized Additive Models: An Introduction with r. Chapman; Hall/CRC.\n\\[\n\\sqrt{C_i} = \\beta_0 + \\sum_{j=1}^{n}s_j(x_{ij}) + e_i\n\\tag{8.2}\\]\nwhere \\(C_i\\) is the ith pollutant concentration, \\(\\beta_0\\) is the overall mean of the response, \\(s_j(x_{ij})\\) is the smooth function of ith value of covariate \\(j\\), \\(n\\) is the total number of covariates, and \\(e_i\\) is the \\(i\\)th residual. Note that \\(C_i\\) is square-root transformed as the transformation generally produces better model diagnostics e.g. normally distributed residuals.\nThe model chosen for the estimate of the concentration surface is given by Equation 8.3. In this model the square root-transformed concentration is a smooth function of the bivariate wind components \\(u\\) and \\(v\\). Note that the smooth function used is isotropic because \\(u\\) and \\(v\\) are on the same scales. The isotropic smooth avoids the potential difficulty of smoothing two variables on different scales e.g. wind speed and direction, which introduces further complexities.\n\\[\n\\sqrt{C_i} = s(u, v) + e_i\n\\tag{8.3}\\]"
  },
  {
    "objectID": "polar-plots.html#polar-plots-from-an-air-pollution-dispersion-perspective",
    "href": "polar-plots.html#polar-plots-from-an-air-pollution-dispersion-perspective",
    "title": "8  Polar plots",
    "section": "\n8.2 Polar plots from an air pollution dispersion perspective",
    "text": "8.2 Polar plots from an air pollution dispersion perspective\nThis section considers some of the common features that can be observed in polar plots that can be related to the underlying characteristics of dispersion. Hourly dispersion model output can be analysed in the same way as measurements of air pollution but has the advantage of simplifying the types and number of sources considered.\n\n8.2.1 Effect of source type\nAs an example of different dispersion behaviours two source types have been modelled using the ADMS 5.2 model: a small shallow volume source and a single stack. The volume source is assumed to be 10 m deep and cover an area of 50x50 m with the receptor located at (0, 0) and the source centred at (-500, -500). This type of source is a simple representation of urban area sources e.g. road traffic and domestic combustion. The point source is assumed to have a height of 60 m, and efflux velocity of 10 m s-1 and a temperature of 150 °C. The receptor is located about 1 km to the north-west of the source (i.e. the source is to the south-west of the receptor). Hourly meteorological data from the London Heathrow site for 2019 were used in the model.\nThe hourly output from the model was processed and polar plots produced for each source type as shown in Figure 8.1. The shallow volume source (Figure 8.1 (a)) shows a characteristic dispersion pattern where concentrations are highest at low wind speeds (shown by the high values at the centre of the plot), which decrease with increasing wind speed (away from the centre of the plot). This type of pattern is frequently seen in monitoring data and often in urban areas where there are lots of ground-level, non-buoyant sources.\nBy contrast, the pattern from the stack shown in Figure 8.1 (b) is markedly different compared to the shallow volume source. In this case, it is clear the source is located to the south-west and is detected at a range of wind speeds, even up to very high values. This sort of behaviour is typical of a stack source plume being brought down to ground level under higher wind speed conditions. The pattern seen in Figure 8.1 (b) will depend on many factors such as distance to the stack, the nature of the release (efflux velocity and plume exit temperature) and the effect of the atmosphere itself, which is discussed more in the next section.\n\n\n\n\n\n(a) Shallow volume source.\n\n\n\n\n\n\n(b) Stack source.\n\n\n\n\nFigure 8.1: Effect of source type on plume dispersion with wind speed on the radial axis. In these plots the wind direction is the direction the wind is coming from and wind speed forms the radial axis such that zero wind speed is at the centre of the plot and increases away from the centre to 14 m s-1. The concentration of a pollutant is shown by the colour scale.\n\n\n\n8.2.2 Atmospheric stability on the radial axis\nBy default, polar plots use wind speed on the radial axis. However, any numeric variable can be used if it is helpful in distinguishing between different types of source. For example, if a source is known to have an ambient temperature dependence (such as biogenic isoprene emissions), then temperature on the radial axis can usefully highlight these types of source. Similarly, relative humidity might be useful in highlighting re-suspended sources where particles are bound by water and remain on the surface at higher relative humidities.\nFrom a dispersion perspective, atmospheric stability is of fundamental importance to how plumes disperse. Originally, categorical stability classes such as the Pasquill-Gifford were widely used in dispersion modelling, with classes from A to G (A = extremely unstable, D = neutral and G = extremely stable. However, ‘modern’ dispersion models such as the Atmospheric Dispersion Modelling System (ADMS) and the USEPA AERMOD models use the Monin-Obukhov length (LMO) and boundary layer height (H) as the basis of describing the boundary layer and its stability. In particular, the ratio of H/LMO represents a continuous scale of atmospheric stability. Ranges of H/LMO can provide a guide as to whether the atmosphere is unstable, neutral or stable, as shown in Table 8.1. The choice of values to partition H/LMO is somewhat arbitrary but the values given in Table 8.1 are commonly used values.\n\n\nTable 8.1: Stability conditions using the Monin-Obukhov length.\n\nCondition\nRange\n\n\n\nConvective (unstable)\nH/LMO < -0.6\n\n\nNeutral\n-0.6 < H/LMO < 2\n\n\nStable\nH/LMO > 2\n\n\n\n\n\n\n\n\nA continuous measure of atmospheric stability available through H/LMO makes it possible to use atmospheric stability on the radial axis. It should be noted that both ADMS and AERMOD use meteorological pre-processors to calculate these parameters, allowing them to be used in any analysis of dispersion model output or ambient measurements.1\nThe influence of atmospheric stability on plume dispersion can be seen in Figure 8.2, which shows the average plume height and spread (σz) across all conditions modelled. This Figure highlights several important characteristics: that an elevated plume under stable conditions may not reach ground level in proximity to the stack, that plume spread is much less under stable conditions and when the atmosphere is unstable (convective) there is a greater chance of a plume being brought down to ground level close to the source. As discussed in the previous section, the actual variation in these characteristics will depend on many factors, but they do highlight some common behaviours.\n\n\nFigure 8.2: Behaviour of a dispersing plume from a 60 m stack under different conditions of atmospheric stability. The line corresponds to the mean plume centreline height and the shaded area indicates the mean plume spread (σz).\n\n\nJust as in Section 8.2.1, we can illustrate the how concentrations are represented on polar plots when the radial axis is atmospheric stability (H/LMO). Figure 8.3 (a) shows how predicted concentrations vary with atmospheric stability with values close to zero corresponding to neutral conditions and positive values representing stable conditions. For the shallow volume source it is clear that the highest concentrations are observed under stable atmospheric conditions, which tend to also be conditions where the wind speed is low, as shown previously in Figure 8.1 (a).\nBy contrast, the stack dispersion pattern is very different (Figure 8.3 (b)) compared to the shallow volume source. Again, like Figure 8.1 (b) the plume is obvious, but in this case the highest concentrations are seen under unstable atmospheric conditions shown by negative values of H/LMO on the radial axis. These results show that the plume is being mixed down to the receptor when the atmosphere is unstable as illustrated in Figure 8.2.\n\n\n\n\n\n(a) Shallow volume source.\n\n\n\n\n\n\n(b) Stack source.\n\n\n\n\nFigure 8.3: Effect of source type on plume dispersion with atmospheric stability (H/LMO) on the radial axis.\n\n\nThe patterns seen from the examples in this section provide some guidance on how to interpret polar plots. There are of course many other source types and situations that can be considered, but the ability to understand the patterns observed in real data in terms of underlying dispersion (or chemistry) is an important aspect of using these types of analysis effectively."
  },
  {
    "objectID": "polar-plots.html#sec-polar-plot-examples",
    "href": "polar-plots.html#sec-polar-plot-examples",
    "title": "8  Polar plots",
    "section": "\n8.3 Examples",
    "text": "8.3 Examples\nWe first use the function in its simplest form to make a polar plot of NOx. The code is very simple as shown in Figure 8.4.\nFirst, load the packages we need.\n\nlibrary(openair)\nlibrary(tidyverse)\n\n\npolarPlot(mydata, pollutant = \"nox\")\n\n\n\nFigure 8.4: Default use of the polarPlot function applied to Marylebone Road NOx concentrations.\n\n\n\n\nThis produces Figure 8.4. The scale is automatically set using whatever units the original data are in. This plot clearly shows highest NOx concentrations when the wind is from the south-west. Given that the monitor is on the south side of the street and the highest concentrations are recorded when the wind is blowing away from the monitor is strong evidence of street canyon recirculation.\nFigure 8.5 and Figure 8.6 shows polar plots using different defaults and for other pollutants. In the first (Figure 8.5), a different colour scheme is used and some adjustments are made to the key. In Figure 8.6, SO2 concentrations are shown. What is interesting about this plot compared with either Figure 8.5 or Figure 8.4 is that the concentration pattern is very different i.e. high concentrations with high wind speeds from the east. The most likely source of this SO2 are industrial sources to the east of London. The plot does still however show evidence of a source to the south-west, similar to the plot for NOx, which implies that road traffic sources of SO2 can also be detected.\nThese plots often show interesting features at higher wind speeds. For these conditions there can be very few measurements and therefore greater uncertainty in the calculation of the surface. There are several ways in which this issue can be tackled. First, it is possible to avoid smoothing altogether and use polarFreq. The problem with this approach is that it is difficult to know how best to bin wind speed and direction: the choice of interval tends to be arbitrary. Second, the effect of setting a minimum number of measurements in each wind speed-direction bin can be examined through min.bin. It is possible that a single point at high wind speed conditions can strongly affect the surface prediction. Therefore, setting min.bin = 3, for example, will remove all wind speed-direction bins with fewer than 3 measurements before fitting the surface. This is a useful strategy for testing how sensitive the plotted surface is to the number of measurements available. While this is a useful strategy to get a feel for how the surface changes with different min.bin settings, it is still difficult to know how many points should be used as a minimum. Third, consider setting uncertainty = TRUE. This option will show the predicted surface together with upper and lower 95% confidence intervals, which take account of the frequency of measurements. The uncertainty approach ought to be the most robust and removes any arbitrary setting of other options. There is a close relationship between the amount of smoothing and the uncertainty: more smoothing will tend to reveal less detail and lower uncertainties in the fitted surface and vice-versa.\nThe default however is to down-weight the bins with few data points when fitting a surface. Weights of 0.25, 0.5 and 0.75 are used for bins containing 1, 2 and 3 data points respectively. The advantage of this approach is that no data are actually removed (which is what happens when using min.bin). This approach should be robust in a very wide range of situations and is also similar to the approaches used when trying to locate sources when using back trajectories as described in Chapter 10. Users can ignore the automatic weighting by supplying the option weights = c(1, 1, 1).\n\n## NOx plot\npolarPlot(mydata, pollutant = \"nox\", col = \"turbo\", \n          key.position = \"bottom\",\n          key.header = \"mean nox (ug/m3)\", \n          key.footer = NULL)\n\n\n\nFigure 8.5: Example plots using the polarPlot function with different options for the mean concentration of NOx.\n\n\n\n\n\npolarPlot(mydata, pollutant = \"so2\")\n\n\n\nFigure 8.6: Example plots using the polarPlot function for the mean concentration of SO2.\n\n\n\n\nA very useful approach for understanding air pollution is to consider ratios of pollutants. One reason is that pollutant ratios can be largely independent of meteorological variation. In many circumstances it is possible to gain a lot of insight into sources if pollutant ratios are considered. First, it is necessary to calculate a ratio, which is easy in R. In this example we consider the ratio of SO2/NOx:\n\nlibrary(tidyverse)\nmydata <- mutate(mydata, ratio = so2 / nox)\n\nThis makes a new variable called ratio. Sometimes it can be problematic if there are values equal to zero on the denominator, as is the case here. The mean and maximum value of the ratio is infinite, as shown by the Inf in the statistics below. Luckily, R can deal with infinity and the openair functions will remove these values before performing calculations. It is very simple therefore to calculate ratios.\n\nsummary(mydata$ratio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   0.018   0.024     Inf   0.034     Inf   11782 \n\n\nA polar plot of the SO2/NOx ratio is shown in Figure 8.7. The plot highlights some new features not seen before. First, to the north there seems to be evidence that the air tends to have a higher SO2/NOx ratio. Also, the source to the east has a higher SO2/NOx ratio compared with that when the wind is from the south-west i.e. dominated by road sources. It seems therefore that the easterly source(s), which are believed to be industrial sources have a different SO2/NOx ratio compared with road sources. This is a very simple analysis, but ratios can be used effectively in many functions and are particularly useful in the presence of high source complexity.\nArguably a better approach would be to calculate the ratio (slope) from a regression, as described in Section 8.6. One reason is that simply dividing one concentration by another would include the effect of background concentrations, which could be large. A regression-based approach avoids this problem.\n\npolarPlot(mydata, pollutant = \"ratio\", main = \"so2/nox ratio\")\n\n\n\nFigure 8.7: Bivariate polar plot of the ratio of SO2/NOx.\n\n\n\n\nSometimes when considering ratios it might be necessary to limit the values in some way; perhaps due to some unusually low value denominator data resulting in a few very high values for the ratio. This is easy to do with the dplyr filter command. The code below selects ratios less than 0.1.\n\npolarPlot(filter(mydata, ratio < 0.1), pollutant = \"ratio\")\n\nThe uncertainties in the surface can be calculated by setting the option uncertainty = TRUE. The details are described above and here we show the example of SO2 concentrations (Figure 8.8). In general the uncertainties are higher at high wind speeds i.e. at the ‘fringes’ of a plot where there are fewer data. However, the magnitude depends on both the frequency and magnitude of the concentration close to the points of interest. The pattern of uncertainty is not always obvious and it can differ markedly for different pollutants.\n\npolarPlot(mydata, pollutant = \"so2\", uncertainty = TRUE)\n\n\n\nFigure 8.8: Bivariate polar plot of SO2 concentrations at Marylebone Road. Three surfaces are shown: the central prediction (middle) and the lower and upper 95% estimated uncertainties. These plots help to show that in this particular case, some of the concentrations for strong easterly and south-easterly winds are rather uncertain. However, the central feature to the east remains, suggesting this feature is real and not an artifact of there being too few data.\n\n\n\n\nThe polarPlot function can also produce plots dependent on another variable (see the type option). For example, the variation of SO2 concentrations at Marylebone Road by hour of the day in the code below. The function was called as shown in below, and in this case the minimum number of points in each wind speed/direction was set to 2.\n\npolarPlot(mydata, pollutant = \"so2\", type = \"hour\", min.bin = 2)\n\nThis plot shows that concentrations of SO2 tend to be highest from the east (as also shown in Figure 8.6 and for hours in the morning. Together these plots can help better understand different source types. For example, does a source only seem to be present during weekdays, or winter months etc. In the case of type = \"hour\", the more obvious presence during the morning hours could be due to meteorological factors and this possibility should be investigated as well. In other settings where there are many sources that vary in their source emission and temporal characteristics, the polarPlot function should prove to be very useful.\nOne issue to be aware of is the amount of data required to generate some of these plots; particularly the hourly plots. If only a relatively short time series is available there may not be sufficient information to produce useful plots. Whether this is important or not will depend on the specific circumstances e.g. the prevalence of wind speeds and directions from the direction of interest. When used to produce many plots (e.g. type = \"hour\"), the run time can be quite long."
  },
  {
    "objectID": "polar-plots.html#nonparametric-wind-regression-nwr",
    "href": "polar-plots.html#nonparametric-wind-regression-nwr",
    "title": "8  Polar plots",
    "section": "\n8.4 Nonparametric Wind Regression, NWR",
    "text": "8.4 Nonparametric Wind Regression, NWR\nAn alternative approach to modelling the surface concentrations with a GAM is to use kernel smoothers, as described by Henry et al. (2009). In NWR, smoothing is achieved using nonparametric kernel smoothers that weight concentrations on a surface according to their proximity to defined wind speed and direction intervals. In the approach adopted in openair (which is not identical to Henry et al. (2009)), Gaussian smoothers are used for both wind direction and wind speed. Unlike the default GAM approach in openair, the NWR technique works directly with the raw (often hourly) data. It tends to provide similar results to openair but may have advantages in certain situations e.g. when there is insufficient data available to use a GAM. The width of the Gaussian kernels (𝜎) is controlled by the options wd_spread and ws_spread.\n\nHenry, Ronald, Gary A. Norris, Ram Vedantham, and Jay R. Turner. 2009. “Source Region Identification Using Kernel Smoothing.” {Article}. Environmental Science & Technology 43 (11): 4090–97. https://doi.org/{10.1021/es8011723}.\nAn example for SO2 concentrations is shown in Figure 8.9, which can be compared with Figure 8.6.\n\npolarPlot(mydata, pollutant = \"so2\", statistic = \"nwr\")\n\n\n\nFigure 8.9: polarPlot of SO2 concentrations at Marylebone Road based on the NWR approach."
  },
  {
    "objectID": "polar-plots.html#conditional-probability-function-cpf-plot",
    "href": "polar-plots.html#conditional-probability-function-cpf-plot",
    "title": "8  Polar plots",
    "section": "\n8.5 Conditional Probability Function (CPF) plot",
    "text": "8.5 Conditional Probability Function (CPF) plot\nThe conditional probability functions (CPF) was described in Section 7.3 in the context of the percentileRose function. The CPF was originally used to show the wind directions that dominate a (specified) high concentration of a pollutant; showing the probability of such concentrations occurring by wind direction Ashbaugh et al. (1985). However, these ideas can very usefully be applied to bivariate polar plots. In this case the CPF is defined as CPF = \\(m_{\\theta,j}/n_{\\theta,j}\\), where \\(m_{\\theta,j}\\) is the number of samples in the wind sector \\(\\theta\\) and wind speed interval \\(j\\) with mixing ratios greater than some ‘high’ concentration, and \\(n_{\\theta, j}\\) is the total number of samples in the same wind direction-speed interval. Note that \\(j\\) does not have to be wind speed but could be any numeric variable e.g. ambient temperature. CPF analysis is very useful for showing which wind direction, wind speed intervals are dominated by high concentrations and give the probability of doing so. A full explanation of the development and use of the bivariate case of the CPF is described in Uria-Tellaetxe and Carslaw (2014) where it is applied to monitoring data close to steelworks.\n\nAshbaugh, Lowell L., William C. Malm, and Willy Z. Sadeh. 1985. “A residence time probability analysis of sulfur concentrations at grand Canyon National Park.” Atmospheric Environment (1967) 19 (8): 1263–70. https://doi.org/10.1016/0004-6981(85)90256-2.\n\npolarPlot(mydata,\n  pollutant = \"so2\",\n  statistic = \"cpf\",\n  percentile = 90\n)\n\n\n\nFigure 8.10: polarPlot of SO2 concentrations at Marylebone Road based on the CPF function.\n\n\n\n\nAn example of a CPF polar plot is shown in Figure 8.10 for the 90th percentile concentration of SO2. This plot shows that for most wind speed-directions the probability of SO2 concentrations being greater than the 90th percentile is zero. The clearest areas where the probability is higher is to the east. Indeed, the plot now clearly reveals two potential sources of SO2, which are not as apparent in the ‘standard’ plot shown in Figure 8.6. Note that Figure 8.10 also gives the calculated percentile at the bottom of the plot (9.2 ppb in this case). Figure 8.10 can also be compared with the CPF plot based only on wind direction shown in Figure 7.4. While Figure 7.4 very clearly shows that easterly wind dominate high concentrations of SO2, Figure 8.10 provides additional valuable information by also considering wind speed, which in this case is able to discriminate between two sources (or groups of sources) to the east.\nThe polar CPF plot is therefore potentially very useful for source identification and characterisation. It is, for example, worth also considering other percentile levels and other pollutants. For example, considering the 95th percentile for SO2 ‘removes’ one of the sources (the one at highest wind speed). This helps to show some maybe important differences between the sources that could easily have been missed. Similarly, considering other pollutants can help build up a good understanding of these sources. A CPF plot for NO2 at the 90th percentile shows the single dominance of the road source. However, a CPF plot at the 75th percentile level indicates source contributions from the east (likely tall stacks), which again are not as clear in the standard bivariate polar plot. Considering a range of percentile values can therefore help to build up a more complete understanding of source contributions.\n\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(0, 10))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(10, 20))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(20, 30))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(30, 40))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(40, 50))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(50, 60))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(60, 70))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(70, 80))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(80, 90))\npolarPlot(mydata, poll = \"so2\", stati = \"cpf\", percentile = c(90, 100))\n\n\n\n\n\n(a) Percentile 0-10%.\n\n\n\n\n\n\n(b) Percentile 10-20%.\n\n\n\n\n\n\n(c) Percentile 20-30%.\n\n\n\n\n\n\n\n\n(d) Percentile 30-40%.\n\n\n\n\n\n\n(e) Percentile 40-50%.\n\n\n\n\n\n\n(f) Percentile 50-60%.\n\n\n\n\n\n\n\n\n(g) Percentile 60-70%.\n\n\n\n\n\n\n(h) Percentile 70-80%.\n\n\n\n\n\n\n(i) Percentile 80-90%.\n\n\n\n\n\n\n\n\n(j) Percentile 90-100%.\n\n\n\n\nFigure 8.11: polarPlot of SO2 concentrations at Marylebone Road based on the CPF function for a range of percentile intervals from 0–10, 10–20, … , 90–100.\n\n\n\nHowever, even more useful information can be gained by considering intervals of percentiles e.g. 50–60, 60–70 etc. By considering intervals of percentiles it becomes clear that some sources only affect a limited percentile range. polarPlot can accept a percentile argument of length two e.g. percentile =  c(80, 90). In this case concentrations in the range from the lower to upper percentiles will be considered. In Figure 8.11 for example, it is apparent that the road source to the south west is only important between the 60 to 90th percentiles. As mentioned previously, the chimney stacks to the east are important for the higher percentiles (90 to 100). What is interesting though is the emergence of what appears to be other sources at the lower percentile intervals. These potential sources are not apparent in Figure 8.6. The other interesting aspect is that it does seem that specific sources tend to be prominent for specific percentile ranges. If this characteristic is shown to be the case more generally, then CPF intervals could be a powerful way in which to identify many sources. Whether these particular sources are important or not is questionable and depends on the aims of the analysis. However, there is no reason to believe that the potential sources shown in the percentile ranges 0 to 50 are artefacts. They could for example be signals from more distant point sources whose plumes have diluted more over longer distances. Such sources would be ‘washed out’ in an ordinary polar plot. For a fuller example of this approach see Uria-Tellaetxe and Carslaw (2014).\n\nUria-Tellaetxe, I, and D. C. Carslaw. 2014. “Conditional Bivariate Probability Function for Source Identification.” Environmental Modelling & Software 59: 1–9. https://doi.org/10.1016/j.envsoft.2014.05.002.\nNote that it is easy to work out what the concentration intervals are for the percentiles shown in Figure 8.11:\n\nquantile(mydata$so2, probs = seq(0, 1, by = 0.1), na.rm = TRUE)\n\n     0%     10%     20%     30%     40%     50%     60%     70%     80%     90% \n 0.0000  1.0125  1.8825  2.5000  3.2500  4.0000  4.9375  5.9100  7.2375  9.2500 \n   100% \n63.2050 \n\n\nTo plot the Figures on one page it is necessary to make the plot objects first and then decide how to plot them. To plot the Figures in a particular layout see Section 2.9."
  },
  {
    "objectID": "polar-plots.html#sec-polar-pairwise",
    "href": "polar-plots.html#sec-polar-pairwise",
    "title": "8  Polar plots",
    "section": "\n8.6 Pairwise statistics",
    "text": "8.6 Pairwise statistics\nGrange et al. (2016) further developed the capabilities of the polarPlot function by allowing pairwise statistics to be used. This method makes it possible to consider the relationship between two pollutants to be considered. The relationship between two pollutants often yields useful source apportionment information and when combined with the polarPlot function can provide enhanced information. The pairwise statistics that can be considered include:\n\nGrange, Stuart K, Alastair C Lewis, and David C Carslaw. 2016. “Source Apportionment Advances Using Polar Plots of Bivariate Correlation and Regression Statistics.” Atmospheric Environment 145: 128–34.\n\nThe Pearson or Spearman correlation coefficient, \\(r\\);\nThe robust slope (gradient) resulting from a linear regressions between two pollutants.\nThe quantile slope from a quantile regression applied to two variables with a quantile value of \\(tau\\). By default, the median slope (i.e. \\(tau\\) = 0.5) is used by the actual level can be set by the user.\n‘York’ regression that accounts for errors in the ‘x’ and the ‘y’ variables.\n\nThe calculation involves a weighted Pearson correlation coefficient, which is weighted by Gaussian kernels for wind direction and the radial variable (by default wind speed). More weight is assigned to values close to a wind speed-direction interval. Kernel weighting is used to ensure that all data are used rather than relying on the potentially small number of values in a wind speed-direction interval. Two important settings are ws_spread and wd_spread that determine the σ value used in the Gaussian kernel weightings. Setting these values too low will result in potentially noisy results. As a guide, values of ws_spread around a tenth of the maximum wind speed seem reasonable and 5-10° for wd_spread.\nAn example usage scenario is that measurements of metal concentrations are made close to a steelworks where there is interest in understanding the principal sources. While it is useful to consider the correlation between potentially many metal concentrations, the contention is that if the correlation is also considered as a function of wind speed and direction, improved information will be available of the types of sources contributing. For example, it may be that Fe and Mn are quite strongly correlated overall, but they tend to be most correlated under specific wind speed and direction ranges — suggesting a specific source origin.\nAs an example of usage we will consider the relationship between PM2.5 and PM10 at the rural Harwell site in Oxfordshire. Additionally, we will use meteorological data from a nearby site rather than rely on modelled values that are provided in importAURN.\n\nlibrary(worldmet) # to access met data\nlibrary(tidyverse) \n\nhar <- importAURN(\"har\", year = 2013)\n\n# import met data from nearby site (Benson)\nmet <- importNOAA(code = \"036580-99999\", year = 2013)\n\n# merge AQ and met but don't use modelled ws and wd\nhar <- inner_join(\n  select(har, -ws, -wd, -air_temp),\n  met,\n  by = \"date\"\n)\n\nAn example pairwise regression surface relationship relating PM2.5 and PM10 is shown in Figure 8.12. This plot reveals that almost all the PM10 is in the form of PM2.5 when the wind has an easterly component, which is attributed to the large secondary contribution likely dominated by ammonium nitrate.\nA simple scatter plot between PM2.5 and PM10 strongly suggests a 1:1 relationship and it is not obvious that there is a higher PM2.5/PM10 ratio when the wind is from the east.\n\npolarPlot(har,\n  poll = c(\"pm2.5\", \"pm10\"),\n  statistic = \"robust_slope\",\n  col = \"turbo\",\n  limits = c(0, 1),\n  ws_spread = 1.5,\n  wd_spread = 10\n)\n\n\n\nFigure 8.12: Use of the polarPlot function to investigate the linear regression slope between PM2.5 and PM10 at Harwell in 2013. In this case the robust slope is calculated.\n\n\n\n\nThe York regression option requires special mention. In ordinary linear regression it is assumed there is no error in the ‘x’ variable. However, in atmospheric science, the interest is often in regressing two variables that both have uncertainty. There are various methods available, such as Reduced Major Axis (RMA) regression. However, a more general solution to this problem was solved a long time ago by the geophysicist Derek York from the University of Toronto (York 1968; York et al. 2004).2 While the York approach is ideally suited to atmospheric applications, the challenge may be obtaining the uncertainties in the measurements themselves. As discussed in Wu and Yu (2018), the York approach can still work well if the error in one of the variables is unknown or the measurement error cannot be trusted. In the example below, the errors are estimated and fixed for illustration purposes, but in many applications could be provided directly based on how well an instrument performs.\n\nYork, Derek. 1968. “Least Squares Fitting of a Straight Line with Correlated Errors.” Earth and Planetary Science Letters 5 (January): 320–24. https://doi.org/10.1016/s0012-821x(68)80059-7.\n\nYork, Derek, Norman M. Evensen, Margarita López Martıńez, and Jonás De Basabe Delgado. 2004. “Unified Equations for the Slope, Intercept, and Standard Errors of the Best Straight Line.” American Journal of Physics 72 (3): 367–75. https://doi.org/10.1119/1.1632486.\n\nWu, Cheng, and Jian Zhen Yu. 2018. “Evaluation of Linear Regression Techniques for Atmospheric Applications: The Importance of Appropriate Weighting.” Atmospheric Measurement Techniques 11 (2): 1233–50. https://doi.org/10.5194/amt-11-1233-2018.\n\n# assign simple / fixed uncertainties\n\nhar <- mutate(har, pm10_error = 2, pm2.5_error = 2)\n\npolarPlot(har,\n  poll = c(\"pm2.5\", \"pm10\"),\n  statistic = \"york_slope\",\n  col = \"turbo\",\n  limits = c(0, 1),\n  ws_spread = 1.5,\n  wd_spread = 10,\n  x_error = \"pm10_error\", \n  y_error = \"pm2.5_error\"\n)\n\n\n\nFigure 8.13: Use of the polarPlot function to investigate the linear regression slope between PM2.5 and PM10 at Harwell in 2013. In this case the York regression slope is calculated."
  },
  {
    "objectID": "polar-plots.html#sec-polar-compare",
    "href": "polar-plots.html#sec-polar-compare",
    "title": "8  Polar plots",
    "section": "\n8.7 Comparing two time periods",
    "text": "8.7 Comparing two time periods\nPolar plots can be useful for characterising the presence of different source types. Often the interest is in knowing how things have changed over time. It can be difficult from time series data alone to characterise these changes — and especially whether there is evidence that particular sources have changed their strength.\nThe polarDiff function has been written to compare polar plot surfaces between two time periods. The main purpose is to highlight whether there is evidence that source strengths may have changed. To illustrate the use of polarDiff data from a site close to a complex steelworks is used (Port Talbot in Wales). Two years will be compared as an example: 2012 and 2019. First, we need to import the air quality data for these two years:\n\nport_talbot <- importAURN(site = \"pt4\", year = c(2012, 2019))\n\nport_talbot\n\n# A tibble: 17,544 × 18\n   site      code  date                   co   nox   no2    no    o3   so2  pm10\n   <chr>     <chr> <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Port Tal… PT4   2012-01-01 00:00:00  0.12    11    11     0    44     3    13\n 2 Port Tal… PT4   2012-01-01 01:00:00  0.12    11    11     0    44     0    17\n 3 Port Tal… PT4   2012-01-01 02:00:00  0.12    11    11     0    46     0    16\n 4 Port Tal… PT4   2012-01-01 03:00:00  0.47    15    15     0    42     3    40\n 5 Port Tal… PT4   2012-01-01 04:00:00  0.81    13    13     0    40     5    48\n 6 Port Tal… PT4   2012-01-01 05:00:00  1.05    13    13     0    40     8    40\n 7 Port Tal… PT4   2012-01-01 06:00:00  1.16    15    13     1    40     8    45\n 8 Port Tal… PT4   2012-01-01 07:00:00  1.05    21    19     1    40    11    47\n 9 Port Tal… PT4   2012-01-01 08:00:00  1.28    19    17     1    40    11    44\n10 Port Tal… PT4   2012-01-01 09:00:00  1.28    23    19     2    38     8    46\n# … with 17,534 more rows, and 8 more variables: pm2.5 <dbl>, v10 <dbl>,\n#   v2.5 <dbl>, nv10 <dbl>, nv2.5 <dbl>, ws <dbl>, wd <dbl>, air_temp <dbl>\n\n\n\n8.7.1 Differences in absolute concentrations\nWe can now generate a polar plot of the difference between the two years (2019 with 2012 subtracted). The first example if for SO2. The three main arguments for the function are the ‘before’ data, the ‘after’ data and pollutant name.\n\npolarDiff(before = selectByDate(port_talbot, year = 2012), \n          after = selectByDate(port_talbot, year = 2019), \n          pollutant = \"so2\")\n\n\n\nFigure 8.14: A difference plot for SO2 concentrations at Port Talbot between 2012 and 2019.\n\n\n\n\nFigure 8.14 shows very clearly an area to the south-west where concentrations of SO2 have decreased considerably between 2012 and 2019, perhaps due to mitigation measures on the steelworks. Interestingly, the analysis for PM10 shown in Figure 8.15 shows a distinct area of increased concentration to the WSW, which would merit further investigation.\n\npolarDiff(before = selectByDate(port_talbot, year = 2012), \n          after = selectByDate(port_talbot, year = 2019), \n          pollutant = \"pm10\")\n\n\n\nFigure 8.15: A difference plot for PM10 concentrations at Port Talbot between 2012 and 2019.\n\n\n\n\nWhen selecting the periods, it is possible to refine the specific periods of interest using the selectByDate function described in Section 25.1. For example, a comparison might usefully consider weekdays only, or certain hours of the year, and so on. Such time period selections can quickly ‘home in’ on specific source types by focusing on the periods they are likely to be most active.\n\n8.7.2 Differences in probability that a concentration is exceeded\nThe polarDiff function is also not restricted to comparing two time periods. Any two surfaces can be compared (provided the same pollutant is in each data set). For example, it is possible to consider the difference between two nearby sites or two co-located instruments.\nDifferences between two time periods can also be considered using the statistic = \"cpf\" option, which would highlight how the probability that a concentration is exceeded between two time periods changes. This approach has also been considered for back trajectories Masiol et al. (2019). Some care is needed to ensure the probabilities that a specific concentration is exceeded because (for example) the 90th percentile concentration in one period will likely not be the same in another period.\n\nMasiol, Mauro, Stefania Squizzato, Meng-Dawn Cheng, David Q. Rich, and Philip K. Hopke. 2019. “Differential Probability Functions for Investigating Long-Term Changes in Local and Regional Air Pollution Sources.” Aerosol and Air Quality Research 19 (4): 724–36. https://doi.org/10.4209/aaqr.2018.09.0327.\nUsing the example of SO2 we can consider the probability that 10 μg m-3 (close to the 90th percentile in 2012 — but could be an air quality standard for example) is exceeded and how this changes over time. In this case, the percentile has length 3 (percentile = c(10, 100, -1)). The last negative number instructs polarPlot to interpret the the previous two numbers in an absolute sense — in this case that the SO2 concentration is more than 10 and less than 100 μg m-3. Note that the 100 μg m-3 is higher than the maximum concentration, which effectively means it provides the probability that the SO2 concentration is above 10 μg m-3; so it could be any number higher than the maximum value that exists in the data.\n\npolarDiff(before = selectByDate(port_talbot, year = 2012), \n          after = selectByDate(port_talbot, year = 2019), \n          pollutant = \"so2\", \n          statistic = \"cpf\", \n          percentile = c(10, 100, -1), \n          limits = c(-0.5, 0.5))\n\n\n\nFigure 8.16: Change in probability that 10 μg m-3 is exceeded from 2012 to 2019.\n\n\n\n\nFigure 8.16 effectively shows there has been a clear reduction in the probability that 10 μg m-3 is exceeded between 2012 and 2019 when the wind is from the south-west."
  },
  {
    "objectID": "polar-plots.html#sec-polarCluster",
    "href": "polar-plots.html#sec-polarCluster",
    "title": "8  Polar plots",
    "section": "\n8.8 Clustering",
    "text": "8.8 Clustering\n\n8.8.1 Clustering concentrations\nThe polarPlot function will often identify interesting features that would be useful to analyse further. It is possible to select areas of interest based only on a consideration of a plot. Such a selection could be based on wind direction and wind speed intervals for example e.g.\n\nsubdata <- filter(mydata, ws > 3, wd >= 180, wd <= 270)\n\nwhich would select wind speeds >3 m s-1 and wind directions from 180 to 270 degrees from mydata. That subset of data, subdata, could then be analysed using other functions. While this approach may be useful in many circumstances it is rather arbitrary. In fact, the choice of ‘interesting feature’ in the first place can even depend on the colour scale used, which is not very robust. Furthermore, many interesting patterns can be difficult to select and won’t always fall into convenient intervals of other variables such as wind speed and direction.\nA better approach is to use a method that can select group similar features together. One such approach is to use cluster analysis. openair uses k-means clustering as a way in which bivariate polar plot features can be identified and grouped. The main purpose of grouping data in this way is to identify records in the original time series data by cluster to enable post-processing to better understand potential source characteristics. The process of grouping data in k-means clustering proceeds as follows. First, \\(k\\) points are randomly chosen form the space represented by the objects that are being clustered into \\(k\\) groups. These points represent initial group centroids. Each object is assigned to the group that has the closest centroid. When all objects have been assigned, the positions of the \\(k\\) centroids is re-calculated. The previous two steps are repeated until the centroids no longer move. This produces a separation of the objects into groups from which the metric to be minimised can be calculated.\nCentral to the idea of clustering data is the concept of distance i.e. some measure of similarity or dissimilarity between points. Clusters should be comprised of points separated by small distances relative to the distance between the clusters. Careful consideration is required to define the distance measure used because the effectiveness of clustering itself fundamentally depends on its choice. The similarity of concentrations shown in Figure 8.4 for example is determined by three variables: the \\(u\\) and \\(v\\) wind components and the concentration. All three variables are equally important in characterising the concentration-location information, but they exist on different scales i.e. a wind speed-direction measure and a concentration. Let \\(X = \\{x_i\\}, i = 1,\\ldots,n\\) be a set of \\(n\\) points to be clustered into \\(K\\) clusters, \\(C = \\{c_k, k = 1,\\ldots,K\\}\\). The basic k-means algorithm for \\(K\\) clusters is obtained by minimising:\n\\[\n\\sum_{k=1}^{K} \\sum_{x_i \\in c_k} || x_i - \\mu_k ||^2\n\\tag{8.4}\\]\nwhere \\(|| x_i - \\mu_k ||^2\\) is a chosen distance measure, \\(\\mu_k\\) is the mean of cluster \\(c_k\\).\nThe distance measure is defined as the Euclidean distance:\n\\[\nd_{x, y} = \\left({\\sum_{j=1}^{J} (x_j - y_j) ^ 2}\\right)^{1/2}\n\\tag{8.5}\\]\nWhere x and y are two J-dimensional vectors, which have been standardized by subtracting the mean and dividing by the standard deviation. In the current case \\(J\\) is of length three i.e. the wind components \\(u\\) and \\(v\\) and the concentration \\(C\\), each of which is standardized e.g.:\n\\[\nx_j = \\left(\\frac{x_j - \\overline{x}}{\\sigma_x}\\right)\n\\tag{8.6}\\]\nStandardization is necessary because the wind components \\(u\\) and \\(v\\) are on different scales to the concentration. In principle, more weight could be given to the concentration rather than the \\(u\\) and \\(v\\) components, although this would tend to identify clusters with similar concentrations but different source origins.\npolarCluster can be thought of as the ‘local’ version of clustering of back trajectories. Rather than using air mass origins, wind speed, wind direction and concentration are used to group similar conditions together. Section 10.8 provides the details of clustering back trajectories in openair. A fuller description of the clustering approach is described in (Carslaw and Beevers 2013).\n\nCarslaw, D. C., and S. D. Beevers. 2013. “Characterising and Understanding Emission Sources Using Bivariate Polar Plots and k-Means Clustering.” Environmental Modelling & Software 40 (0): 325–29. https://doi.org/10.1016/j.envsoft.2012.09.005.\nThe use of the polarCluster is very similar to the use of all openair functions. While there are many techniques available to try and find the optimum number of clusters, it is difficult for these approaches to work in a consistent way for identifying features in bivariate polar plots. For this reason it is best to consider a range of solutions that covers a number of clusters.\nCluster analysis is computationally intensive and the polarCluster function can take a comparatively long time to run. The basic idea is to calculate the solution to several cluster levels and then choose one that offers the most appropriate solution for post-processing.\nThe example given below is for concentrations of SO2, shown in Figure 8.6 and the aim is to identify features in that plot. A range of numbers of clusters will be calculated — in this case from two to ten.\n\npolarCluster(mydata, pollutant=\"so2\", n.clusters=2:10, cols= \"Set2\")\n\n\n\nFigure 8.17: Use of the polarCluster function applied to SO2 concentrations at Marylebone Road. In this case 2 to 10 clusters have been chosen.\n\n\n\n\n\nresults <- polarCluster(mydata,\n  pollutant = \"so2\",\n  n.clusters = 8,\n  cols = \"Set2\"\n)\n\n\n\nFigure 8.18: Use of the polarCluster function applied to SO2 concentrations at Marylebone Road. In this case 8 clusters have been chosen.\n\n\n\n\nThe real benefit of polarCluster is being able to identify clusters in the original data frame. To do this, the results from the analysis must be read into a new variable, as in Figure 8.18, where the results are read into a data frame results. Now it is possible to use this new information. In the 8-cluster solution to Figure 8.18, cluster 6 seems to capture the elevated SO2 concentrations to the east well (see Figure 8.6 for comparison), while cluster 5 will strongly represent the road contribution.\nThe results are here:\n\nhead(results[[\"data\"]])\n\n# A tibble: 6 × 13\n  date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n1 1998-01-01 00:00:00  0.6    280   285    39     1    29  4.72  3.37    NA\n2 1998-01-01 01:00:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n3 1998-01-01 02:00:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n4 1998-01-01 03:00:00  2.16   170   493    52     3    35  7.66 10.2     NA\n5 1998-01-01 04:00:00  2.4    180   468    78     2    34  8.07  8.91    NA\n6 1998-01-01 05:00:00  3      190   264    42     0    16  5.50  3.05    NA\n# … with 3 more variables: ratio <dbl>, .id <int>, cluster <chr>\n\n\nNote that there is an additional column cluster that gives the cluster a particular row belongs to and that this is a character variable. It might be easier to read these results into a new data frame:\n\nresults <- results[[\"data\"]]\n\nIt is easy to find out how many points are in each cluster:\n\ntable(results[[\"cluster\"]])\n\n\n   C1    C2    C3    C4    C5    C6    C7    C8 \n  247   424   125 24241 16161  2590  7764  2755 \n\n\nNow other openair analysis functions can be used to analyse the results. For example, to consider the temporal variations by cluster:\n\ntimeVariation(results, pollutant = \"so2\", group = \"cluster\",\n              key.columns = 4,\n              col = \"Set2\", ci = FALSE, lwd = 3)\n\n\n\nFigure 8.19: Temporal variation in SO2 split by cluster.\n\n\n\n\nOr if we just want to plot a couple of clusters (5 and 6) using the same colours as in Figure 8.18:\n\ntimeVariation(\n  dplyr::filter(results, cluster %in% c(\"C5\", \"C6\")),\n  pollutant = \"so2\",\n  group = \"cluster\",\n  col = openColours(\"Set2\", 8)[5:6],\n  lwd = 3\n)\n\n\n\nFigure 8.20: Temporal variation in SO2 split by cluster — showing only two clusters.\n\n\n\n\npolarCluster will work on any surface that can be plotted by polarPlot e.g. the radial variable does not have to be wind speed but could be another variable such as temperature. While it is not always possible for polarCluster to identify all features in a surface it certainly makes it easier to post-process polarPlots using other openair functions or indeed other analyses altogether.\nAnother useful way of understanding the clusters is to use the timeProp function, which can display a time series as a bar chart split by a categorical variable (in this case the cluster). In this case it is useful to plot the time series of SO2and show how much of the concentration is contributed to by each cluster. Such a plot is shown in Figure 8.21. It is now easy to see for example that many of the peaks in SO2 are associated with cluster 6 (power station sources from the east), seen in Figure 8.18. Cluster 6 is particularly prominent during springtime, but those sources also make important contributions through the whole year.\n\ntimeProp(\n  selectByDate(results, year = 2003),\n  pollutant = \"so2\",\n  avg.time = \"day\",\n  proportion = \"cluster\",\n  col = \"Set2\",\n  key.position = \"top\",\n  key.columns = 8,\n  date.breaks = 10,\n  ylab = \"so2 (ug/m3)\"\n)\n\n\n\nFigure 8.21: Temporal variation in daily SO2 concentration at the Marylebone Road site show by contribution of each cluster for 2003.\n\n\n\n\n\n8.8.2 Clustering differences in concentrations\nSection 8.7 showed how to consider differences in polar plot surfaces, which can be useful for considering how source strengths may have changed in time. This section considers how to apply cluster analysis to these difference surfaces. The clustering works in the same way as shown earlier in this section but with an additional argument (after) which represents the second data frame used to calculate the difference between two surfaces.\nThe function is called as shown in Figure 8.22. Note that the output is recorded in clust_out. The clustering is applied to the difference plots shown in Figure 8.14.\n\nclust_out <- polarCluster(selectByDate(port_talbot, year = 2012),\n  after = selectByDate(port_talbot, year = 2019),\n  pollutant = \"so2\",\n  n.clusters = 5\n)\n\n\n\nFigure 8.22: Clustering differences in polar plot concentrations of SO2 concentrations at Port Talbot between 2012 and 2019.\n\n\n\n\nThe clust_out usefully contains the two original data sets that were used to make the difference surface as well as the assigned cluster. These two outputs can be combined and post-processed in different ways. In the code below the mean concentrations by cluster are calculated but other useful analyses could be considered knowing that Cluster 3 is most strongly associated with the change in SO2 concentration. The latter point is important because clustering features by how they change ought to provide an improved analysis of the effect of interventions — rather than just considering absolute concentrations in a before or after period.\n\nclust_data <- bind_rows(clust_out$data, clust_out$after)\n\n# consider mean SO2 by cluster\ngroup_by(clust_data, cluster) %>% \n  summarise(mean_so2 = mean(so2, na.rm = TRUE))\n\n# A tibble: 11 × 2\n   cluster mean_so2\n   <chr>      <dbl>\n 1 1          10.1 \n 2 2           3.33\n 3 3          10.6 \n 4 4           1.93\n 5 5           1.32\n 6 C1          5.99\n 7 C2          4.10\n 8 C3         16.6 \n 9 C4          2.27\n10 C5          1.02\n11 <NA>        5.17"
  },
  {
    "objectID": "polar-plots.html#polar-plots-on-an-interactive-map",
    "href": "polar-plots.html#polar-plots-on-an-interactive-map",
    "title": "8  Polar plots",
    "section": "\n8.9 Polar plots on an interactive map",
    "text": "8.9 Polar plots on an interactive map\nAn R package called openairmaps has been developed to plot polar plots and other “directional analysis” visualisations on interactive leaflet maps. This package can be installed from CRAN, similar to openair.\n\ninstall.packages(\"openairmaps\")\n\nThe function polarMap requires information on the site (name or code), latitude and longitude in addition to the usual information required by the polarPlot function. The package comes with some example data (polar_data) that can be used as a template for using different data. This function can plot any type of polar plot, e.g., with different options for statistic.\nFirst, load the package and check out the data format.\n\nlibrary(openairmaps)\n\nglimpse(polar_data)\n\nRows: 35,040\nColumns: 13\n$ date       <dttm> 2009-01-01 00:00:00, 2009-01-01 01:00:00, 2009-01-01 02:00…\n$ nox        <dbl> 113, 40, 48, 36, 40, 50, 50, 53, 80, 111, 206, 113, 86, 82,…\n$ no2        <dbl> 46, 32, 36, 29, 32, 36, 34, 34, 50, 59, 67, 61, 52, 53, 52,…\n$ pm2.5      <dbl> 42, 45, 43, 37, 36, 33, 33, 31, 27, 28, 37, 30, 27, 29, 27,…\n$ pm10       <dbl> 46, 49, 46, NA, 38, 32, 36, 32, 30, 32, 39, 37, 32, 33, 34,…\n$ site       <chr> \"London Bloomsbury\", \"London Bloomsbury\", \"London Bloomsbur…\n$ lat        <dbl> 51.52229, 51.52229, 51.52229, 51.52229, 51.52229, 51.52229,…\n$ lon        <dbl> -0.125889, -0.125889, -0.125889, -0.125889, -0.125889, -0.1…\n$ site_type  <chr> \"Urban Background\", \"Urban Background\", \"Urban Background\",…\n$ wd         <dbl> 58.92536, 74.46675, 30.00000, 45.00000, 70.00000, 46.63627,…\n$ ws         <dbl> 2.066667, 1.900000, 1.550000, 2.100000, 1.500000, 2.100000,…\n$ visibility <dbl> 5000.000, 4933.333, 5000.000, 4900.000, 5000.000, 6000.000,…\n$ air_temp   <dbl> 0.8666667, 0.8666667, 0.8000000, 0.8500000, 0.8666667, 0.96…\n\n\nPlot an interactive map.\n\npolarMap(\n  polar_data,\n  latitude = \"lat\",\n  longitude = \"lon\",\n  pollutant = \"nox\"\n)\n\n\nFigure 8.23: Interactive map of polar plots with default map.\n\n\n\nThe polarMap function allows for control of many parts of the leaflet map. For example, you can provide one (or more) different base map(s) — there are many! You may also visualise multiple pollutants at once which can then be toggled between.\n\npolarMap(openairmaps::polar_data, \n         pollutant = c(\"nox\", \"no2\"),\n         latitude = \"lat\",\n         longitude = \"lon\",\n         provider = \"CartoDB.Positron\")\n\n\nFigure 8.24: Interactive map of polar plots for multiple pollutants with a different base map.\n\n\n\nNote that if you are using the importAURN family of functions it is useful to add the option meta = TRUE, which will return the latitude and longitude of the site together with the air quality data.\nopenairmaps isn’t limited to just polarMap — almost any kind of “directional analysis” plot can be placed on a map. There are many similar functions which correspond to other openair plotting functions; annulusMap, freqMap, percentileMap, polarMap, pollroseMap, and windroseMap.\nFor more information about using openairmaps to create maps with polar plots as markers, please refer to the Directional Analysis Maps Page]."
  },
  {
    "objectID": "polar-annulus.html#sec-polarAnnulusBack",
    "href": "polar-annulus.html#sec-polarAnnulusBack",
    "title": "9  Polar annulus",
    "section": "\n9.1 Purpose",
    "text": "9.1 Purpose\nThe polarAnnulus function provides a way in which to consider the temporal aspects of a pollutant concentration by wind direction. This is another means of visualising diurnal, day of week, seasonal and trend variations. Plotting as an annulus, rather than a circle avoids to some extent the difficulty in interpreting values close to the origin. These plots have the capacity to display potentially important information regarding sources; particularly if more than one pollutant is available."
  },
  {
    "objectID": "polar-annulus.html#sec-examAnnulus",
    "href": "polar-annulus.html#sec-examAnnulus",
    "title": "9  Polar annulus",
    "section": "\n9.2 Example of use",
    "text": "9.2 Example of use\nWe apply the four variations of the polarAnnulus plot to PM10 concentrations at Marylebone Road. Figure 9.1 shows the different temporal components. Similar to other analyses for PM10, the trend plot show that concentrations are dominated by southerly winds and there is little overall change in concentrations from 1998 to 2005, as shown by the red colouring over the period. The seasonal plot shows that February/March is important for easterly winds, while the summer/late summer period is more important for southerly and south-westerly winds. The day of the week plot clearly shows concentrations to be elevated for during weekdays but not weekends — for all wind directions. Finally, the diurnal plot highlights that higher concentrations are observed from 6 am to 6 pm.\nInterestingly, the plot for NOx and CO (not shown, but easily produced) did not show such a strong contribution for south-easterly winds. This raises the question whether the higher particle concentrations seen for these wind directions are dominated by different sources (i.e. not the road itself). One explanation is that during easterly flow, concentrations are strongly affected by long-range transport. However, as shown in the diurnal plot in Figure 9.1, the contribution from the south-east also has a sharply defined profile — showing very low concentrations at night, similar to the likely contribution from the road. This type of profile might not be expected from a long-range source where emissions are well-mixed and secondary particle formation has had time to occur. The same is also true for the day of the week plot, where there is little evidence of ‘smeared-out’ long-range transport sources. These findings may suggest a different, local source of PM10 that is not the road itself. Clearly, a more detailed analysis would be required to confirm the patterns shown; but it does highlight the benefit of being able to analyse data in different ways.\n\nlibrary(openair) # load openair\npolarAnnulus(mydata, \n             pollutant = \"pm10\", \n             period = \"trend\", \n             main = \"Trend\")\n\npolarAnnulus(mydata, \n             pollutant = \"pm10\", \n             period = \"season\", \n             main = \"Season\")\n\npolarAnnulus(mydata, \n             pollutant = \"pm10\", \n             period = \"weekday\", \n             main = \"Weekday\")\n\npolarAnnulus(mydata, \n             pollutant = \"pm10\",\n             period = \"hour\", \n             main = \"Hour\")\n\n\n\n\n\n(a) By trend.\n\n\n\n\n\n\n(b) By season.\n\n\n\n\n\n\n\n\n(c) By weekday.\n\n\n\n\n\n\n(d) By hour.\n\n\n\n\nFigure 9.1: Examples of the polarAnnulus function applied to Marylebone Road.\n\n\n\nWhere there is interest in considering the wind direction dependence of concentrations, it can be worth filtering for wind speeds. At low wind speed with wind direction becomes highly variable (and is often associated with high pollutant concentrations). Therefore, for some situations it might be worth considering removing the very low wind speeds. The code below provides two ways of doing this using the dplyr filter function. The first selects data where the wind speed is >2 m s-1. The second part shows how to select wind speeds greater than the 10th percentile, using the quantile function. The latter way of selecting is quite useful, because it is known how much data are selected i.e. in this case 90%. It is worth experimenting with different values because it is also important not to lose information by ignoring wind speeds that provide useful information.\n\n## wind speed >2\npolarAnnulus(filter(mydata, ws > 2), \n             pollutant = \"pm10\", \n             type = \"hour\")\n\n## wind speed > 10th percentile\npolarAnnulus(filter(\n  mydata, \n  ws > quantile(ws, probs = 0.1, na.rm = TRUE)\n  ),\n  pollutant =\"pm10\", \n  type = \"hour\")"
  },
  {
    "objectID": "trajectory-analysis.html#introduction",
    "href": "trajectory-analysis.html#introduction",
    "title": "10  Trajectory analysis",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\nBack trajectories are extremely useful in air pollution and can provide important information on air mass origins. Despite the clear usefulness of back trajectories, their use tends to be restricted to the research community. Back trajectories are used for many purposes from understanding the origins of air masses over a few days to undertaking longer term analyses. They are often used to filter air mass origins to allow for more refined analyses of air pollution — for example trends in concentration by air mass origin. They are often also combined with more sophisticated analyses such as cluster analysis to help group similar type of air mass by origin.\nPerhaps one of the reasons why back trajectory analysis is not carried out more often is that it can be time consuming to do. This is particularly so if one wants to consider several years at several sites. It can also be difficult to access back trajectory data. In an attempt to overcome some of these issues and expand the possibilities for data analysis, openair makes several functions available to access and analyse pre-calculated back trajectories.\nCurrently these functions allow for the import of pre-calculated back trajectories are several pre-define locations and some trajectory plotting functions. In time all of these functions will be developed to allow more sophisticated analyses to be undertaken. .\nThe importTraj function imports pre-calculated back trajectories using the Hysplit trajectory model Hybrid Single Particle Lagrangian Integrated Trajectory Model (Stein et al. 2015). Trajectories are run at 3-hour intervals and stored in yearly files (see below). The trajectories are started at ground-level (10m) and propagated backwards in time. The data are stored on web-servers at Ricardo Energy & Environment similar to that for importAURN, which makes it very easy to import pre-processed trajectory data for a range of locations and years. 1\n\nStein, A. F., R. R. Draxler, G. D. Rolph, B. J. B. Stunder, M. D. Cohen, and F. Ngan. 2015. “NOAA’s HYSPLIT Atmospheric Transport and Dispersion Modeling System.” Bulletin of the American Meteorological Society 96 (12): 2059–77. https://doi.org/10.1175/bams-d-14-00110.1.\n\n\n\n\n\n\nGenerate your own back trajectories\n\n\n\nUsers may for various reasons wish to run Hysplit themselves e.g. for different starting heights, longer periods or more locations. Code and instructions have been provided in Appendix B for users wishing to do this. Users can also use different means of calculating back trajectories e.g. ECMWF and plot them in openair provided a few basic fields are present: date (POSIXct), lat (decimal latitude), lon (decimal longitude) and hour.inc the hour offset from the arrival date (i.e. from zero decreasing to the length of the back trajectories). See ?importTraj for more details.\n\n\nThese trajectories have been calculated using the Global NOAA-NCEP/NCAR reanalysis data archives. The global data are on a latitude-longitude grid (2.5°). Note that there are many meteorological data sets that can be used to run Hysplit e.g. including ECMWF data. However, in order to make it practicable to run and store trajectories for many years and sites, the NOAA-NCEP/NCAR reanalysis data is most useful. In addition, these archives are available for use widely, which is not the case for many other data sets e.g. ECMWF. Hysplit calculated trajectories based on archive data may be distributed without permission (see https://ready.arl.noaa.gov/HYSPLIT_agreement.php). For those wanting, for example, to consider higher resolution meteorological data sets it may be better to run the trajectories separately.\nopenair uses the mapproj package to allow users to user different map projections. By default, the projection used is Lambert conformal, which is a conic projection best used for mid-latitude areas. The Hysplit model itself will use any one of three different projections depending on the latitude of the origin. If the latitude greater than 55.0 (or less than -55.0) then a polar stereographic projection is used, if the latitude greater than -25.0 and less than 25.0 the mercator projection is used and elsewhere (the mid-latitudes) the Lambert projection. All these projections (and many others) are available in the mapproj package.\nUsers should see the help file for importTraj to get an up to date list of receptors where back trajectories have been calculated. First, the packages are loaded that are needed.\n\nlibrary(openair)\nlibrary(tidyverse)\nlibrary(lubridate)\n\nAs an example, we will import trajectories for London in 2010. Importing them is easy:\n\n\n\n\ntraj <- importTraj(site = \"london\", year = 2010)\n\nThe file itself contains lots of information that is of use for plotting back trajectories:\n\nhead(traj)\n\n  receptor year month day hour hour.inc    lat    lon height pressure\n1        1 2010     1   1    9        0 51.500 -0.100   10.0    994.7\n2        1 2010     1   1    8       -1 51.766  0.057   10.3    994.9\n3        1 2010     1   1    7       -2 52.030  0.250   10.5    995.0\n4        1 2010     1   1    6       -3 52.295  0.488   10.8    995.0\n5        1 2010     1   1    5       -4 52.554  0.767   11.0    995.4\n6        1 2010     1   1    4       -5 52.797  1.065   11.3    995.6\n                date2                date\n1 2010-01-01 09:00:00 2010-01-01 09:00:00\n2 2010-01-01 08:00:00 2010-01-01 09:00:00\n3 2010-01-01 07:00:00 2010-01-01 09:00:00\n4 2010-01-01 06:00:00 2010-01-01 09:00:00\n5 2010-01-01 05:00:00 2010-01-01 09:00:00\n6 2010-01-01 04:00:00 2010-01-01 09:00:00\n\n\nThe traj data frame contains among other things the latitude and longitude of the back trajectory, the height (m) and pressure (Pa) of the trajectory. The date field is the arrival time of the air-mass and is useful for linking with ambient measurement data.\nThe trajPlot function is used for plotting back trajectory lines and density plots and has the following options:"
  },
  {
    "objectID": "trajectory-analysis.html#plotting-trajectories",
    "href": "trajectory-analysis.html#plotting-trajectories",
    "title": "10  Trajectory analysis",
    "section": "\n10.2 Plotting trajectories",
    "text": "10.2 Plotting trajectories\nNext, we consider how to plot back trajectories with a few simple examples. The first example will consider a potentially interesting period when the Icelandic volcano, Eyjafjallajokull erupted in April 2010. The eruption of Eyjafjallajokull resulted in a flight-ban that lasted six days across many European airports. In Figure 10.1 selectByDate is used to consider the 7 days of interest and we choose to plot the back trajectories as lines rather than points (the default). Figure 10.1 does indeed show that many of the back trajectories originated from Iceland over this period. Note also the plot automatically includes a world base map. The base map itself is not at very high resolution by default but is useful for the sorts of spatial scales that back trajectories exist over. The base map is also global, so provided that there are pre-calculated back trajectories, these maps can be generated anywhere in the world. By default the function uses the ‘world’ map from the maps package. If map.res = \"hires\" then the (much) more detailed base map worldHires from the mapdata package is used.2\n\nselectByDate(traj,\n  start = \"15/4/2010\",\n  end = \"21/4/2010\"\n) %>%\n  trajPlot(\n    map.cols = openColours(\"hue\", 10),\n    col = \"grey30\"\n  )\n\n\n\nFigure 10.1: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010. Note the additional option to vary the country colours using map.cols. By default the map colours for all countries are grey.\n\n\n\n\nNote that trajPlot will only plot full length trajectories. This can be important when plotting something like a single month e.g. by using selectByDate when on partial sections of some trajectories may be selected.\nThere are a few other ways of representing the data shown in Figure 10.1. For example, it might be useful to plot the trajectories for each day. To do this we need to make a new column day which can be used in the plotting. The first example considers plotting the back trajectories in separate panels (Figure 10.2).\n\n## make a day column\ntraj$day <- as.Date(traj$date)\n\n## plot it choosing a specific layout\nselectByDate(traj,\n  start = \"15/4/2010\",\n  end = \"21/4/2010\"\n) %>%\n  trajPlot(\n    type = \"day\",\n    layout = c(7, 1)\n  )\n\n\n\nFigure 10.2: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, shown separately for each day.\n\n\n\n\nAnother way of plotting the data is to group the trajectories by day and colour them. This time we also set a few other options to get the layout we want — shown in Figure 10.3.\n\nselectByDate(traj,\n  start = \"15/4/2010\",\n  end = \"21/4/2010\"\n) %>%\n  trajPlot(\n    group = \"day\", col = \"turbo\",\n    lwd = 2, key.pos = \"top\",\n    key.col = 4,\n    ylim = c(50, 79)\n  )\n\n\n\nFigure 10.3: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, shown grouped for each day and coloured accordingly.\n\n\n\n\nSo far the plots have provided information on where the back trajectories come from, grouped or split by day. It is also possible, in common with most other openair functions to split the trajectories by many other variables e.g. month, season and so on. However, perhaps one of the most useful approaches is to link the back trajectories with the concentrations of a pollutant. As mentioned previously, the back trajectory data has a column date representing the arrival time of the air mass that can be used to link with concentration measurements. A couple of steps are required to do this using the left_join function.\n\n## import data for North Kensington\nkc1 <- importAURN(\"kc1\", year = 2010)\n# now merge with trajectory data by 'date'\ntraj <- left_join(traj, kc1, by = \"date\")\n## look at first few lines\nhead(traj)\n\n  receptor year month        day hour hour.inc    lat    lon height pressure\n1        1 2010     1 2010-01-01    9        0 51.500 -0.100   10.0    994.7\n2        1 2010     1 2010-01-01    8       -1 51.766  0.057   10.3    994.9\n3        1 2010     1 2010-01-01    7       -2 52.030  0.250   10.5    995.0\n4        1 2010     1 2010-01-01    6       -3 52.295  0.488   10.8    995.0\n5        1 2010     1 2010-01-01    5       -4 52.554  0.767   11.0    995.4\n6        1 2010     1 2010-01-01    4       -5 52.797  1.065   11.3    995.6\n                date2                date                 site code  co nox no2\n1 2010-01-01 09:00:00 2010-01-01 09:00:00 London N. Kensington  KC1 0.3  38  29\n2 2010-01-01 08:00:00 2010-01-01 09:00:00 London N. Kensington  KC1 0.3  38  29\n3 2010-01-01 07:00:00 2010-01-01 09:00:00 London N. Kensington  KC1 0.3  38  29\n4 2010-01-01 06:00:00 2010-01-01 09:00:00 London N. Kensington  KC1 0.3  38  29\n5 2010-01-01 05:00:00 2010-01-01 09:00:00 London N. Kensington  KC1 0.3  38  29\n6 2010-01-01 04:00:00 2010-01-01 09:00:00 London N. Kensington  KC1 0.3  38  29\n  no o3 so2 pm10 pm2.5 v10 v2.5 nv10 nv2.5 ws wd air_temp\n1  6 46   0    8    NA   0   NA    8    NA NA NA       NA\n2  6 46   0    8    NA   0   NA    8    NA NA NA       NA\n3  6 46   0    8    NA   0   NA    8    NA NA NA       NA\n4  6 46   0    8    NA   0   NA    8    NA NA NA       NA\n5  6 46   0    8    NA   0   NA    8    NA NA NA       NA\n6  6 46   0    8    NA   0   NA    8    NA NA NA       NA\n\n\nThis time we can use the option pollutant in the function trajPlot, which will plot the back trajectories coloured by the concentration of a pollutant. Figure 10.4 does seem to show elevated PM10 concentrations originating from Iceland over the period of interest. In fact, these elevated concentrations occur on two days as shown in Figure 10.2. However, care is needed when interpreting such data because other analysis would need to rule out other reasons why PM10 could be elevated; in particular due to local sources of PM10. There are lots of openair functions that can help here e.g. timeVariation or timePlot to see if NOx concentrations were also elevated (which they seem to be). It would also be worth considering other sites for back trajectories that could be less influenced by local emissions.\n\nselectByDate(traj,\n  start = \"15/4/2010\",\n  end = \"21/4/2010\"\n) %>%\n  trajPlot(\n    pollutant = \"pm10\",\n    col = \"turbo\", lwd = 2\n  )\n\n\n\nFigure 10.4: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, coloured by the concentration of PM10 in μg m-3.\n\n\n\n\nHowever, it is possible to account for the PM that is local to some extent by considering the relationship between NOx and PM10 (or PM2.5). For example, using scatterPlot (not shown):\n\nscatterPlot(kc1, x = \"nox\", \n            y = \"pm2.5\", \n            avg = \"day\", \n            linear = TRUE)\n\nwhich suggests a gradient of 0.084. Therefore, we can remove the PM10 that is associated NOx in kc1 data, making a new column pm.new:\n\nkc1 <- mutate(kc1, pm.new = pm10 - 0.084 * nox)\n\nWe have already merged kc1 with traj, so to keep things simple we import traj again and merge it with kc1. Note that if we had thought of this initially, pm.new would have been calculated first before merging with traj.\n\ntraj <- importTraj(site = \"london\", year = 2010)\ntraj <- left_join(traj, kc1, by = \"date\")\n\nNow it is possible to plot the trajectories:\n\nselectByDate(traj,\n  start = \"15/4/2010\",\n  end = \"21/4/2010\"\n) %>%\n  trajPlot(\n    pollutant = \"pm.new\",\n    col = \"turbo\", lwd = 2\n  )\n\nWhich, interestingly still clearly shows elevated PM10 concentrations for those two days that cross Iceland. The same is also true for PM2.5. However, as mentioned previously, checking other sites in more rural areas would be a good idea."
  },
  {
    "objectID": "trajectory-analysis.html#trajLevel",
    "href": "trajectory-analysis.html#trajLevel",
    "title": "10  Trajectory analysis",
    "section": "\n10.3 Trajectory gridded frequencies",
    "text": "10.3 Trajectory gridded frequencies\nThe Hysplit model itself contains various analysis options for gridding trajectory data. Similar capabilities are also available in openair where the analyses can be extended using other openair capabilities. It is useful to gain an idea of where trajectories come from. Over the course of a year representing trajectories as lines or points results in a lot of over-plotting. Therefore it is useful to grid the trajectory data and calculate various statistics by considering latitude-longitude intervals. sec- The first analysis considers the number of unique trajectories in a particular grid square. This is achieved by using the trajLevel function and setting the statistic option to “frequency”. Figure 10.5 shows the frequency of back trajectory crossings for the North Kensington data. In this case it highlights that most trajectory origins are from the west and north for 2010 at this site. Note that in this case, pollutant can just be the trajectory height (or another numeric field) rather than an actual pollutant because only the frequencies are considered.\n\ntrajLevel(traj, statistic = \"frequency\")\n\n\n\nFigure 10.5: Gridded back trajectory frequencies. The border = NA option removes the border around each grid cell.\n\n\n\n\nIt is also possible to use hexagonal binning to gain an idea about trajectory frequencies. In this case each 3-hour point along each trajectory is used in the counting. The code below focuses more on Europe and uses the hexagonal binning method. Note that the effect of the very few high number of points at the origin has been diminished by plotting the data on a log scale — see Section 26.2.1 for details.\n\nfilter(traj, lat > 30, lat < 70, lon > -30, lon < 20) %>%\n  trajLevel(\n    method = \"hexbin\", col = \"turbo\",\n    xbin = 40\n  )\n\n\n\nFigure 10.6: Gridded back trajectory frequencies with hexagonal binning."
  },
  {
    "objectID": "trajectory-analysis.html#trajectory-source-contribution-functions",
    "href": "trajectory-analysis.html#trajectory-source-contribution-functions",
    "title": "10  Trajectory analysis",
    "section": "\n10.4 Trajectory source contribution functions",
    "text": "10.4 Trajectory source contribution functions\nBack trajectories offer the possibility to undertake receptor modelling to identify the location of major emission sources. When many back trajectories (over months to years) are analysed in specific ways they begin to show the geographic origin most associated with elevated concentrations. With enough (dissimilar) trajectories those locations leading to the highest concentrations begin to be revealed. When a whole year of back trajectory data is plotted the individual back trajectories can extend 1000s of km. There are many approaches using back trajectories in this way and (Fleming, Monks, and Manning 2012) provide a good overview of the methods available. openair has implemented a few of these techniques and over time these will be refined and extended.\n\n10.4.1 Identifying the contribution of high concentration back trajectories\nA useful analysis to undertake is to consider the pattern of frequencies for two different conditions. In particular, there is often interest in the origin of high concentrations for different pollutants. For example, compared with data over a whole year, how do the frequencies of occurrence differ? Figure 10.7 shows an example of such an analysis for PM10 concentrations. By default the function will compare concentrations >90th percentile with the full year. The percentile level is controlled by the option percentile. Note also there is an option min.bin that will exclude grid cells where there are fewer than min.bin data points. The analysis compares the percentage of time the air masses are in particular grid squares for all data and a subset of data where the concentrations are greater than the given percentile. The graph shows the absolute percentage difference between the two cases i.e. high minus base.\nFigure 10.7 shows that compared with the whole year, high PM10 concentrations (90th percentile) are more prevalent when the trajectories originate from the east, which is seen by the positive values in the plot. Similarly there are relatively fewer occurrences of these high concentration back trajectories when they originate from the west. This analysis is in keeping with the highest PM10 concentrations being largely controlled by secondary aerosol formation from air-masses originating during anticyclonic conditions from mainland Europe.\n\ntrajLevel(traj, pollutant = \"pm10\", \n          statistic = \"difference\",\n          col = c(\"skyblue\", \"white\", \"tomato\"), \n          min.bin = 50, \n          border = NA, \n          xlim = c(-20, 20), \n          ylim = c(40, 70))\n\n\n\nFigure 10.7: Gridded back trajectory frequencies showing the percentage difference in occurrence for high PM10 concentrations (90th percentile) compared with conditions over the full year.\n\n\n\n\nNote that it is also possible to use conditioning with these plots. For example to split the frequency results by season:\n\ntrajLevel(traj, \n          pollutant = \"pm10\", \n          statistic = \"frequency\", \n          col = \"heat\",\n          type = \"season\")\n\n\n10.4.2 Allocating trajectories to different wind sectors\nOne of the key aspects of trajectory analysis is knowing something about where air masses have come from. Cluster analysis can be used to group trajectories based on their origins and this is discussed in Section 10.8. A simple approach is to consider different wind sectors e.g. N, NE, E and calculate the proportion of time a particular back trajectory resides in a specific sector. It is then possible to allocate a particular trajectory to a sector based on some assumption about the proportion of time it is in that sector — for example, assume a trajectory is from the west sector if it spends at least 50% of its time in that sector or otherwise record the allocation as ‘unallocated’. The code below can be used as the basis of such an approach.\n\n\n\nFirst we import the trajectories, which in this case are for London in 2010:\n\ntraj <- importTraj(site = \"london\", year = 2010)\n\n\nalloc <- traj\n\nid <- which(alloc$hour.inc == 0) \ny0 <- alloc$lat[id[1]]\nx0 <- alloc$lon[id[1]]\n\n## calculate angle and then assign sector\nalloc <- mutate(\n  alloc, \n  angle = atan2(lon - x0, lat - y0) * 360 / 2 / pi,\n  angle = ifelse(angle < 0, angle + 360 , angle),\n  sector = cut(angle, \n               breaks = seq(22.5, 382.5, 45),\n               labels = c(\"NE\", \"E\", \"SE\", \n                          \"S\", \"SW\", \"W\",\n                          \"NW\", \"N\")),\n  sector = as.character(sector),\n  sector = ifelse(is.na(sector), \"N\", sector)\n) \n\nalloc <- group_by(alloc, date, sector) %>% \n  mutate(n = n()) %>% \n  group_by(date) %>% \n  arrange(date, n) %>% \n  slice_tail(n = 1) %>% \n  mutate(sector = ifelse(n > 50, sector, \"unallocated\")) %>% \n  select(date, sector, n)\n\n# combine with trajectories\ntraj <- left_join(traj, alloc, by = \"date\")\n\nNow it is possible to post-process the data. traj now has the angle, sector and allocation (sector).\n\nhead(traj)\n\n  receptor year month day hour hour.inc    lat    lon height pressure\n1        1 2010     1   1    9        0 51.500 -0.100   10.0    994.7\n2        1 2010     1   1    8       -1 51.766  0.057   10.3    994.9\n3        1 2010     1   1    7       -2 52.030  0.250   10.5    995.0\n4        1 2010     1   1    6       -3 52.295  0.488   10.8    995.0\n5        1 2010     1   1    5       -4 52.554  0.767   11.0    995.4\n6        1 2010     1   1    4       -5 52.797  1.065   11.3    995.6\n                date2                date      sector n\n1 2010-01-01 09:00:00 2010-01-01 09:00:00 unallocated 9\n2 2010-01-01 08:00:00 2010-01-01 09:00:00 unallocated 9\n3 2010-01-01 07:00:00 2010-01-01 09:00:00 unallocated 9\n4 2010-01-01 06:00:00 2010-01-01 09:00:00 unallocated 9\n5 2010-01-01 05:00:00 2010-01-01 09:00:00 unallocated 9\n6 2010-01-01 04:00:00 2010-01-01 09:00:00 unallocated 9\n\n\nFirst, merge the air quality data from North Kensington:\n\ntraj <- left_join(traj, kc1, by = \"date\")\n\nWe can work out the mean concentration by allocation, which shows the clear importance for the east and south-east sectors.\n\ngroup_by(traj, sector) %>% \n  summarise(PM2.5 = mean(pm2.5, na.rm = TRUE))\n\n# A tibble: 9 × 2\n  sector      PM2.5\n  <chr>       <dbl>\n1 E            21.6\n2 N            11.8\n3 NE           12.6\n4 NW           13.6\n5 S            14.5\n6 SE           28.7\n7 SW           10.8\n8 W            11.8\n9 unallocated  15.5\n\n\nFinally, the percentage of the year in each sector can be calculated as follows:\n\ngroup_by(traj, sector) %>% \n  summarise(n = n()) %>% \n  mutate(percent = 100 * n / nrow(traj))\n\n# A tibble: 9 × 3\n  sector          n percent\n  <chr>       <int>   <dbl>\n1 E           18898    6.71\n2 N           15520    5.51\n3 NE          28867   10.3 \n4 NW          25802    9.16\n5 S            6111    2.17\n6 SE           5820    2.07\n7 SW          24638    8.75\n8 W           72750   25.8 \n9 unallocated 83141   29.5"
  },
  {
    "objectID": "trajectory-analysis.html#potential-source-contribution-function-pscf",
    "href": "trajectory-analysis.html#potential-source-contribution-function-pscf",
    "title": "10  Trajectory analysis",
    "section": "\n10.5 Potential Source Contribution Function (PSCF)",
    "text": "10.5 Potential Source Contribution Function (PSCF)\nIf statistic = \"pscf\" then the Potential Source Contribution Function (PSCF) is plotted. The PSCF calculates the probability that a source is located at latitude \\(i\\) and longitude \\(j\\) (Fleming, Monks, and Manning 2012; Pekney et al. 2006). The PSCF is somewhat analogous to the CPF function described on Section 7.3 that considers local wind direction probabilities. In fact, the two approaches have been shown to work well together (Pekney et al. 2006). The PSCF approach has been widely used in the analysis of air mass back trajectories. (Ara Begum et al. 2005) for example assessed the method against the known locations of wildfires and found it performed well for PM2.5, EC (elemental carbon) and OC (organic carbon) and that other (non-fire related) species such as sulphate had different source origins. The basis of PSCF is that if a source is located at (\\(i\\), \\(j\\)), an air parcel back trajectory passing through that location indicates that material from the source can be collected and transported along the trajectory to the receptor site. PSCF solves\n\nFleming, Z. L., P. S. Monks, and A. J. Manning. 2012. “Review: Untangling the influence of air-mass history in interpreting observed atmospheric composition.” Atmospheric Research 104-105: 1–39. https://doi.org/10.1016/j.atmosres.2011.09.009.\n\nPekney, Natalie J., Cliff I. Davidson, Liming Zhou, and Philip K. Hopke. 2006. “Application of PSCF and CPF to PMF-Modeled Sources of PM 2.5 in Pittsburgh.” Aerosol Science and Technology 40 (10): 952–61. https://doi.org/10.1080/02786820500543324.\n\nAra Begum, Bilkis, Eugene Kim, Cheol-Heon Jeong, Doh-Won Lee, and Philip K. Hopke. 2005. “Evaluation of the potential source contribution function using the 2002 Quebec forest fire episode.” Atmospheric Environment 39 (20): 3719–24. https://doi.org/10.1016/j.atmosenv.2005.03.008.\n\\[\nPSCF = \\frac{m_{ij}}{n_{ij}}\n\\tag{10.1}\\]\nwhere \\(n_{ij}\\) is the number of times that the trajectories passed through the cell (\\(i\\), \\(j\\)) and \\(m_{ij}\\) is the number of times that a source concentration was high when the trajectories passed through the cell (\\(i\\), \\(j\\)). The criterion for determining \\(m_{ij}\\) is controlled by percentile, which by default is 90. Note also that cells with few data have a weighting factor applied to reduce their effect.\nAn example of a PSCF plot is shown in Figure 10.8 for PM2.5 for concentrations 90th percentile. This Figure gives a very clear indication that the principal (high) sources are dominated by source origins in mainland Europe — particularly around the Benelux countries.\n\nfilter(traj, lon > -20, lon < 20, lat > 45, lat < 60) %>%\n  trajLevel(\n    pollutant = \"pm2.5\", statistic = \"pscf\",\n    col = \"increment\",\n    border = NA\n  )\n\n\n\nFigure 10.8: PSCF probabilities for PM2.5 concentrations (90th percentile)."
  },
  {
    "objectID": "trajectory-analysis.html#concentration-weighted-trajectory-cwt",
    "href": "trajectory-analysis.html#concentration-weighted-trajectory-cwt",
    "title": "10  Trajectory analysis",
    "section": "\n10.6 Concentration Weighted Trajectory (CWT)",
    "text": "10.6 Concentration Weighted Trajectory (CWT)\nA limitation of the PSCF method is that grid cells can have the same PSCF value when sample concentrations are either only slightly higher or much higher than the criterion (Hsu, Holsen, and Hopke 2003). As a result, it can be difficult to distinguish moderate sources from strong ones. (Seibert et al. 1994) computed concentration fields to identify source areas of pollutants. This approach is sometimes referred to as the CWT or CF (concentration field). A grid domain was used as in the PSCF method. For each grid cell, the mean (CWT) or logarithmic mean (used in the Residence Time Weighted Concentration (RTWC) method) concentration of a pollutant species was calculated as follows:\n\nSeibert, P, H Kromp-Kolb, U Baltensperger, and DT Jost. 1994. “Trajectory Analysis of High-Alpine Air Pollution Data.” NATO Challenges of Modern Society 18: 595–95.\n\\[\n  ln(\\overline{C}_{ij}) = \\frac{1}{\\sum_{k=1}^{N}\\tau_{ijk}}\\sum_{k=1}^{N}ln(c_k)\\tau_{ijk}\n\\tag{10.2}\\]\nwhere \\(i\\) and \\(j\\) are the indices of grid, \\(k\\) the index of trajectory, \\(N\\) the total number of trajectories used in analysis, \\(c_k\\) the pollutant concentration measured upon arrival of trajectory \\(k\\), and \\(\\tau_{ijk}\\) the residence time of trajectory \\(k\\) in grid cell (\\(i\\), \\(j\\)). A high value of \\(\\overline{C}_{ij}\\) means that, air parcels passing over cell (\\(i\\), \\(j\\)) would, on average, cause high concentrations at the receptor site.\nFigure 10.9 shows the situation for PM2.5 concentrations. It was calculated by recording the associated PM2.5 concentration for each point on the back trajectory based on the arrival time concentration using 2010 data. The plot shows the geographic areas most strongly associated with high PM2.5 concentrations i.e. to the east in continental Europe. Both the CWT and PSCF methods have been shown to give similar results and each have their advantages and disadvantages (Lupu and Maenhaut 2002; Hsu, Holsen, and Hopke 2003). Figure 10.9 can be compared with Figure 10.8 to compare the overall identification of source regions using the CWT and PSCF techniques. Overall the agreement is good in that similar geographic locations are identified as being important for PM2.5.\n\nLupu, Alexandru, and Willy Maenhaut. 2002. “Application and comparison of two statistical trajectory techniques for identification of source regions of atmospheric aerosol species.” Atmospheric Environment 36: 5607–18.\n\nHsu, Ying-Kuang, Thomas M. Holsen, and Philip K. Hopke. 2003. “Comparison of hybrid receptor models to locate PCB sources in Chicago.” Atmospheric Environment 37 (4): 545–62. https://doi.org/10.1016/S1352-2310(02)00886-5.\n\nfilter(traj, lon > -20, lon < 20, lat > 45, lat < 60) %>%\n  trajLevel(\n    pollutant = \"pm2.5\",\n    statistic = \"cwt\",\n    col = \"increment\",\n    border = \"white\"\n  )\n\n\n\nFigure 10.9: Gridded back trajectory concentrations showing mean PM2.5 concentrations using the CWT approach.\n\n\n\n\nFigure 10.9 is useful, but it can be clearer if the trajectory surface is smoothed, which has been done for PM2.5 concentrations shown in Figure 10.10.\n\nfilter(traj, lat > 45 & lat < 60 & lon > -20 & lon < 20) %>%\n  trajLevel(\n    pollutant = \"pm2.5\",\n    statistic = \"cwt\",\n    smooth = TRUE,\n    col = \"increment\"\n  )\n\n\n\nFigure 10.10: Gridded and smoothed back trajectory concentrations showing mean PM2.5 concentrations using the CWT approach.\n\n\n\n\nIn common with most other openair functions, the flexible type option can be used to split the data in different ways. For example, to plot the smoothed back trajectories for PM2.5 concentrations by season.\n\nfilter(traj, lat > 40 & lat < 70 & lon > -20 & lon < 20) %>%\n  trajLevel(\n    pollutant = \"pm2.5\",\n    type = \"season\",\n    statistic = \"pscf\",\n    layout = c(4, 1)\n  )\n\nIt should be noted that it makes sense to analyse back trajectories for pollutants that have a large regional component — such as particles or O3. It makes little sense to analyse pollutants that are known to have local impacts e.g. NOx However, a species such as NOx can be helpful to exclude ‘fresh’ emissions from the analysis."
  },
  {
    "objectID": "trajectory-analysis.html#simplified-quantitative-transport-bias-analysis-sqtba",
    "href": "trajectory-analysis.html#simplified-quantitative-transport-bias-analysis-sqtba",
    "title": "10  Trajectory analysis",
    "section": "\n10.7 Simplified Quantitative Transport Bias Analysis (SQTBA)",
    "text": "10.7 Simplified Quantitative Transport Bias Analysis (SQTBA)\nBoth CWT and PSCF techniques is essence assume that the trajectory paths represent a good estimate of the centreline of the air mass origin. However, trajectory analysis is implicitly uncertain, not least because of the difficulty of accurately describing the dispersion of pollutants in the atmosphere. Hysplit has many approaches to deal with these uncertainties such as running an ensemble of trajectories with different starting locations. However, for longer term analysis it can be very time consuming to run the model to explore the influence of meteorological variation.\nSQTBA is an approach that recognises the natural processes of plume dispersion and uses this understanding as part of back trajectory analysis. The approach has been used in many studies that aim to better understand sources regions for PM2.5 (or components of PM2.5) e.g. Brook, Johnson, and Mamedov (2004) considering PM2.5 in North America, Zhao, Hopke, and Zhou (2007) modelling particulate nitrate and sulphate, L. Zhou (2004) comparing different source attribution techniques including SQTBA and the consideration of sources of ammonia in New York State (C. Zhou et al. 2019).\n\nBrook, Jeffrey R., David Johnson, and Alexandre Mamedov. 2004. “Determination of the Source Areas Contributing to Regionally High Warm Season PM2.5in Eastern North America.” Journal of the Air & Waste Management Association 54 (9): 1162–69. https://doi.org/10.1080/10473289.2004.10470984.\n\nZhao, Weixiang, Philip K. Hopke, and Liming Zhou. 2007. “Spatial Distribution of Source Locations for Particulate Nitrate and Sulfate in the Upper-Midwestern United States.” Atmospheric Environment 41 (9): 1831–47. https://doi.org/10.1016/j.atmosenv.2006.10.060.\n\nZhou, L. 2004. “Comparison of Two Trajectory Based Models for Locating Particle Sources for Two Rural New York Sites.” Atmospheric Environment 38 (13): 1955–63. https://doi.org/10.1016/j.atmosenv.2003.12.034.\n\nZhou, Chuanlong, Hao Zhou, Thomas M. Holsen, Philip K. Hopke, Eric S. Edgerton, and James J. Schwab. 2019. “Ambient Ammonia Concentrations Across New York State.” Journal of Geophysical Research: Atmospheres 124 (14): 8287–8302. https://doi.org/10.1029/2019jd030380.\nThe basic idea, only described briefly here, is that there is a natural transport behaviour of dispersing air masses that can be described by the Gaussian plume equation. Rather than a single back trajectory being considered, it is assumed that concentrations are represented according to Gaussian plume dilution along each trajectory — giving a probability of a certain concentration at a particular point on a grid. In effect, the further from a receptor (measurement site), the wider the plume (σ value), which has the effect of spreading-out the likely origin of the source. This approach is an effective, pragmatic solution to better account for the uncertainties in the location of upwind sources.\nQ(x,t| x’, t’), is the probability for the air parcel at position x’ at time t’ to arrive at position x at time t is given by:\n\\[\n  Q(x, t, x', t') = \\frac{1}{2 \\pi \\sigma_x(t') \\sigma_y(t')}e^{-\\frac{1}{2}\\Bigg[\\bigg(\\frac{X - x'(t')}{\\sigma_x(t')}\\bigg)^2 + \\bigg(\\frac{Y - y'(t')}{\\sigma_y(t')}\\bigg)^2\\Bigg]}\n\\]\nX, Y: Coordinate of the grid centre\nx’ y’: Centreline of trajectory\nσ: Standard deviation of trajectories at two directions which are assumed to grow with time\nσx(t’) = σy(t’) = at’, a is a dispersion speed, equals to 1.5 km h-1. Note that the default σ is assumed to be 1.5 km h-1 rather than 5.4 km h-1 that is commonly used in other studies because testing shows that a lower value reveals more information about source origins without introducing noise. In practice, the value of σ will vary depending on meteorological conditions and be influenced by both turbulence and wind shear. Users can set their own value of σ in the function options.\nThis means at each trajectory point, the probability is calculated across a pre-defined grid. These calculations can take some time to run. For example, with 96 hour back trajectories calculated every 3 hours in a year, there are 840,960 trajectory end points. Each one of these points is used to estimate the probability on a grid, which itself can be large (a one degree grid from 35 to 80°N and -55 to 25°W is 3600 grid points).\nA potential mass transfer potential field is calculated for a given trajectory l, arriving at time t, is integrated over back trajectory time 𝜏:\n\\[\n\\overline{T_l}(x | x') = \\frac{\\int_{t-\\tau}^{t} Q(x,t| x', t')dt'}{\\int_{t-\\tau}^{t}dt'}\n\\]\nand the concentration-weighted field calculated as:\n\\[\n\\widetilde{T}(x|x') = \\sum_{l=1}^{l=L}\\overline{T_l}(x|x')c_l\n\\]\ncl: Receptor concentration of back trajectory I\nL: Total number of back trajectories\nAnd the final SQTBA is calculated as:\n\\[\nSQTBA(x|x') = \\frac{\\widetilde{T}(x|x')}{\\sum_{l=1}^{l=L}\\overline{T_l}(x|x')}\n\\]\nAs an example, consideration is given to PM2.5 concentrations in a similar way as shown for CWT and the PSCF techniques. This plot indicates several potential sources of PM2.5 precursors with the Benelux countries and around the Po Valley in Italy (known high NOx emission regions and potential sources of ammonium nitrate), and also a higher source region in eastern Europe across south Poland.\n\ntrajLevel(traj,\n          pollutant = \"pm2.5\",\n          statistic =  \"sqtba\",\n          map.fill = FALSE,\n          cols = \"default\",\n          lat.inc = 0.5,\n          lon.inc = 0.5\n          )\n\n\n\nFigure 10.11: SQTBA approach applied to PM2.5 concentrations at the North Kensington site in London for 2010.\n\n\n\n\nSome further analysis can be carried out on the TEOM FDMS measurements, which separately reports the volatile and non-volatile components of PM. Figure 10.12 shows the plot for the volatile PM2.5 component, which highlights the source regions of the Benelux counties and (likely) northern Italy.\n\ntrajLevel(traj,\n          pollutant = \"v2.5\",\n          statistic =  \"sqtba\",\n          map.fill = FALSE,\n          cols = \"default\",\n          lat.inc = 0.5,\n          lon.inc = 0.5\n          )\n\n\n\nFigure 10.12: SQTBA approach applied to volatile PM2.5 concentrations at the North Kensington site in London for 2010.\n\n\n\n\nOn the other hand, Figure 10.13 shows the non-volatile PM2.5 component, which more clearly shows eastern Europe as the principal source. This plot differs from Figure 10.12 and likely highlights the higher sulphur emissions from power generation in this part of Europe. More detailed analysis can be carried out if the components of PM are available at an hourly or daily resolution e.g. particulate nitrate, sulphate.\n\ntrajLevel(traj,\n          pollutant = \"nv2.5\",\n          statistic =  \"sqtba\",\n          map.fill = FALSE,\n          cols = \"default\",\n          lat.inc = 0.5,\n          lon.inc = 0.5\n          )\n\n\n\nFigure 10.13: SQTBA approach applied to the non-volatile PM2.5 concentrations at the North Kensington site in London for 2010.\n\n\n\n\nThe analysis above was demonstrated for a single site. However, to exploit the full potential of the SQTBA approach, multiple receptors should be considered. The reason why multiple sites should be considered is that there will be better geographic coverage of the trajectories and potential for source regions to be sampled multiple times. To conduct an analysis of multiple sites, trajectories linked to pollutant concentrations should be combined into a single data frame in the format similar to traj above but with an additional variable that identifies the different sites. When running trajLevel the option .combine should be used to identify the relevant column e.g. .combine = \"site\".\nThe results returned from multiple site analysis are normalised by each site individually by dividing the output by the mean of the pollutant concentration. This approach ensures that sites with potentially very different mean concentrations can be used in a consistent way to identify source regions, without single sites dominating the output."
  },
  {
    "objectID": "trajectory-analysis.html#sec-trajCluster",
    "href": "trajectory-analysis.html#sec-trajCluster",
    "title": "10  Trajectory analysis",
    "section": "\n10.8 Trajectory clustering",
    "text": "10.8 Trajectory clustering\nOften it is useful to use cluster analysis on back trajectories to group similar air mass origins together. The principal purpose of clustering back trajectories is to post-process data according to cluster origin. By grouping data with similar geographic origins it is possible to gain information on pollutant species with similar chemical histories. There are several ways in which clustering can be carried out and several measures of the similarity of different clusters. A key issue is how the distance matrix is calculated, which determines the similarity (or dissimilarity) of different back trajectories. The simplest measure is the Euclidean distance. However, an angle-based measure is also often used. The two distance measures are defined below. In openair the distance matrices are calculated using C\\(++\\) code because their calculation is computationally intensive. Note that these calculations can also be performed directly in the Hysplit model itself.\nThe Euclidean distance between two trajectories is given by Equation @eq-Euclid. Where \\(X_1\\), \\(Y_1\\) and \\(X_2\\), \\(Y_2\\) are the latitude and longitude coordinates of back trajectories \\(1\\) and \\(2\\), respectively. \\(n\\) is the number of back trajectory points (96 hours in this case).\n\\[\nd_{1, 2} = \\left({\\sum_{i=1}^{n} ((X_{1i} - X_{2i}) ^ 2 + (Y_{1i} - Y_{2i})) ^ 2}\\right)^{1/2}\n\\tag{10.3}\\]\nThe angle distance matrix is a measure of how similar two back trajectory points are in terms of their angle from the origin i.e. the starting location of the back trajectories. The angle-based measure will often capture some of the important circulatory features in the atmosphere e.g. situations where there is a high pressure located to the east of the UK. However, the most appropriate distance measure will be application dependent and is probably best tested by the extent to which they are able to differentiate different air-mass characteristics, which can be tested through post-processing. The angle-based distance measure is defined as:\n\\[\nd_{1, 2} = \\frac{1}{n}\\sum_{i=1}^{n}cos^{-1} \\left(0.5\\frac{A_i + B_i + C_i}{\\sqrt{A_iB_i}}\\right)  \n\\tag{10.4}\\]\nwhere\n\\[\n  A_i = (X_1(i) - X_0)^2 + (Y_1(i) - Y_0)^2\n\\tag{10.5}\\]\n\\[\n  B_i = (X_2(i) - X_0)^2 + (Y_2(i) - Y_0)^2\n\\tag{10.6}\\]\n\\[\n  C_i = (X_2(i) - X_1(i))^2 + (Y_2(i) - Y_1(i))^2\n\\tag{10.7}\\]\nwhere \\(X_0\\) and \\(Y_0\\) are the coordinates of the location being studied i.e. the starting location of the trajectories.\nAs an example we will consider back trajectories for London in 2011.\n\n\n\nFirst, the back trajectory data for London is imported together with the air pollution data for the North Kensington site (KC1).\n\ntraj <- importTraj(site = \"london\", year = 2011)\nkc1 <- importAURN(site = \"kc1\", year = 2011)\n\nThe clusters are straightforward to calculate. In this case the back trajectory data (traj) is supplied and the angle-based distance matrix is used. Furthermore, we choose to calculate 6 clusters and choose a specific colour scheme. In this case we read the output from trajCluster into a variable clust so that the results can be post-processed.\n\nclust <- trajCluster(traj, method = \"Angle\", \n                     n.cluster = 6, \n                     col = \"Set2\",\n                     map.cols = openColours(\"Paired\", 10))\n\n\n\nFigure 10.14: The 6-cluster solution to back trajectories calculated for the London North Kensington site for 2011 showing the mean trajectory for each cluster.\n\n\n\n\nclust returns all the back trajectory information together with the cluster (as a character). This data can now be used together with other data to analyse results further. However, first it is possible to show all trajectories coloured by cluster, although for a year of data there is significant overlap and it is difficult to tell them apart.\n\ntrajPlot(clust$data, group = \"cluster\")\n\nA useful way in which to see where these air masses come from by trajectory is to produce a frequency plot by cluster. Such a plot (not shown, but code below) provides a good indication of the spread of the different trajectory clusters as well as providing an indication of where air masses spend most of their time. For the London 2011 data it can be seen cluster 1 is dominated by air from the European mainland to the south.\n\ntrajLevel(clust$data, type = \"cluster\", \n          col = \"increment\", \n          border = NA)\n\nPerhaps more useful is to merge the cluster data with measurement data. In this case the data at North Kensington site are used. Note that in merging these two data frames it is not necessary to retain all 96 back trajectory hours and for this reason we extract only the first hour.\n\n# use inner join - so only where we have data in each\nkc1 <- inner_join(kc1, \n                  filter(clust$data$traj, hour.inc == 0), \n                  by = \"date\")\n\nNow kc1 contains air pollution data identified by cluster. The size of this data frame is about a third of the original size because back trajectories are only run every 3 hours.\nThe numbers of each cluster are given by:\n\ntable(kc1[[\"cluster\"]])\n\n\n C1  C2  C3  C4  C5  C6 \n347 661 989 277 280 333 \n\n\ni.e. is dominated by clusters 3 and 2 from west and south-west (Atlantic).\nNow it is possible to analyse the concentration data according to the cluster. There are numerous types of analysis that can be carried out with these results, which will depend on what the aims of the analysis are in the first place. However, perhaps one of the first things to consider is how the concentrations vary by cluster. As the summary results below show, there are distinctly different mean concentrations of most pollutants by cluster. For example, clusters 1 and 6 are associated with much higher concentrations of PM10 — approximately double that of other clusters. Both of these clusters originate from continental Europe. Cluster 5 is also relatively high, which tends to come from the rest of the UK. Other clues concerning the types of air-mass can be gained from the mean pressure. For example, cluster 5 is associated with the highest pressure (1014 kPa), and as is seen in Figure 10.14 the shape of the line for cluster 5 is consistent with air-masses associated with a high pressure system (a clockwise-type sweep).\n\ngroup_by(kc1, cluster) %>% \n  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))\n\n# A tibble: 6 × 28\n  cluster    co   nox   no2    no    o3   so2  pm10 pm2.5   v10  v2.5  nv10\n  <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 C1      0.346  89.9  51.4 25.3   32.1  3.12  38.3  31.3  8.79  8.02  29.5\n2 C2      0.205  40.9  30.9  6.55  39.4  1.46  17.2  11.5  4.18  3.13  13.1\n3 C3      0.198  47.2  32.3  9.78  39.7  1.84  18.1  11.3  3.73  2.80  14.3\n4 C4      0.189  44.1  30.6  8.89  39.7  1.56  17.8  11.0  3.41  2.17  14.4\n5 C5      0.202  57.0  39.2 11.7   41.4  2.31  24.3  16.5  5.06  4.50  19.2\n6 C6      0.267  64.9  42.4 14.8   46.3  2.93  36.6  29.7  7.78  7.57  28.8\n# … with 16 more variables: nv2.5 <dbl>, ws <dbl>, wd <dbl>, air_temp <dbl>,\n#   receptor <dbl>, year <dbl>, month <dbl>, day <dbl>, hour <dbl>,\n#   hour.inc <dbl>, lat <dbl>, lon <dbl>, height <dbl>, pressure <dbl>,\n#   traj_len <dbl>, len <dbl>\n\n\nSimple plots can be generated from these results too. For example, it is easy to consider the temporal nature of the volatile component of PM2.5 concentrations (v2.5 in the kc1 data frame). Figure 10.15 for example shows how the concentration of the volatile component of PM2.5 concentrations varies by cluster by plotting the hour of day-month variation. It is clear from Figure 10.15 that the clusters associated with the highest volatile PM2.5 concentrations are clusters 1 and 6 (European origin) and that these concentrations peak during spring. There is less data to see clearly what is going on with cluster 5. Nevertheless, the cluster analysis has clearly separated different air mass characteristics which allows for more refined analysis of different air-mass types.\n\ntrendLevel(kc1, pollutant = \"v2.5\", type = \"cluster\", \n           layout = c(6, 1),\n               cols = \"increment\")\n\n\n\nFigure 10.15: Some of the temporal characteristics of the volatile PM2.5 component plotted by month and hour of the day and by cluster for the London North Kensington site for 2011.\n\n\n\n\nSimilarly, as considered in Section 8.8, the timeVariation function can also be used to consider the temporal components.\nAnother useful plot to consider is timeProp (see Chapter 13), which can show how the concentration of a pollutant is comprised. In this case it is useful to plot the time series of PM2.5and show how much of the concentration is contributed to by each cluster. Such a plot is shown in Figure 10.16. It is now easy to see for example that during the spring months many of the high concentration events were due to clusters 1 and 6, which correspond to European origin air-masses as shown in Figure 10.14.\n\ntimeProp(kc1, pollutant = \"pm2.5\", \n         avg.time = \"day\", proportion = \"cluster\",\n         cols = \"Set2\", \n         key.position = \"top\", key.columns = 6)\n\n\n\nFigure 10.16: Temporal variation in daily PM2.5 concentrations at the North Kensington site show by contribution of each cluster."
  },
  {
    "objectID": "trajectory-analysis.html#trajectories-on-an-interactive-map",
    "href": "trajectory-analysis.html#trajectories-on-an-interactive-map",
    "title": "10  Trajectory analysis",
    "section": "\n10.9 Trajectories on an interactive map",
    "text": "10.9 Trajectories on an interactive map\nAn R package called openairmaps has been developed to plot trajectories on interactive leaflet maps. This package can be installed from CRAN, similar to openair.\n\ninstall.packages(\"openairmaps\")\n\nThe function trajMap is the openairmaps equivalent of the trajPlot function. The package comes with some example data (traj_data) that can be used as a template for using other importTraj outputs.\nFirst, load the package.\n\nlibrary(openairmaps)\n\nPlot an interactive map. Try clicking on the individual\ntrajMap(traj_data, colour = \"pm10\")\n\n\nFigure 10.17: Interactive map of trajectories with the default map.\n\n\nThe trajMap function allows for control of many parts of the leaflet map. For example, you can provide one (or more) different base map(s) — there are many! You can also specify a “control” column, which will allow you to show and hide certain trajectory paths. This is nicely showcased using output from the trajCluster function.\n\nclustdata <- trajCluster(traj_data)\n\ntrajMap(\n  data = clustdata$data$traj,\n  colour = \"cluster\",\n  control = \"cluster\",\n  provider = \"CartoDB.Positron\"\n)\n\n\nFigure 10.18: Interactive map of clustered trajectories on a new basemap.\n\n\nopenairmaps also contains an equivalent to trajLevel. trajlevelMap takes many of the same arguments as trajLevel and works with all statistic types, but returns an interactive map. Try hovering over and clicking on the tiles.\ntrajLevelMap(traj_data, statistic = \"frequency\")\n\n\nFigure 10.19: An interactive ‘frequency’ map plotted by trajLevelMap.\n\n\nFor more information about using openairmaps to visualise trajectories, please refer to the Interactive Trajectory Analysis Page."
  },
  {
    "objectID": "time-plot.html#sec-timePlotBack",
    "href": "time-plot.html#sec-timePlotBack",
    "title": "11  Time series plots",
    "section": "\n11.1 Background",
    "text": "11.1 Background\nThe timePlot function is designed to quickly plot time series of data, perhaps for several pollutants or variables. This is, or should be, a very common task in the analysis of air pollution. In doing so, it is helpful to be able to plot several pollutants at the same time (and maybe other variables) and quickly choose the time periods of interest. It will plot time series of type Date and hourly and high time resolution data.\nWith packages such as ggplot2 it is very easy to plot time series. However, there are a few enhancements in timePlot such as flexible time-averaging and adding annotations for wind speed and direction that make it useful in some situations.\nThe function offers fine control over many of the plot settings such as line type, colour and width. If more than one pollutant is selected, then the time series are shown compactly in different panels with different scales. Sometimes it is useful to get and idea of whether different variables ‘go up and down’ together. Such comparisons in timePlot are made easy by setting group =   TRUE, and maybe also normalise = \"mean\". The latter setting divides each variable by its mean value, thus enabling several variables to be plotted together using the same scale. The normalise option will also take a date as a string (in British format dd/mm/YYYY), in which case all data are normalise to equal 100 at that time. Normalising data like this makes it easy to compare time series on different scales e.g. emissions and ambient measurements.\ntimePlot works very well in conjunction with selectByDate, which makes it easy to select specific time series intervals. See Section 25.1 for examples of how to select parts of a data frame based on the date.\nAnother useful feature of timePlot is the ability to average the data in several ways. This makes it easy, for example, to plot daily or monthly means from hourly data, or hourly means from 15-minute data. See the option avg.time for more details and Section 25.5 where a full description of time averaging of data frames is given."
  },
  {
    "objectID": "time-plot.html#examples-of-time-series-plotting",
    "href": "time-plot.html#examples-of-time-series-plotting",
    "title": "11  Time series plots",
    "section": "\n11.2 Examples of time series plotting",
    "text": "11.2 Examples of time series plotting\nA full set of examples is shown in the help pages — see ?timePlot for details. At the basic level, concentrations are shown using a simple call e.g. to plot time series of NOx and O3 in separate panels with their own scales.\n\ntimePlot(mydata, \n         pollutant = c(\"nox\", \"o3\"), \n         y.relation = \"free\")\n\nOften it is necessary to only consider part of a time series and using the openair function selectByDate makes it easy to do this. Some examples are shown below.\nTo plot data only for 2003:\n\ntimePlot(selectByDate(mydata, year = 2003), \n         pollutant = c(\"nox\", \"o3\"), \n         y.relation = \"free\")\n\nPlots for several pollutants for August 2003, are shown in Figure 11.1.\n\nlibrary(openair)\nlibrary(tidyverse)\n\ntimePlot(selectByDate(mydata, year = 2003, month = \"aug\"),\n         pollutant = c(\"nox\", \"o3\", \"pm25\", \"pm10\", \"ws\"),\n         y.relation = \"free\")\n\n\n\nFigure 11.1: Time series for several variables using the timePlot and the selectByDate functions. The data shown are for August 2003.\n\n\n\n\nSome other examples (not plotted) are:\n\n## plot monthly means of ozone and no2\ntimePlot(mydata, pollutant = c(\"o3\", \"no2\"), avg.time = \"month\",\n         y.relation = \"free\")\n\n## plor 95th percentile monthly concentrations\ntimePlot(mydata, pollutant = c(\"o3\", \"no2\"), avg.time = \"month\",\n         statistic = \"percentile\", percentile = 95,\n         y.relation = \"free\")\n\n## plot the number of valid records in each 2-week period\ntimePlot(mydata, pollutant = c(\"o3\", \"no2\"), avg.time = \"2 week\",\n         statistic = \"frequency\", y.relation = \"free\")\n\nAn example of normalising data is shown in Figure 11.2. In this plot we have:\n\nAveraged the data to annual means;\nChosen to normalise to the beginning of 2008;\nSet the line width to 4 and the line type to 1 (continuous line);\nChosen to group the data in one panel.\n\nFigure 11.2 shows that concentrations of NO2 and O3 have increased over the period 1998–2005; SO2 and CO have shown the greatest reductions (by about 60%), whereas NOx concentrations have decreased by about 20%.\n\ntimePlot(mydata, \n         pollutant = c(\"nox\", \"no2\", \"co\", \"so2\", \"pm10\"),\n         avg.time = \"year\", normalise = \"1/1/1998\", \n         lwd = 4, lty = 1,\n         group = TRUE, ylim = c(0, 120))\n\n\n\nFigure 11.2: An example of normalising time series data to fix values to equal 100 at the beginning of 1998\n\n\n\n\nAnother example is grouping pollutants from several sites on one plot. It is easy to import data from several sites and to plot the data in separate panels e.g.\n\n## import data from 3 sites\naq <- importAURN(site = c(\"kc1\", \"my1\", \"nott\"), \n                      year = 2005:2010)\n\n## plot it\ntimePlot(aq, pollutant = \"nox\", \n         type = \"site\", \n         avg.time = \"month\")\n\n\n\nFigure 11.3: Plot of NOx concentrations from three sites in separate panels.\n\n\n\n\nUsing the code above it is also possible to include several species. But what if we wanted to plot NOx concentrations across all sites in one panel? An example of how to do this is shown below. Note, in order to make referring to the columns easier, we will drop the full (long) site name and use the site code instead.\n\n## first drop site name\naq <- select(aq, -site)\n\n## now reshape the data using the tidyr package\naq <- pivot_wider(aq, id_cols = date, \n                       names_from = code, \n                       values_from = co:air_temp)\n\nnames(aq)\n\n [1] \"date\"          \"co_KC1\"        \"co_MY1\"        \"co_NOTT\"      \n [5] \"nox_KC1\"       \"nox_MY1\"       \"nox_NOTT\"      \"no2_KC1\"      \n [9] \"no2_MY1\"       \"no2_NOTT\"      \"no_KC1\"        \"no_MY1\"       \n[13] \"no_NOTT\"       \"o3_KC1\"        \"o3_MY1\"        \"o3_NOTT\"      \n[17] \"so2_KC1\"       \"so2_MY1\"       \"so2_NOTT\"      \"pm10_KC1\"     \n[21] \"pm10_MY1\"      \"pm10_NOTT\"     \"pm2.5_KC1\"     \"pm2.5_MY1\"    \n[25] \"pm2.5_NOTT\"    \"v10_KC1\"       \"v10_MY1\"       \"v10_NOTT\"     \n[29] \"v2.5_KC1\"      \"v2.5_MY1\"      \"v2.5_NOTT\"     \"nv10_KC1\"     \n[33] \"nv10_MY1\"      \"nv10_NOTT\"     \"nv2.5_KC1\"     \"nv2.5_MY1\"    \n[37] \"nv2.5_NOTT\"    \"ws_KC1\"        \"ws_MY1\"        \"ws_NOTT\"      \n[41] \"wd_KC1\"        \"wd_MY1\"        \"wd_NOTT\"       \"air_temp_KC1\" \n[45] \"air_temp_MY1\"  \"air_temp_NOTT\"\n\n\nThe final step will make columns of each site/pollutant combination e.g. nox_KC1, pm10_KC1 and so on. It is then easy to use any of these names to make the plot (with a few plotting option enhancements):\n\ntimePlot(aq, \n         pollutant = c(\"nox_KC1\", \"nox_MY1\", \"nox_NOTT\"),\n         avg.time = \"month\", group = TRUE,\n         lty = 1, lwd = c(1, 3, 5),\n         ylab = \"nox (ug/m3)\"\n         )\n\n\n\nFigure 11.4: Plot of NOx concentrations from three sites grouped in a single plot.\n\n\n\n\nAn alternative way of selecting all columns containing the character ‘nox’ is to use the grep command. For example:\n\ntimePlot(thedata, pollutant = names(thedata)[grep(pattern = \"nox\", \n                                                  names(thedata))],\n         avg.time = \"month\", group = TRUE)\n\nIf wind speed ws and wind direction wd are available they can be used in plots and shown as ‘wind vectors’. Plotting data in this way conveys more information in an easy-to-understand way, which works best for relatively short time periods e.g. a pollution episode lasting a few days. As an example Figure 11.5 shows the first 48 hours of NOx and NO2 data with wind arrows shown. The arrows are controlled by a list of option that control the length, shape and colour of the arrows. The maximum length of the arrow plotted is a fraction of the plot dimension with the longest arrow being scale of the plot x-y dimension. Note, if the plot size is adjusted manually by the user it should be re-plotted to ensure the correct wind angle. The list may contain other options to panel.arrows in the lattice package. Other useful options include length, which controls the length of the arrow head and angle, which controls the angle of the arrow head. Wind vector arrows can also be used with the scatterPlot function.\n\ntimePlot(head(mydata, 48), pollutant = c(\"nox\", \"no2\"), \n         windflow = list(scale = 0.1, lwd = 2, \n                         col = \"turquoise4\"), \n         lwd = 3, group = FALSE, \n         ylab = \"concentration (ug/m3)\")\n\n\n\nFigure 11.5: An example of using the windflow option in timePlot."
  },
  {
    "objectID": "time-variation.html#sec-purpose-time-var",
    "href": "time-variation.html#sec-purpose-time-var",
    "title": "12  Temporal variations",
    "section": "\n12.1 Purpose",
    "text": "12.1 Purpose\nIn air pollution, the variation of a pollutant by time of day and day of week can reveal useful information concerning the likely sources. For example, road vehicle emissions tend to follow very regular patterns both on a daily and weekly basis. By contrast some industrial emissions or pollutants from natural sources (e.g. sea salt aerosol) may well have very different patterns.\nThe timeVariation function produces four plots: day of the week variation, mean hour of day variation and a combined hour of day – day of week plot and a monthly plot. Also shown on the plots is the 95% confidence interval in the mean. These uncertainty limits can be helpful when trying to determine whether one candidate source is different from another. The uncertainty intervals are calculated through bootstrap re-sampling, which will provide better estimates than the application of assumptions based on normality, particularly when there are few data available. The function can consider one or two input variables. In addition, there is the option of ‘normalising’ concentrations (or other quantities). Normalising is very useful for comparing the patterns of two different pollutants, which often cover very different ranges in concentration. Normalising is achieved by dividing the concentration of a pollutant by its mean value. Note also that any other variables besides pollutant concentrations can be considered e.g. meteorological or traffic data.\nThere is also an option difference which is very useful for considering the difference in two time series and how they vary over different temporal resolutions. Again, bootstrap re-sampling methods are used to estimate the uncertainty of the difference in two means.\n\n\n\n\n\n\nNote!\n\n\n\nCare has been taken to ensure that wind direction (wd) is vector-averaged. Less obvious though is the uncertainty in wind direction. A pragmatic approach has been adopted here that considers how wind direction changes. For example, consider the following wind directions: 10, 10, 10, 180, 180, 180\\(^\\circ\\) The standard deviation of these numbers is 93\\(^\\circ\\). However, what actually occurs is the wind direction is constant at 10\\(^\\circ\\) then switches to 180\\(^\\circ\\). In terms of changes there is a sequence of numbers: 0, 0, 170, 0, 0 with a standard deviation of 76\\(^\\circ\\). We use the latter method as a basis of calculating the 95% confidence intervals in the mean.\nThere are also problems with simple averaging—for example, what is the average of 20 and 200\\(^\\circ\\). It can’t be known. In some situations where the wind direction is bi-modal with differences around 180\\(^\\circ\\), the mean can be ‘unstable’. For example, wind that is funnelled along a valley forcing it to be either easterly or westerly. Consider for example the mean of 0\\(^\\circ\\) and 179\\(^\\circ\\) (89.5\\(^\\circ\\)), but a small change in wind direction to 181\\(^\\circ\\) gives a mean of 270.5\\(^\\circ\\). Some care should be exercised therefore when averaging wind direction. It is always a good idea to use thewindRosefunction with type set to ‘month’ or ‘hour’.\n\n\nThe timeVariation function is probably one of the most useful functions that can be used for the analysis of air pollution. Here are a few uses/advantages:\n\nVariations in time are one of the most useful ways of characterising air pollution for a very wide range of pollutants including local urban pollutants and tropospheric background concentrations of ozone and the like.\nThe function works well in conjunction with other functions such as polarPlot (see Chapter 8), where the latter may identify conditions of interest (say a wind speed/direction range). By sub-setting for those conditions in timeVariation the temporal characteristics of a particular source could be characterised and perhaps contrasted with another subset of conditions.\nThe function can be used to compare a wide range of variables, if available. Suggestions include meteorological e.g. boundary layer height and traffic flows.\nThe function can be used for comparing pollutants over different sites.\nThe function can be used to compare one part of a time series with another. This is often a very powerful thing to do, particularly if concentrations are normalised. For example, there is often interest in knowing how diurnal/weekday/seasonal patterns vary with time. If a pollutant showed signs of an increase in recent years, then splitting the data set and comparing each part together can provide information on what is driving the change. Is there, for example, evidence that morning rush hour concentrations have become more important, or Sundays have become relatively more important? An example is given below using the splitByDate function.\ntimeVariation can be used to consider the differences between two time series, which will have multiple benefits. For example, for model evaluation it can be very revealing to consider the difference between observations and modelled values over different time scales. Considering such differences can help reveal the character and some reasons for why a model departs from reality."
  },
  {
    "objectID": "time-variation.html#applications",
    "href": "time-variation.html#applications",
    "title": "12  Temporal variations",
    "section": "\n12.2 Applications",
    "text": "12.2 Applications\nWe apply the timeVariation function to PM10 concentrations and take the opportunity to filter the data to maximise the signal from the road. The polarPlot function described in Section (Chapter 8) is very useful in this respect in highlighting the conditions under which different sources have their greatest impact. A subset of data is used filtering for wind speeds > 3 m s-1 and wind directions from 100–270 degrees. The code used is:\nThe results are shown in Figure 12.1. The plot shown at the top-left shows the diurnal variation of concentrations for all days. It shows for example that PM10 concentrations tend to peak around 9 am. The shading shows the 95% confidence intervals of the mean. The plot at the top-right shows how PM10 concentrations vary by day of the week. Here there is strong evidence that PM10 is much lower at the weekends and that there is a significant difference compared with weekdays. It also shows that concentrations tend to increase during the weekdays. Finally, the plot at the bottom shows both sets of information together to provide an overview of how concentrations vary.\nNote that the plot need not just consider pollutant concentrations. Other useful variables (if available) are meteorological and traffic flow or speed data. Often, the combination of several sets of data can be very revealing.\nThe filter function is extremely useful in this respect. For example, if it were believed that a source had an effect under specific conditions; they can be isolated with the filter function. It is also useful if it is suspected that two or more sources are important that they can be isolated to some degree and compared. This is where the uncertainty intervals help — they provide an indication whether the behaviour of one source differs significantly from another.\n\nlibrary(openair)\nlibrary(tidyverse)\n\ntimeVariation(filter(mydata, ws > 3,  wd > 100, wd < 270),\n              pollutant = \"pm10\", ylab = \"pm10 (ug/m3)\")\n\n\n\nFigure 12.1: Example plot using the timeVariation function to plot PM10 concentrations at Marylebone Road.\n\n\n\n\nFigure 12.2 shows the function applied to concentrations of NOx, CO, NO2 and O3 concentrations. In this case the concentrations have been normalised. The plot clearly shows the markedly different temporal trends in concentration. For CO, there is a very pronounced increase in concentrations during the peak pm rush hour. The other important difference is on Sundays when CO concentrations are relatively much higher than NOx. This is because flows of cars (mostly petrol) do not change that much by day of the week, but flows of vans and HGVs (diesel vehicles) are much less on Sundays. Note, however, that the monthly trend is very similar in each case — which indicates very similar source origins. Taken together, the plots highlight that traffic emissions dominate this site for CO and NOx, but there are important difference in how these emissions vary by hour of day and day of week.\nAlso shown in the very different behaviour of O3. Because O3 reacts with NO, concentrations of NOx and O3 tend to be anti-correlated. Note also the clear peak in O3 in April/May, which is due to higher northern hemispheric background concentrations in the spring. Even at a busy roadside site in central London this influence is clear to see.\n\ntimeVariation(mydata, \n              pollutant = c(\"nox\", \"co\", \"no2\", \"o3\"), \n              normalise = TRUE)\n\n\n\nFigure 12.2: Example plot using the timeVariation function to plot NOx, CO, NO2 and O3 concentrations at Marylebone Road. In this plot, the concentrations are normalised.\n\n\n\n\nAnother example is splitting the data set by time. We use the splitByDate function to divide up the data into dates before January 2003 and after January 2003. This time the option difference is used to highlight how NO2 concentrations have changed over these two periods. The results are shown in Figure 12.3. There is some indication in this plot that data after 2003 seem to show more of a double peak in the diurnal plots; particularly in the morning rush hour. Also, the difference line does more clearly highlight a more substantial change over weekdays and weekends. Given that cars are approximately constant at this site each day, the change may indicate a change in vehicle emissions from other vehicle types. Given that it is known that primary NO2 emissions are known to have increased sharply from the beginning of 2003 onwards, this perhaps provides clues as to the principal cause.\n\n## split data into two periods (see Utlities section for more details)\nmydata <- splitByDate(mydata, dates= \"1/1/2003\",\n                        labels = c(\"before Jan. 2003\", \"After Jan. 2003\"))\n\ntimeVariation(mydata, pollutant = \"no2\", \n              group = \"split.by\", \n              difference = TRUE)\n\n\n\nFigure 12.3: Example plot using the timeVariation function to plot NO2 concentrations at Marylebone Road. In this plot, the concentrations are shown before and after January 2003.\n\n\n\n\nIn the next example it is shown how to compare one subset of data of interest with another. Again, there can be many reasons for wanting to do this and perhaps the data set at Marylebone Road is not the most interesting to consider. Nevertheless, the code below shows how to approach such a problem. The scenario would be that one is interested in a specific set of conditions and it would be useful to compare that set, with another set. A good example would be from an analysis using the polarPlot function where a ‘feature’ of interest has been identified—maybe an indication of a different source. But does this potentially different source behave differently in terms of temporal variation? If it does, then maybe that provides evidence to support that it is a different source. In a wider context, this approach could be used in many different ways depending on available data. A good example is the analysis of model output where many diagnostic meteorological data are available. This is an area that will be developed.\nThe approach here is to first make a new variable called ‘feature’ and fill it with the value ‘other’. A subset of data is defined and the associated locations in the data frame identified. The subset of data is then used to update the `feature’ field with a new description. This approach could be extended to some quite complex situations.\nThere are a couple of things to note in Figure 12.2. There seems to be evidence that for easterly winds > 4 m s-1 that concentrations of SO2 are lower at night. Also, there is some evidence that concentrations for these conditions are also lower at weekends. This might reflect that SO2 concentrations for these conditions tend to be dominated by tall stack emissions that have different activities to road transport sources. This technique will be returned to with different data sets in future.\n\n## make a field called \"feature\" and fill: make all values = \"other\"\nmydata <- mutate(mydata, \n                 feature = ifelse(ws > 4 & wd > 0 & wd <= 180, \"easterly\", \"other\"))\n\ntimeVariation(mydata, pollutant =\"so2\", group = \"feature\",  ylab = \"so2 (ppb)\",\n              difference = TRUE)\n\n\n\nFigure 12.4: Example plot using the timeVariation function to plot SO2 concentrations at Marylebone Road. In this plot, the concentrations are shown for a subset of easterly conditions and everything else. Note that the uncertainty in the mean values for easterly winds is greater than other. This is mostly because the sample size is much lower for easterly compared with other.\n\n\n\n\nBy default timeVariation shows the mean variation in different temporal components and the 95% confidence interval in the mean. However, it is also possible to show how the data are distributed by using a different option for statistic. When statistic = \"median\" the median line is shown together with the 25/75th and 5/95th quantile values. Users can control the quantile values shown be setting the conf.int. For example, conf.int =   c(0.25, 0.99) will show the median, 25/75th and 1/99th quantile values. The statistic = \"median\" option is therefore very useful for showing how the data are distributed — somewhat similar to a box and whisker plot. Note that it is expected that only one pollutant should be shown when statistic = \"median\" is used due to potential over-plotting; although the function will display several species of required. An example is shown in Figure 12.5 for PM10 concentrations.\n\ntimeVariation(mydata, pollutant = \"pm10\", \n              statistic = \"median\",\n              col = \"firebrick\")\n\n\n\nFigure 12.5: Example plot using the timeVariation function to show the variation in the median, 25/75th and 5/95th quantile values for PM10. The shading shows the extent to the 25/75th and 5/95th quantiles."
  },
  {
    "objectID": "time-variation.html#output",
    "href": "time-variation.html#output",
    "title": "12  Temporal variations",
    "section": "\n12.3 Output",
    "text": "12.3 Output\nThe timeVariation function produces several outputs that can be used for further analysis or plotting. It is necessary to read the output into a variable for further processing. The code below shows the different objects that are returned and the code shows how to access them.\n\nmyOutput <- timeVariation(mydata, pollutant = \"so2\")\n## show the first part of the day/hour variation\n## note that value = mean, and Upper/Lower the 95% confid. intervals\nhead(myOutput$data$day.hour)\n\n# A tibble: 6 × 8\n# Groups:   ci [1]\n  variable wkday      hour default                        Mean Lower Upper    ci\n  <fct>    <ord>     <int> <fct>                         <dbl> <dbl> <dbl> <dbl>\n1 so2      Monday        0 01 January 1998 to 23 June 2…  2.93  2.69  3.16  0.95\n2 so2      Tuesday       0 01 January 1998 to 23 June 2…  3.21  3.01  3.44  0.95\n3 so2      Wednesday     0 01 January 1998 to 23 June 2…  3.35  3.07  3.66  0.95\n4 so2      Thursday      0 01 January 1998 to 23 June 2…  3.22  2.87  3.56  0.95\n5 so2      Friday        0 01 January 1998 to 23 June 2…  3.64  3.38  3.96  0.95\n6 so2      Saturday      0 01 January 1998 to 23 June 2…  4.25  4.04  4.52  0.95\n\n\n\n## can make a new data frame of this data e.g.\nday.hour <- myOutput$data$day.hour\nhead(day.hour)\n\n# A tibble: 6 × 8\n# Groups:   ci [1]\n  variable wkday      hour default                        Mean Lower Upper    ci\n  <fct>    <ord>     <int> <fct>                         <dbl> <dbl> <dbl> <dbl>\n1 so2      Monday        0 01 January 1998 to 23 June 2…  2.93  2.69  3.16  0.95\n2 so2      Tuesday       0 01 January 1998 to 23 June 2…  3.21  3.01  3.44  0.95\n3 so2      Wednesday     0 01 January 1998 to 23 June 2…  3.35  3.07  3.66  0.95\n4 so2      Thursday      0 01 January 1998 to 23 June 2…  3.22  2.87  3.56  0.95\n5 so2      Friday        0 01 January 1998 to 23 June 2…  3.64  3.38  3.96  0.95\n6 so2      Saturday      0 01 January 1998 to 23 June 2…  4.25  4.04  4.52  0.95\n\n\nAll the numerical results are given by:\n\nmyOutput$data$day.hour ## are the weekday and hour results\nmyOutput$data$hour ## are the diurnal results\nmyOutput$data$day ## are the weekday results\nmyOutput$data$month ## are the monthly results\n\nIt is also possible to plot the individual plots that make up the (four) plots produced by timeVariation:\n\n## just the diurnal variation\nplot(myOutput, subset = \"hour\")\n## day and hour\nplot(myOutput, subset = \"day.hour\")\n## weekday variation\nplot(myOutput, subset = \"day\")\n## monthly variation\nplot(myOutput, subset = \"month\")"
  },
  {
    "objectID": "time-proportion.html#sec-timePropBack",
    "href": "time-proportion.html#sec-timePropBack",
    "title": "13  Time proportion plots",
    "section": "\n13.1 Background",
    "text": "13.1 Background\nThe timeProp (‘time proportion’) function shows time series plots as stacked bar charts. For a particular time, proportions of a chosen variable are shown as a stacked bar chart. The different categories in the bar chart are made up from a character or factor variable in a data frame. The function is primarily developed to support the plotting of cluster analysis output from polarCluster (see Section 8.8) and trajCluster (see Section 10.8) that consider local and regional (back trajectory) cluster analysis respectively. However, the function has more general use for understanding time series data. In order to plot time series in this way, some sort of time aggregation is needed, which is controlled by the option avg.time.\nThe plot shows the value of pollutant on the y-axis (averaged according to avg.time). The time intervals are made up of bars split according to proportion. The bars therefore show how the total value of `pollutant} is made up for any time interval."
  },
  {
    "objectID": "time-proportion.html#sec-timePropEx",
    "href": "time-proportion.html#sec-timePropEx",
    "title": "13  Time proportion plots",
    "section": "\n13.2 Examples",
    "text": "13.2 Examples\nAn example of the timeProp function is shown in Figure 13.1. In this example SO2 concentrations are considered for 2003 (using the selectByDate function). The averaging period is set to 3 days and the mean concentration is plotted and the proportion contribution by wind sector is given. Other options are chosen to place the key at the top and choose the number of columns used in the key. It is apparent from Figure 13.1) that the highest SO2 concentrations are dominated by winds from an easterly sector, but actually occur throughout the year.\n\nlibrary(openair) # load the package\n\ntimeProp(selectByDate(mydata, year = 2003),\n         pollutant = \"so2\", avg.time = \"3 day\",\n         proportion = \"wd\", \n         date.breaks = 10, key.position = \"top\",\n         key.columns = 8, ylab = \"so2 (ug/m3)\")\n\n\n\nFigure 13.1: timeProp plot for SO2 concentrations in 2003. The data are categorised into 8 wind sectors for 3-day averages.\n\n\n\n\nNote that proportion can be an existing categorical (i.e. factor or character) variable in a data frame. If a numeric variable is supplied, then it is typically cut into four quantile levels. So, for example, the plot below would show four intervals of wind speed, which would help show the wind speed conditions that control high SO2 concentration — and importantly, when they occur.\nAn example of using timeProp with a continuous variable is shown in Figure 13.2. In this case the wind speed values are split into 3 quantile levels. The number of quantiles used is determined by the option n.levels. This approach can be used for any numeric variables.\n\ntimeProp(selectByDate(mydata, year = 2003),\n         pollutant = \"so2\",\n         avg.time = \"3 day\",\n         n.levels = 3,\n         cols = \"viridis\",\n         proportion = \"ws\", date.breaks = 10,\n         key.position = \"top\", key.columns = 3)\n\n\n\nFigure 13.2: timeProp plot for SO2 concentrations in 2003. The data are categorised into 4 wind speed categories for 3-day averages.\n\n\n\n\nOne of the key uses of timeProp is to post-process cluster analysis data. Users should consider the uses of timeProp for cluster analysis shown in Section 8.8 and Section 10.8. In both these cases the cluster analysis yields a categorical output directly i.e. cluster, which lends itself to analysis using timeProp."
  },
  {
    "objectID": "trend-level.html#another-way-of-representing-trends",
    "href": "trend-level.html#another-way-of-representing-trends",
    "title": "14  Trend heat maps",
    "section": "\n14.1 Another way of representing trends",
    "text": "14.1 Another way of representing trends\nThe trendLevel function provides a way of rapidly showing a large amount of data in a condensed way. It is particularly useful for plotting the level of a value against two categorical variables. These categorical variables can pre-exist in a data set or be made on the fly using openair. By default it will show the mean value of a variable against two categorical variables but can also consider a wider range of statistics e.g. the maximum, frequency, or indeed a user-defined function. The function is much more flexible than this by showing temporal data and can plot ‘heat maps’ in many flexible ways. Both continuous colour scales and user-defined categorical scales can be used.\nThe trendLevel function shows how the value of a variable varies according to intervals of two other variables. The \\(x\\) and \\(y\\) variables can be categorical (factor or character) or numeric. The third variable (\\(z\\)) must be numeric and is coloured according to its value. Despite being called trendLevel the function is flexible enough to consider a wide range of plotting variables.\nIf the \\(x\\) and \\(y\\) variables are not categorical they are made so by splitting the data into quantiles (using cutData). Furthermore, the user can supply as many levels as they wish for the quantile using the option n.levels. Remember also there are lots of built-in options for x or y based on temporal variations (see Section 25.2) e.g. “month” (the default), “week”, “daylight” and so on."
  },
  {
    "objectID": "trend-level.html#sec-TrendLevelEx",
    "href": "trend-level.html#sec-TrendLevelEx",
    "title": "14  Trend heat maps",
    "section": "\n14.2 Examples",
    "text": "14.2 Examples\nThe standard output from trendLevel is shown in Figure 14.1, which shows the variation in NOx concentrations by year and hour of the day. By default the function will use “month” for the x-axis and “hour” for the y-axis.\n\nlibrary(openair)\nlibrary(tidyverse)\n\ntrendLevel(mydata, pollutant = \"nox\")\n\n\n\nFigure 14.1: Example output from trendLevel.\n\n\n\n\n\ntrendLevel(mydata, pollutant = \"nox\", y = \"wd\", \n           border = \"white\", \n           cols = \"turbo\")\n\n\n\nFigure 14.2: trendLevel output with wind direction as y.\n\n\n\n\nFigure 14.3 indicates that the highest NOx concentrations most strongly associate with wind sectors about 200 degrees, appear to be decreasing over the years, but do not appear to associate with an SO2 rich NOx source. Using type = \"so2\" would have conditioned by absolute SO2 concentration. As both a moderate contribution from an SO2 rich source and a high contribution from an SO2 poor source could generate similar SO2 concentrations, such conditioning can sometimes blur interpretations. The use of this type of ‘over pollutant’ ratio reduces this blurring by focusing conditioning on cases when NOx concentrations (be they high or low) associate with relatively high or low SO2 concentrations.\n\n## new field: so2/nox ratio\nmydata <- mutate(mydata, ratio =  so2 / nox)\n\n## condition by mydata$new\ntrendLevel(mydata, \"nox\", x = \"year\", y = \"wd\", \n           type = \"ratio\",\n           cols = \"inferno\")\n\n\n\nFigure 14.3: trendLevel output with SO2 : NOx ratio type conditioning.\n\n\n\n\nThe plot can be used in much more flexible ways. Here are some examples (not plotted):\nA plot of mean O3 concentration shown by season and by daylight/nighttime hours.\n\ntrendLevel(mydata, x = \"season\", y = \"daylight\", pollutant = \"o3\")\n\nOr by season and hour of the day:\n\n    trendLevel(mydata, x = \"season\", y = \"hour\", \n               pollutant = \"o3\",\n               cols = \"increment\")\n\nHow about NOx versus NO2 coloured by the concentration of O3? scatterPlot could also be used to produce such a plot. However, one interesting difference with using trendLevel is that the data are split into quantiles where equal numbers of data exist in each interval. This approach can make it a bit easier to see the underlying relationship between variables. A scatter plot may have too much data to be clear and also outliers (or regions with relatively few data) that make it harder to see what is going on. The plot generated by the command below makes it a bit easier to see that it is the higher quantiles of NO2 that are associated with higher O3 concentration (as well as low NOx and NO2 concentrations).\n\ntrendLevel(mydata, x = \"nox\", y = \"no2\", pollutant = \"o3\", \n           border = \"white\",\n           n.levels = 30, statistic = \"max\", \n           limits = c(0, 50))\n\n\n\nFigure 14.4: trendLevel showing NOx against NO2, coloured by the concentration of O3.\n\n\n\n\nThe plot can also be shown by wind direction sector, this time showing how O3 varies by weekday, wind direction sector and NOx quantile.\n\ntrendLevel(mydata, x = \"nox\", y = \"weekday\", pollutant = \"o3\",\n           border = \"white\", n.levels = 10, statistic = \"max\",\n           limits = c(0, 50), type = \"wd\")\n\nBy default trendLevel subsamples the plotted pollutant data by the supplied x, y and type parameters and in each case calculates the mean. The option statistic has always let you apply other statistics. For example, trendLevel also calculated the maximum via the option statistic = \"max\". The user may also use their own statistic function.\nAs a simple example, consider the above plot which summarises by mean. This tells us about average concentrations. It might also be useful to consider a particular percentile of concentrations. This can be done by defining one’s own function as shown in Figure 14.5.\n\n## function to estimate 95th percentile\npercentile <- function(x) quantile(x, probs = 0.95, na.rm = TRUE)\n\n## apply to present plot\ntrendLevel(mydata, \"nox\", x = \"year\", y = \"wd\", \n           type = \"ratio\",\n           cols = \"viridis\",\n           statistic = percentile)\n\n\n\nFigure 14.5: trendLevel using locally defined statistic.\n\n\n\n\nThis type of flexibility really opens up the potential of the function as a screening tool for the early stages of data analysis. Increased control of x, y, type and statistic allow you to very quick explore your data and develop an understanding of how different parameters interact. Patterns in trendLevel plots can also help to direct your openair analysis. For example, possible trends in data conditioned by year would suggest that functions like smoothTrend or TheilSen could provide further insight. Likewise, windRose or polarPlot could be useful next steps if wind speed and direct conditioning produces interesting features. However, perhaps most interestingly, novel conditioning or the incorporation of novel parameters in this type of highly flexible function provides a means of developing new data visualisation and analysis methods.\ntrendLevel can also be used with user defined discrete colour scales as shown in Figure 14.6. In this case the default \\(x\\) and \\(y\\) variables are chosen (month and hour) split by type (year).\n\ntrendLevel(mydata, pollutant = \"no2\",\n           x = \"week\",\n           border = \"white\",  statistic = \"max\",\n           breaks = c(0, 50, 100, 500),\n           labels = c(\"low\", \"medium\", \"high\"),\n           cols = c(\"forestgreen\", \"yellow\", \"red\"),\n           key.position = \"top\")\n\n\n\nFigure 14.6: trendLevel plot for maximum NO2 concentrations using a user-defined discrete colour scale."
  },
  {
    "objectID": "calendar-plot.html#purpose",
    "href": "calendar-plot.html#purpose",
    "title": "15  Calendar plots",
    "section": "\n15.1 Purpose",
    "text": "15.1 Purpose\nSometimes it is useful to visualise data in a familiar way. Calendars are the obvious way to represent data for data on the time scale of days or months. The calendarPlot function provides an effective way to visualise data in this way by showing daily concentrations laid out in a calendar format. The concentration of a species is shown by its colour. The data can be shown in different ways. By default, calendarPlot overlays the day of the month. However, if wind speed and wind direction are available then an arrow can be shown for each day giving the vector-averaged wind direction. In addition, the arrow can be scaled according to the wind speed to highlight both the direction and strength of the wind on a particular day, which can help show the influence of meteorology on pollutant concentrations.\ncalendarPlot can also show the daily mean concentration as a number on each day and can be extended to highlight those conditions where daily mean (or maximum etc.) concentrations are above a particular threshold. This approach is useful for highlighting daily air quality limits e.g. when the daily mean concentration is greater than 50 μg m-3.\nThe calendarPlot function can also be used to plot categorical scales. This is useful for plotting concentrations expressed as an air quality index i.e. intervals of concentrations that are expressed in ways like ‘very good’, ‘good’, ‘poor’ and so on."
  },
  {
    "objectID": "calendar-plot.html#calendar-examples",
    "href": "calendar-plot.html#calendar-examples",
    "title": "15  Calendar plots",
    "section": "\n15.2 Calendar examples",
    "text": "15.2 Calendar examples\nThe function is called in the usual way. As a minimum, a data frame, pollutant and year is required. So to show O3 concentrations for each day in 2003 (Figure 15.1). Note that if year is not supplied the full data set will be used.\n\nlibrary(openair)\ncalendarPlot(mydata, pollutant = \"o3\", year = 2003)\n\n\n\nFigure 15.1: calendarPlot for O3 concentrations in 2003.\n\n\n\n\nIt is sometimes useful to annotate the plots with other information. It is possible to show the daily mean wind angle, which can also be scaled to wind speed. The idea here being to provide some information on meteorological conditions on each day. Another useful option is to set annotate = \"value\" in which case the daily concentration will be shown on each day. Furthermore, it is sometimes useful to highlight particular values more clearly. For example, to highlight daily mean PM10 concentrations above 50 μg m-3. This is where setting lim (a concentration limit) is useful. In setting lim the user can then differentiate the values below and above lim by colour of text, size of text and type of text e.g. plain and bold.\nFigure 15.2 highlights those days where PM10 concentrations exceed 50 μg m-3 by making the annotation for those days bigger, bold and orange. Plotting the data in this way clearly shows the days where PM10 > 50 μg m-3.\nOther openair functions can be used to plot other statistics. For example, rollingMean could be used to calculate rolling 8-hour mean O3 concentrations. Then, calendarPlot could be used with statistic = \"max\" to show days where the maximum daily rolling 8-hour mean O3 concentration is greater than a certain threshold e.g. 100 or 120 μg m-3.\ncalendarPlot for PM10 concentrations in 2003 with annotations highlighting those days where the concentration of PM10 >50 μg m-3. The numbers show the PM10 concentration in μg m-3.\n\ncalendarPlot(mydata,\n  pollutant = \"pm10\", year = 2003,\n  annotate = \"value\",\n  lim = 50,\n  cols = \"Purples\",\n  col.lim = c(\"black\", \"orange\"),\n  layout = c(4, 3)\n)\n\n\n\nFigure 15.2: calendarPlot for PM10 concentrations in 2003 with annotations highlighting those days where the concentration of PM10 >50 μg m-3. The numbers show the PM10 concentration in μg m-3.\n\n\n\n\nTo show wind angle, scaled to wind speed (Figure 15.3).\n\ncalendarPlot(mydata,\n  pollutant = \"o3\", year = 2003,\n  annotate = \"ws\"\n)\n\n\n\nFigure 15.3: calendarPlot for O3 concentrations in 2003 with annotations showing wind angle scaled to wind speed i.e. the longer the arrow, the higher the wind speed. It shows for example high O3 concentrations on the 17 and 18th of April were associated with strong north-easterly winds.\n\n\n\n\nNote again that selectByDate can be useful. For example, to plot select months:\n\ncalendarPlot(selectByDate(mydata,\n  year = 2003,\n  month = c(\"jun\", \"jul\", \"aug\")\n),\npollutant = \"o3\", year = 2003\n)\n\nFigure 15.4 shows an example of plotting data with a categorical scale. In this case the options labels and breaks have been used to define concentration intervals and their descriptions. Note that breaks needs to be one longer than labels. In the example in Figure 15.4 the first interval (‘Very low’) is defined as concentrations from 0 to 50 (ppb), ‘Low’ is 50 to 100 and so on. Note that the upper value of breaks should be a number greater than the maximum value contained in the data to ensure that it is encompassed. In the example given in Figure 15.4 the maximum daily concentration is plotted. These types of plots are very useful for considering national or international air quality indexes.\n\ncalendarPlot(mydata,\n  pollutant = \"no2\", year = 2003,\n  breaks = c(0, 50, 100, 150, 1000),\n  labels = c(\"Very low\", \"Low\", \"High\", \"Very High\"),\n  cols = \"increment\", statistic = \"max\"\n)\n\n\n\nFigure 15.4: calendarPlot for NO2 concentrations in 2003 with a user-defined categorical scale.\n\n\n\n\nThe user can explicitly set each colour interval:\n\ncalendarPlot(mydata,\n  pollutant = \"no2\", year = 2003,\n  breaks = c(0, 50, 100, 150, 1000),\n  labels = c(\"Very low\", \"Low\", \"High\", \"Very High\"),\n  cols = c(\"lightblue\", \"forestgreen\", \"yellow\", \"red\"),\n  statistic = \"max\"\n)\n\nNote that in the case of categorical scales it is possible to define the breaks and labels first and then make the plot. For example:\n\nbreaks <- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500)\nlabels <- c(\n  \"Low.1\", \"Low.2\", \"Low.3\", \"Moderate.4\", \"Moderate.5\", \"Moderate.6\",\n  \"High.7\", \"High.8\", \"High.9\", \"Very High.10\"\n)\n\ncalendarPlot(mydata,\n  pollutant = \"no2\", year = 2003,\n  breaks = breaks, labels = labels,\n  cols = \"turbo\", statistic = \"max\"\n)\n\nIt is also possible to first use rollingMean to calculate statistics. For example, if one was interested in plotting the maximum daily rolling 8-hour mean concentration, the data could be prepared and plotted as follows.\n\n## makes a new field 'rolling8o3'\ndat <- rollingMean(mydata, pollutant = \"o3\", hours = 8)\nbreaks <- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500)\nlabels <- c(\n  \"Low.1\", \"Low.2\", \"Low.3\", \"Moderate.4\", \"Moderate.5\", \"Moderate.6\",\n  \"High.7\", \"High.8\", \"High.9\", \"Very High.10\"\n)\n\ncalendarPlot(dat,\n  pollutant = \"rolling8o3\", year = 2003,\n  breaks = breaks, labels = labels,\n  cols = \"turbo\", statistic = \"max\"\n)\n\nThe UK has an air quality index for O3, NO2, PM10 and described in detail at (http://uk-air.defra.gov.uk/air-pollution/daqi) and COMEAP (2011). The index is most relevant to air quality forecasting, but is used widely for public information. Most other countries have similar indexes. Note that the indexes are calculated for different averaging times dependent on the pollutant: rolling 8-hour mean for O3, hourly means for NO2 and a fixed 24-hour mean for PM10 and PM2.5.\n\nCOMEAP. 2011. “Review of the UK Air Quality Index: A Report by the Committee on the Medical Effects of Air Pollutants.” http://comeap.org.uk/documents/reports/130-review-of-the-uk-air-quality-index.html.\nIn the code below the labels and breaks are defined for each pollutant to make it easier to use the index in the calendarPlot function.\n\n## the labels - same for all species\nlabels <- c(\n  \"1 - Low\", \"2 - Low\", \"3 - Low\", \"4 - Moderate\", \"5 - Moderate\",\n  \"6 - Moderate\", \"7 - High\", \"8 - High\", \"9 - High\", \"10 - Very High\"\n)\no3.breaks <- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500)\nno2.breaks <- c(0, 67, 134, 200, 268, 335, 400, 468, 535, 600, 1000)\npm10.breaks <- c(0, 17, 34, 50, 59, 67, 75, 84, 92, 100, 1000)\npm25.breaks <- c(0, 12, 24, 35, 42, 47, 53, 59, 65, 70, 1000)\n\nRemember it is necessary to use the correct averaging time. Assuming data are imported using importAURN or similar, then the units will be in μg m-3 — if not the user should ensure this is the case. Note that rather than showing the day of the month (the default), annotate = \"value\" can be used to show the actual numeric value on each day. In this way, the colours represent the categorical interval the concentration on a day corresponds to and the actual value itself is shown.\n\n## import test data\ndat <- importAURN(site = \"kc1\", year = 2010)\n\n## no2 index example\ncalendarPlot(dat,\n  year = 2010, pollutant = \"no2\", labels = labels,\n  breaks = no2.breaks, statistic = \"max\", cols = \"turbo\"\n)\n\n## for PM10 or PM2.5 we need the daily mean concentration\ncalendarPlot(dat,\n  year = 2010, pollutant = \"pm10\", labels = labels,\n  breaks = pm10.breaks, statistic = \"mean\", cols = \"turbo\"\n)\n\n## for ozone, need the rolling 8-hour mean\ndat <- rollingMean(dat, pollutant = \"o3\", hours = 8)\ncalendarPlot(dat,\n  year = 2010, pollutant = \"rolling8o3\", labels = labels,\n  breaks = o3.breaks, statistic = \"max\", cols = \"turbo\"\n)"
  },
  {
    "objectID": "theil-sen.html#trend-estimates",
    "href": "theil-sen.html#trend-estimates",
    "title": "16  Theil-Sen trends",
    "section": "\n16.1 Trend estimates",
    "text": "16.1 Trend estimates\nCalculating trends for air pollutants is one of the most important and common tasks that can be undertaken. Trends are calculated for all sorts of reasons. Sometimes it is useful to have a general idea about how concentrations might have changed. On other occasions a more definitive analysis is required; for example, to establish statistically whether a trend is significant or not. The whole area of trend calculation is a complex one and frequently trends are calculated with little consideration as to their validity. Perhaps the most common approach is to apply linear regression and not think twice about it. However, there can be many pitfalls when using ordinary linear regression, such as the assumption of normality, autocorrelation etc.\nOne commonly used approach for trend calculation in studies of air pollution is the non-parametric Mann-Kendall approach (Hirsch, Slack, and Smith 1982). Wilcox (2010) provides an excellent case for using ‘modern methods’ for regression including the benefits of non-parametric approaches and bootstrap simulations. Note also that the all the regression parameters are estimated through bootstrap resampling.\n\nHirsch, R. M., J. R. Slack, and R. A. Smith. 1982. “Techniques of Trend Analysis for Monthly Water-Quality Data.” Water Resources Research 18 (1): 107–21.\n\nWilcox, Rand R. 2010. Fundamentals of Modern Statistical Methods: Substantially Improving Power and Accuracy. 2nd ed. Springer New York. http://www.springerlink.com/content/978-1-4419-5524-1.\n\nTheil, H. 1950. “A Rank Invariant Method of Linear and Polynomial Regression Analysis, i, II, III.” Proceedings of the Koninklijke Nederlandse Akademie Wetenschappen, Series A – Mathematical Sciences 53: 386–92, 521–25, 1397–1412.\n\nSen, P. K. 1968. “Estimates of Regression Coefficient Based on Kendall’s Tau.” Journal of the American Statistical Association 63(324): 1379–89.\nThe Theil-Sen method dates back to 1950, but the basic idea pre-dates 1950 (Theil 1950; Sen 1968). It is one of those methods that required the invention of fast computers to be practical. The basic idea is as follows. Given a set of \\(n\\) \\(x\\), \\(y\\) pairs, the slopes between all pairs of points are calculated. Note, the number of slopes can increase by \\(\\approx\\) \\(n^2\\) so that the number of slopes can increase rapidly as the length of the data set increases. The Theil-Sen estimate of the slope is the median of all these slopes. The advantage of the using the Theil-Sen estimator is that it tends to yield accurate confidence intervals even with non-normal data and heteroscedasticity (non-constant error variance). It is also resistant to outliers — both characteristics can be important in air pollution. As previously mentioned, the estimates of these parameters can be made more robust through bootstrap-resampling, which further adds to the computational burden, but is not an issue for most time series which are expressed either as monthly or annual means. Bootstrap resampling also provides the estimate of \\(p\\) for the slope.\nAn issue that can be very important for time series is dependence or autocorrelation in the data. Normal (in the statistical sense) statistics assume that data are independent, but in time series this is rarely the case. The issue is that neighbouring data points are similar to one another (correlated) and therefore not independent. Ignoring this dependence would tend to give an overly optimistic impression of uncertainties. However, taking account of it is far from simple. A discussion of these issues is beyond the aims of this report and readers are referred to standard statistical texts on the issue. In openair we follow the suggestion of Kunsch (1989) of setting the block length to \\(n^{1/3}\\) where n is the length of the time series.\n\nKunsch, H. R. 1989. “The Jackknife and the Bootstrap for General Stationary Observations.” Annals of Statistics 17 (3): 1217–41.\nThere is a temptation when considering trends to use all the available data. Why? Often it is useful to consider specific periods. For example, is there any evidence that concentrations of NOx have decreased since 2000? Clearly, the time period used depends on both the data and the questions, but it is good to be aware that considering subsets of data can be very insightful.\nAnother aspect is that almost all trends are shown as mean concentration versus time; typically by year. Such analyses are very useful for understanding how concentrations have changed through time and for comparison with air quality limits and regulations. However, if one is interested in understanding why trends are as they are, it can be helpful to consider how concentrations vary in other ways. The trend functions in openair do just this. Trends can be plotted by day of the week, month, hour of the day, by wind direction sector and by different wind speed ranges. All these capabilities are easy to use and their effectiveness will depend on the situation in question. One of the reasons that trends are not considered in these many different ways is that there can be a considerable overhead in carrying out the analysis, which is avoided by using these functions. Few, for example, would consider a detailed trend analysis by hour of the day, ensuring that robust statistical methods were used and uncertainties calculated. However, it can be useful to consider how concentrations vary in this way. It may be, for example, that the hours around midday are dominated by heavy vehicle emissions rather than by cars — so is the trend for a pollutant different for those hours compared with say, hours dominated by other vehicle types? Similarly, a much more focussed trend analysis can be done by considering different wind direction, as this can help isolate different source influences.\nThe TheilSen function is typically used to determine trends in pollutant concentrations over several years. However, it can be used to calculate the trend in any numeric variable. It calculates monthly mean values from daily, hourly or higher time resolution data, as well as working directly with monthly means. Whether it is meaningful to calculate trends over shorter periods of time (e.g. 2 years) depends very much on the data. It may well be that statistically significant trends can be detected over relatively short periods but it is another matter whether it matters. Because seasonal effects can be important for monthly data, there is the option to deseasonalise the data first. The timeVariation function are both useful to determine whether there is a seasonal cycle that should be removed.\nNote also that the symbols shown next to each trend estimate relate to how statistically significant the trend estimate is: \\(p\\) \\(<\\) 0.001 = \\(\\ast\\ast\\ast\\), \\(p\\) \\(<\\) 0.01 = \\(\\ast\\ast\\), \\(p\\) \\(<\\) 0.05 = \\(\\ast\\) and \\(p\\) \\(<\\) 0.1 = \\(+\\)."
  },
  {
    "objectID": "theil-sen.html#example-trend-analysis",
    "href": "theil-sen.html#example-trend-analysis",
    "title": "16  Theil-Sen trends",
    "section": "\n16.2 Example trend analysis",
    "text": "16.2 Example trend analysis\nWe first show the use of the TheilSen function by applying it to concentrations of O3. The function is called as shown in Figure 16.1.\nThe plot shows the deseasonalised monthly mean concentrations of O3. The solid red line shows the trend estimate and the dashed red lines show the 95% confidence intervals for the trend based on resampling methods. The overall trend is shown at the top-left as 0.38 (ppb) per year and the 95% confidence intervals in the slope from 0.21–0.51 ppb/year. The \\(\\ast\\ast\\ast\\) show that the trend is significant to the 0.001 level.\n\nlibrary(openair)\nTheilSen(mydata, pollutant = \"o3\", \n         ylab = \"ozone (ppb)\", \n         deseason = TRUE,\n         date.format = \"%Y\")\n\n[1] \"Taking bootstrap samples. Please wait.\"\n\n\n\n\nFigure 16.1: Trends in ozone at Marylebone Road. The plot shows the deseasonalised monthly mean concentrations of O3. The solid red line shows the trend estimate and the dashed red lines show the 95% confidence intervals for the trend based on resampling methods. The overall trend is shown at the top-left as 0.38 (ppb) per year and the 95% confidence intervals in the slope from 0.21–0.51 ppb/year. The ‘∗∗∗’ show that the trend is significant to the 0.001 level.\n\n\n\n\nBecause the function runs simulations to estimate the uncertainty in the slope, it can take a little time for all the calculations to finish. These printed results show that in this case the trend in O3 was +0.38 units (i.e. ppb) per year as an average over the entire period. It also shows the 95% confidence intervals in the trend ranged between 0.21 to 0.51 ppb/year. Finally, the significance level in this case is very high; providing very strong evidence that concentrations of O3 increased over the period. The plot together with the summary results is shown in Figure 16.1. Note that if one wanted to display the confidence intervals in the slope at the 99% confidence intervals, the code would be Figure 16.2.\n\nTheilSen(mydata, pollutant = \"o3\", ylab = \"ozone (ppb)\", alpha = 0.01)\n\nSometimes it is useful to consider a subset of data, perhaps by excluding some years. This is easy with the filter function. The following code calculates trends for years greater than 1999 i.e. from 2000 onward.\n\nTheilSen(filter(mydata, format(date, \"%Y\") > 1999), \n         pollutant = \"o3\",\n         ylab = \"ozone (ppb)\")\n\nIt is also possible to calculate trends in many other ways e.g. by wind direction. Considering how trends vary by wind direction can be extremely useful because the influence of different sources invariably depends on the direction of the wind. The TheilSen function splits the wind direction into 8 sectors i.e. N, NE, E etc. The Theil-Sen slopes are then calculated for each direction in turn. This function takes rather longer to run because the simulations need to be run eight times in total. Considering concentrations of O3 again, the output is shown in Figure 16.2. Note that this plot is specifically laid out to assist interpretation, with each panel located at the correct point on the compass. This makes it easy to see immediately that there is essentially no trend in O3 for southerly winds i.e. where the road itself has the strongest influence. On the other hand the strongest evidence of increasing O3 are for northerly winds, where the influence of the road is much less. The reason that there is no trend in O3 for southerly winds is that there is always a great excess of NO, which reacts with O3 to form NO2. At this particular location it will probably take many more years before O3 concentrations start to increase when the wind direction is southerly. Nevertheless, there will always be some hours that do not have such high concentrations of NO.\n\nTheilSen(mydata, pollutant = \"o3\", type = \"wd\", \n         deseason = TRUE,\n         date.format = \"%Y\",\n         ylab = \"ozone (ppb)\")\n\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n\n\n\n\nFigure 16.2: Trends in ozone at Marylebone Road split by eight wind sectors. The TheilSen function will automatically organise the separate panels by the different compass directions.\n\n\n\n\nThe option slope.percent can be set to express slope estimates as a percentage change per year. This is useful for comparing slopes for sites with very different concentration levels and for comparison with emission inventories. The percentage change uses the concentration at the beginning and end months to express the mean slope.\nThe trend, \\(T\\) is defined as:\n\\[\nT [\\%.yr^{-1}] = 100.\\left(\\frac{C_{End}}{C_{Start}}\n  - 1\\right)\\Bigg /N_{years} (\\#eq:MannK)\n\\]\nwhere \\(C_{End}\\) and \\(C_{Start}\\) are the mean concentrations for the end and start date, respectfully. \\(N_{years}\\) is the number of years (or fractions of) the time series spans.\n\nTheilSen(mydata, pollutant = \"o3\", deseason = TRUE,\n            slope.percent = TRUE)\n\n\n\n\n\n\n\nCareful with percentages!\n\n\n\nSometimes considering percentages can be misleading. A specific case to look out for is when the uncertainties in the slope are wide and go from a negative concentration to a positive, which is problematic. In this case it is best to stick with changes in absolute concentrations.\n\n\nThe TheilSen function was written to work with hourly data, which is then averaged into monthly or annual data. However, it is realised that users may already have data that is monthly or annual. The function can therefore accept as input monthly or annual data directly. However, it is necessary to ensure the date field is in the correct format. Assuming data in an Excel file in the format dd/mm/YYYY (e.g. 23/11/2008), it is necessary to convert this to a date format understood by R, as shown below. Similarly, if annual data were available, get the dates in formats like ‘2005-01-01’, ‘2006-01-01’ … and make sure the date is again formatted using as.Date. Note that if dates are pre-formatted as YYYY-mm-dd, then it is sufficient to use as.Date without providing any format information because it is already in the correct format.\n\nmydata$date = as.Date(mydata$date, format = \"%d/%m/%Y\")\n\nFinally, the TheilSen function can consider trends at different sites, provided the input data are correctly formatted. For input, a data frame with three columns is required: date, pollutant and site. The call would then be, for example:\n\nTheilSen(mydata, pollutant = \"no2\", type = \"site\")\n\n\n16.2.1 Output\nThe TheilSen function provides lots of output data for further analysis or adding to a report. To obtain it, it is necessary to read it into a variable:\n\nMKresults <-  TheilSen(mydata, \n                       pollutant = \"o3\", \n                       deseason = TRUE, \n                       type = \"wd\")\n\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n[1] \"Taking bootstrap samples. Please wait.\"\n\n\nThis returns a list of two data frames containing all the monthly mean values and trend statistics and an aggregated summary. The first 6 lines are shown next:\n\nhead(MKresults$data[[1]])\n\n  wd       date      conc         a            b   upper.a     upper.b  lower.a\n1  E 1998-01-01  5.552255 -2.825018 0.0007185139 -8.826644 0.001230108 3.848277\n2  E 1998-02-01  2.919638 -2.825018 0.0007185139 -8.826644 0.001230108 3.848277\n3  E 1998-03-01  3.849364 -2.825018 0.0007185139 -8.826644 0.001230108 3.848277\n4  E 1998-04-01  4.051669 -2.825018 0.0007185139 -8.826644 0.001230108 3.848277\n5  E 1998-05-01  2.304685 -2.825018 0.0007185139 -8.826644 0.001230108 3.848277\n6  E 1998-06-01 -1.560437 -2.825018 0.0007185139 -8.826644 0.001230108 3.848277\n       lower.b          p p.stars     slope intercept intercept.lower\n1 0.0001449689 0.01669449       * 0.2622576 -2.825018        3.848277\n2 0.0001449689 0.01669449       * 0.2622576 -2.825018        3.848277\n3 0.0001449689 0.01669449       * 0.2622576 -2.825018        3.848277\n4 0.0001449689 0.01669449       * 0.2622576 -2.825018        3.848277\n5 0.0001449689 0.01669449       * 0.2622576 -2.825018        3.848277\n6 0.0001449689 0.01669449       * 0.2622576 -2.825018        3.848277\n  intercept.upper      lower     upper slope.percent lower.percent\n1       -8.826644 0.05291365 0.4489894      5.798024     0.9925886\n2       -8.826644 0.05291365 0.4489894      5.798024     0.9925886\n3       -8.826644 0.05291365 0.4489894      5.798024     0.9925886\n4       -8.826644 0.05291365 0.4489894      5.798024     0.9925886\n5       -8.826644 0.05291365 0.4489894      5.798024     0.9925886\n6       -8.826644 0.05291365 0.4489894      5.798024     0.9925886\n  upper.percent\n1      11.96135\n2      11.96135\n3      11.96135\n4      11.96135\n5      11.96135\n6      11.96135\n\n\nOften only the trend statistics are required and not all the monthly values. These can be obtained by:\n\nMKresults$data[[2]]\n\n  wd p.stars       date     conc          a            b     upper.a\n1  E       * 2001-09-15 5.989974  -2.825018 7.185139e-04  -8.8266445\n2  N     *** 2001-09-15 9.786267 -19.142101 2.485907e-03 -28.7157811\n3 NE     *** 2001-09-15 9.728994  -9.728230 1.624926e-03 -24.8702213\n4 NW     *** 2001-09-15 9.785862 -12.953955 1.938155e-03 -22.4189130\n5  S         2001-09-15 5.052729   4.472597 2.509278e-05   1.0745158\n6 SE         2001-09-15 5.780644   4.822710 7.357566e-05   0.8230454\n7 SW         2001-09-15 4.761100   1.818338 2.444785e-04  -1.9192684\n8  W      ** 2001-09-15 5.618710  -1.179040 5.736252e-04  -5.6097531\n       upper.b   lower.a       lower.b           p       slope  intercept\n1 0.0012301079  3.848277  1.449689e-04 0.016694491 0.262257566  -2.825018\n2 0.0032893253 -9.725531  1.649014e-03 0.000000000 0.907355926 -19.142101\n3 0.0029640565  3.185675  5.003905e-04 0.000000000 0.593097968  -9.728230\n4 0.0027688783 -3.377315  1.123129e-03 0.000000000 0.707426586 -12.953955\n5 0.0003281524  7.157070 -2.081446e-04 0.868113523 0.009158865   4.472597\n6 0.0004318445  8.727037 -2.572925e-04 0.684474124 0.026855115   4.822710\n7 0.0005716588  5.538209 -8.999935e-05 0.110183639 0.089234663   1.818338\n8 0.0009552742  3.039959  2.044476e-04 0.003338898 0.209373207  -1.179040\n  intercept.lower intercept.upper       lower     upper slope.percent\n1        3.848277      -8.8266445  0.05291365 0.4489894     5.7980241\n2       -9.725531     -28.7157811  0.60188995 1.2006037    14.4454294\n3        3.185675     -24.8702213  0.18264253 1.0818806     8.6082382\n4       -3.377315     -22.4189130  0.40994205 1.0106406    10.3009943\n5        7.157070       1.0745158 -0.07597279 0.1197756     0.1936654\n6        8.727037       0.8230454 -0.09391175 0.1576232     0.4816915\n7        5.538209      -1.9192684 -0.03284976 0.2086555     2.0662774\n8        3.039959      -5.6097531  0.07462338 0.3486751     4.4667001\n  lower.percent upper.percent\n1     0.9925886     11.961348\n2     8.4310939     24.381954\n3     2.1996728     19.875875\n4     5.0554430     17.134132\n5    -1.5108817      2.703415\n6    -1.5406212      3.008353\n7    -0.7113748      5.313239\n8     1.4544073      8.381943\n\n\nIn the results above the lower and upper fields provide the 95% (or chosen confidence interval using the alpha option) of the trend and slope is the trend estimate expressed in units/year."
  },
  {
    "objectID": "smooth-trend.html#sec-SmoothTrendInfo",
    "href": "smooth-trend.html#sec-SmoothTrendInfo",
    "title": "17  Smooth trends",
    "section": "\n17.1 Background",
    "text": "17.1 Background\nThe smoothTrend function calculates smooth trends in the monthly mean concentrations of pollutants. In its basic use it will generate a plot of monthly concentrations and fit a smooth line to the data and show the 95% confidence intervals of the fit. The smooth line is essentially determined using Generalized Additive Modelling using the mgcv package. This package provides a comprehensive and powerful set of methods for modelling data. In this case, however, the model is a relationship between time and pollutant concentration i.e. a trend. One of the principal advantages of this approach is that the amount of smoothness in the trend is optimised in the sense that it is neither too smooth (therefore missing important features) nor too variable (perhaps fitting ‘noise’ rather than real effects). Some background information on the use of this approach in an air quality setting can be found in Carslaw, Beevers, and Tate (2007).\n\nCarslaw, D. C., S. D. Beevers, and J. E. Tate. 2007. “Modelling and Assessing Trends in Traffic-Related Emissions Using a Generalised Additive Modelling Approach.” Atmospheric Environment 41 (26): 5289–99.\nAppendix C considers smooth trends in more detail and considers how different models can be developed that can be quite sophisticated. Readers should consider this section if they are considering trend analysis in more depth.\nThe user can select to deseasonalise the data first to provide a clearer indication of the overall trend on a monthly basis. The data are deseasonalised using the stl function. The user may also select to use bootstrap simulations to provide an alternative method of estimating the uncertainties in the trend. In addition, the simulated estimates of uncertainty can account for autocorrelation in the residuals using a block bootstrap approach."
  },
  {
    "objectID": "smooth-trend.html#examples-of-trend-analysis",
    "href": "smooth-trend.html#examples-of-trend-analysis",
    "title": "17  Smooth trends",
    "section": "\n17.2 Examples of trend analysis",
    "text": "17.2 Examples of trend analysis\nWe apply the function to concentrations of O3 and NO2 using the code below. The first plot shows the smooth trend in raw O3 concentrations, which shows a very clear seasonal cycle. By removing the seasonal cycle of O3, a better indication of the trend is given, shown in the second plot. Removing the seasonal cycle is more effective for pollutants (or locations) where the seasonal cycle is stronger e.g. for ozone and background sites. Figure 17.1 shows the results of the simulations for NO2 without the seasonal cycle removed. It is clear from this plot that there is little evidence of a seasonal cycle. The principal advantage of the smoothing approach compared with the Theil-Sen method is also clearly shown in this plot. Concentrations of NO2 first decrease, then increase strongly. The trend is therefore not monotonic, violating the Theil-Sen assumptions. Finally, the last plot shows the effects of first deaseasonalising the data: in this case with little effect.\n\nlibrary(openair) # load the package\n\nsmoothTrend(mydata, pollutant = \"o3\", ylab = \"concentration (ppb)\",\n            main = \"monthly mean o3\")\n\nsmoothTrend(mydata, pollutant = \"o3\", deseason = TRUE, ylab = \"concentration (ppb)\",\n            main = \"monthly mean deseasonalised o3\")\n\nsmoothTrend(mydata, pollutant = \"no2\", simulate = TRUE, ylab = \"concentration (ppb)\",\n            main = \"monthly mean no2 (bootstrap uncertainties)\")\nsmoothTrend(mydata, pollutant = \"no2\", deseason = TRUE, simulate =TRUE,\n            ylab = \"concentration (ppb)\",\n            main = \"monthly mean deseasonalised no2 (bootstrap uncertainties)\")\n\n\n\n[1] \"Taking bootstrap samples. Please wait...\"\n\n\n[1] \"Taking bootstrap samples. Please wait...\"\n\n\n\n\n\n\n(a) Monthly mean O3.\n\n\n\n\n\n\n(b) Deseasonalised O3.\n\n\n\n\n\n\n\n\n(c) Monthly mean NO2 with bootstrap uncertainties.\n\n\n\n\n\n\n(d) Deseasonalised monthly mean NO2 with bootstrap uncertainties.\n\n\n\n\nFigure 17.1: Examples of the smoothTrend function applied to Marylebone Road.’\n\n\n\nThe smoothTrend function share many of the functionalities of the TheilSen function. Figure 17.2 shows the result of applying this function to O3 concentrations. The code that produced Figure 17.2) was:\n\nsmoothTrend(mydata, pollutant = \"o3\", deseason = TRUE,\n             type = \"wd\")\n\n\n\nFigure 17.2: Trends in O3 using the smoothTrend function applied to Marylebone Road. The shading shows the estimated 95% confidence intervals.\n\n\n\n\nThe smoothTrend function can easily be used to gain a large amount of information on trends easily. For example, how do trends in NO2, O3 and PM10 vary by season and wind sector. There are 8 wind sectors and four seasons i.e. 32 plots. In Figure 17.3 all three pollutants are chosen and two types (season and wind direction). We also reduce the number of axis labels and the line to improve clarity. There are numerous combinations of analyses that could be produced here and it is very easy to explore the data in a wide number of ways.\n\nsmoothTrend(mydata, pollutant = c(\"no2\", \"pm10\", \"o3\"), \n            type = c(\"wd\", \"season\"),\n            date.breaks = 3, lty = 0)\n\n\n\nFigure 17.3: The smoothTrend function applied to three pollutants, split by wind sector and season."
  },
  {
    "objectID": "smooth-trend.html#seasonal-averages",
    "href": "smooth-trend.html#seasonal-averages",
    "title": "17  Smooth trends",
    "section": "\n17.3 Seasonal averages",
    "text": "17.3 Seasonal averages\nIf the interest is in considering seasonal trends, it makes sense to set the averaging time to ‘season’ and also show each season in a separate panel. In Figure 17.4 some data is imported for O3 from Lullington Heath on the south coast of England. This plot shows there is strongest evidence for an increase in O3 concentrations during springtime from about 2012 onwards. A probable reason for this increase is a general reduction in NOx concentrations.\n\nlh <- importAURN(site = \"lh\", year = 2000:2019)\n\n\nsmoothTrend(lh, pollutant = \"o3\",\n            avg.time = \"season\",\n            type = \"season\",\n            date.breaks = 4)\n\n\n\nFigure 17.4: Trends in O3 by season at the Lullington Heath site with a seasonal averaging time and panel for each season."
  },
  {
    "objectID": "mod-stats.html#sec-ModBack",
    "href": "mod-stats.html#sec-ModBack",
    "title": "18  Model evaluation",
    "section": "\n18.1 Background",
    "text": "18.1 Background\nThe modStats function provides key model evaluation statistics for comparing models against measurements and models against other models.\nThere are a very wide range of evaluation statistics that can be used to assess model performance. There is, however, no single statistic that encapsulates all aspects of interest. For this reason it is useful to consider several performance statistics and also to understand the sort of information or insight they might provide.\nIn the following definitions, \\(O_i\\) represents the \\(i\\)th observed value and \\(M_i\\) represents the \\(i\\)th modelled value for a total of \\(n\\) observations.\nFraction of predictions within a factor or two, FAC2\nThe fraction of modelled values within a factor of two of the observed values are the fraction of model predictions that satisfy:\n\\[\n   0.5 \\leq \\frac{M_i}{O_i} \\leq 2.0\n\\tag{18.1}\\]\nMean bias, MB\nThe mean bias provides a good indication of the mean over or under estimate of predictions. Mean bias in the same units as the quantities being considered.\n\\[\nMB = \\frac{1}{n}\\sum_{i=1}^{N} M_i - O_i  \n\\tag{18.2}\\]\nMean Gross Error, MGE\nThe mean gross error provides a good indication of the mean error regardless of whether it is an over or under estimate. Mean gross error is in the same units as the quantities being considered.\n\\[\nMGE = \\frac{1}{n}\\sum_{i=1}^{N} |M_i - O_i|  \n\\tag{18.3}\\]\nNormalised mean bias, NMB\nThe normalised mean bias is useful for comparing pollutants that cover different concentration scales and the mean bias is normalised by dividing by the observed concentration.\n\\[\nNMB = \\frac{\\sum\\limits_{i=1}^{n} M_i - O_i}{\\sum\\limits_{i=1}^{n} O_i}\n\\tag{18.4}\\]\nNormalised mean gross error, NMGE\nThe normalised mean gross error further ignores whether a prediction is an over or under estimate.\n\\[\nNMGE = \\frac{\\sum\\limits_{i=1}^{n} |M_i - O_i|}{\\sum\\limits_{i=1}^{n} O_i}\n\\tag{18.5}\\]\nRoot mean squared error, RMSE\nThe RMSE is a commonly used statistic that provides a good overall measure of how close modelled values are to predicted values.\n\\[\n   RMSE = \\left({\\frac{\\sum\\limits_{i=1}^{n} (M_i - O_i)^2}{n}}\\right)^{1/2}   \n\\tag{18.6}\\]\nCorrelation coefficient, r\nThe (Pearson) correlation coefficient is a measure of the strength of the linear relationship between two variables. If there is perfect linear relationship with positive slope between the two variables, \\(r\\) = 1. If there is a perfect linear relationship with negative slope between the two variables \\(r\\) = -1. A correlation coefficient of 0 means that there is no linear relationship between the variables. Note that modStats accepts an option method, which can be set to “kendall” and “spearman” for alternative calculations of \\(r\\).\n\\[\nr = \\frac{1}{(n-1)}\\sum\\limits_{i=1}^{n}\\left(\\frac{M_i - \\overline{M}}{\\sigma_M}\\right)\\left(\\frac{O_i - \\overline{O}}{\\sigma_O}\\right)   \n\\tag{18.7}\\]\nCoefficient of Efficiency, COE\nThe Coefficient of Efficiency based on Legates and McCabe (2012) and Legates and McCabe Jr (1999). There have been many suggestions for measuring model performance over the years, but the \\(COE\\) is a simple formulation which is easy to interpret.\n\nLegates, D. R., and G. J. McCabe. 2012. “A Refined Index of Model Performance: A Rejoinder.” International Journal of Climatology.\n\nLegates, D. R., and G. J. McCabe Jr. 1999. “Evaluating the Use of ‘Goodness-of-Fit’ Measures in Hydrologic and Hydroclimatic Model Validation.” Water Resources Research 35 (1): 233–41.\nA perfect model has a \\(COE\\) = 1. As noted by Legates and McCabe although the \\(COE\\) has no lower bound, a value of \\(COE\\) = 0.0 has a fundamental meaning. It implies that the model is no more able to predict the observed values than does the observed mean. Therefore, since the model can explain no more of the variation in the observed values than can the observed mean, such a model can have no predictive advantage.\nFor negative values of \\(COE\\), the model is less effective than the observed mean in predicting the variation in the observations.\n\\[\nCOE = 1.0 - \\frac{\\sum\\limits_{i=1}^{n} |M_i\n             -O_i|}{\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|}  \n\\tag{18.8}\\]\nIndex of Agreement, IOA\nThe Index of Agreement, \\(IOA\\) is commonly used in model evaluation (Willmott, Robeson, and Matsuura 2011). It spans between -1 and +1 with values approaching +1 representing better model performance. An \\(IOA\\) of 0.5, for example, indicates that the sum of the error-magnitudes is one half of the sum of the observed-deviation magnitudes. When \\(IOA\\) = 0.0, it signifies that the sum of the magnitudes of the errors and the sum of the observed-deviation magnitudes are equivalent. When \\(IOA\\) = -0.5, it indicates that the sum of the error-magnitudes is twice the sum of the perfect model-deviation and observed-deviation magnitudes. Values of \\(IOA\\) near -1.0 can mean that the model-estimated deviations about \\(O\\) are poor estimates of the observed deviations; but, they also can mean that there simply is little observed variability — so some caution is needed when the \\(IOA\\) approaches -1. It is defined as (with \\(c = 2\\)):\n\nWillmott, Cort J, Scott M Robeson, and Kenji Matsuura. 2011. “A Refined Index of Model Performance.” International Journal of Climatology.\n\\[\nIOA = \\left\\{\n   \\begin{array}{l}\n     1.0 - \\frac{\\displaystyle \\sum\\limits_{i=1}^{n} |M_i-O_i|}\n{\\displaystyle c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|}, when \\\\\n           \\sum\\limits_{i=1}^{n} |M_i-O_i| \\le c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}| \\\\\n          \\frac{\\displaystyle c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|}\n{\\displaystyle \\sum\\limits_{i=1}^{n} |M_i-O_i|} - 1.0, when \\\\\n           \\sum\\limits_{i=1}^{n} |M_i-O_i| > c\\sum\\limits_{i=1}^{n} |O_i - \\overline{O}|\n   \\end{array}\n\\right.\n\\tag{18.9}\\]"
  },
  {
    "objectID": "mod-stats.html#ModEx",
    "href": "mod-stats.html#ModEx",
    "title": "18  Model evaluation",
    "section": "\n18.2 Example of use",
    "text": "18.2 Example of use\nThe function can be called very simply and only requires two numeric fields to compare. To show how the function works, some synthetic data will be generated for 5 models.\n\n## observations; 100 random numbers\nset.seed(10)\nobs <- 100 * runif(100)\nmod1 <- data.frame(obs, mod = obs + 10, model = \"model 1\")\nmod2 <- data.frame(obs, mod = obs + 20 * rnorm(100), model = \"model 2\")\nmod3 <- data.frame(obs, mod = obs - 10 * rnorm(100), model = \"model 3\")\nmod4 <- data.frame(obs, mod = obs / 2 + 10 * rnorm(100), model = \"model 4\")\nmod5 <- data.frame(obs, mod = obs * 1.5 + 3 * rnorm(100), model = \"model 5\")\nmodData <- rbind(mod1, mod2, mod3, mod4, mod5)\nhead(modData)\n\n        obs      mod   model\n1 50.747820 60.74782 model 1\n2 30.676851 40.67685 model 1\n3 42.690767 52.69077 model 1\n4 69.310208 79.31021 model 1\n5  8.513597 18.51360 model 1\n6 22.543662 32.54366 model 1\n\n\nWe now have a data frame with observations and predictions for 5 models. The evaluation of the statistics is given by:\n\nlibrary(openair) # load package\n\nmodStats(modData, obs = \"obs\", mod = \"mod\", type = \"model\")\n\n# A tibble: 5 × 12\n  model       n  FAC2      MB   MGE     NMB  NMGE  RMSE     r         P      COE\n  <fct>   <int> <dbl>   <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 model 1   100  0.89  10     10     0.225  0.225 10    1     0          0.538  \n2 model 2   100  0.79   0.922 16.6   0.0207 0.373 19.3  0.826 4.04e- 26  0.234  \n3 model 3   100  0.88   1.01   7.89  0.0228 0.177  9.45 0.937 1.35e- 46  0.636  \n4 model 4   100  0.56 -20.6   21.9  -0.463  0.491 25.8  0.814 6.97e- 25 -0.00933\n5 model 5   100  0.96  22.5   22.6   0.506  0.507 26.1  0.996 1.05e-106 -0.0420 \n# … with 1 more variable: IOA <dbl>\n\n\nIt is possible to rank the statistics based on the , which is a good general indicator of model performance.\n\nmodStats(modData, obs = \"obs\", mod = \"mod\", type = \"model\", \n         rank.name = \"model\")\n\n# A tibble: 5 × 12\n  model       n  FAC2      MB   MGE     NMB  NMGE  RMSE     r         P      COE\n  <fct>   <int> <dbl>   <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>     <dbl>    <dbl>\n1 model 3   100  0.88   1.01   7.89  0.0228 0.177  9.45 0.937 1.35e- 46  0.636  \n2 model 1   100  0.89  10     10     0.225  0.225 10    1     0          0.538  \n3 model 2   100  0.79   0.922 16.6   0.0207 0.373 19.3  0.826 4.04e- 26  0.234  \n4 model 4   100  0.56 -20.6   21.9  -0.463  0.491 25.8  0.814 6.97e- 25 -0.00933\n5 model 5   100  0.96  22.5   22.6   0.506  0.507 26.1  0.996 1.05e-106 -0.0420 \n# … with 1 more variable: IOA <dbl>\n\n\nThe modStats function is however much more flexible than indicated above. While it is useful to calculate model evaluation statistics in a straightforward way it can be much more informative to consider the statistics split by different periods.\nData have been assembled from a Defra model evaluation exercise which consists of hourly O3 predictions at 15 receptor points around the UK for 2006. The aim here is not to identify a particular model that is ‘best’ and for this reason the models are simply referred to as model 1, model 2 and so on. We will aim to make the data more widely available. However, data set has this form:\n\nload(\"book_data/modelData.RData\")\nhead(modTest)\n\n        site                date o3   mod   group\n1 Aston.Hill 2006-01-01 00:00:00 NA    NA model 1\n2 Aston.Hill 2006-01-01 01:00:00 74 65.28 model 1\n3 Aston.Hill 2006-01-01 02:00:00 72 64.64 model 1\n4 Aston.Hill 2006-01-01 03:00:00 72 64.46 model 1\n5 Aston.Hill 2006-01-01 04:00:00 70 64.88 model 1\n6 Aston.Hill 2006-01-01 05:00:00 66 65.80 model 1\n\n\nThere are columns representing the receptor location (site), the date, measured values (o3), model predictions (mod) and the model itself (group). There are numerous ways in which the statistics can be calculated. However, of interest here is how the models perform at a single receptor by season. The seasonal nature of O3 is a very important characteristic and it is worth considering in more detail. The statistics are easy enough to calculate as shown below. In this example a subset of the data is selected to consider only the Harwell site. Second, the type option is used to split the calculations by season and model. Finally, the statistics are grouped by the \\(IOA\\) for each season. It is now very easy how model performance changes by season and which models perform best in each season.\n\noptions(digits = 2) ## don't display too many decimal places\nmodStats(subset(modTest, site == \"Harwell\"), \n         obs = \"o3\", mod = \"mod\",\n         type = c(\"season\", \"group\"), rank = \"group\")\n\n# A tibble: 36 × 13\n   season     group     n  FAC2     MB   MGE     NMB  NMGE  RMSE     r         P\n   <ord>      <fct> <int> <dbl>  <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>     <dbl>\n 1 spring (M… mode…  1905 0.875   8.58  16.5  0.137  0.263  21.9 0.576 1.10e-168\n 2 spring (M… mode…  1905 0.876  -2.04  16.8 -0.0325 0.268  21.8 0.452 1.50e- 96\n 3 spring (M… mode…  1905 0.872  11.9   17.7  0.190  0.282  23.7 0.519 9.23e-132\n 4 spring (M… mode…  1905 0.885   4.98  18.1  0.0793 0.289  23.9 0.361 9.74e- 60\n 5 spring (M… mode…  1905 0.870  10.8   19.5  0.172  0.311  24.0 0.588 1.60e-177\n 6 spring (M… mode…  1825 0.821   7.79  20.0  0.123  0.317  25.8 0.480 1.36e-105\n 7 spring (M… mode…  1905 0.786 -13.7   21.4 -0.218  0.340  26.1 0.531 6.17e-139\n 8 spring (M… mode…  1905 0.732 -13.4   24.3 -0.213  0.388  29.8 0.312 3.08e- 44\n 9 spring (M… mode…  1905 0.743   1.01  24.9  0.0161 0.396  32.8 0.264 8.39e- 32\n10 summer (J… mode…  2002 0.918   3.83  15.5  0.0632 0.256  20.9 0.750 0        \n# … with 26 more rows, and 2 more variables: COE <dbl>, IOA <dbl>\n\n\nNote that it is possible to read the results of the modStats function into a data frame, which then allows the results to be plotted. This is generally a good idea when there is a lot of numeric data to consider and plots will convey the information better.\nThe modStats function is much more flexible than indicated above and can be used in lots of interesting ways. The type option in particular makes it possible to split the statistics in numerous ways. For example, to summarise the performance of models by site, model and day of the week:\n\nmodStats(modStats, obs = \"o3\", mod = \"mod\",\n         type = c(\"site\", \"weekday\", \"group\"), \n         rank = \"group\")\n\nSimilarly, if other data are available e.g. meteorological data or other pollutant species then these variables can also be used to test models against ranges in their values. This capability is potentially very useful because it allows for a much more probing analysis into model evaluation. For example, with wind speed and direction it is easy to consider how model performance varies by wind speed intervals or wind sectors, both of which could reveal important performance characteristics."
  },
  {
    "objectID": "taylor-diagram.html#sec-TaylorBack",
    "href": "taylor-diagram.html#sec-TaylorBack",
    "title": "19  Taylor Diagam",
    "section": "\n19.1 Background",
    "text": "19.1 Background\nThe Taylor Diagram is one of the more useful methods for evaluating model performance. Details of the diagram can be found at http://www-pcmdi.llnl.gov/about/staff/Taylor/CV/Taylor_diagram_primer.pdf and in Taylor (2001). The diagram provides a way of showing how three complementary model performance statistics vary simultaneously. These statistics are the correlation coefficient R, the standard deviation (sigma) and the (centred) root-mean-square error. These three statistics can be plotted on one (2D) graph because of the way they are related to one another which can be represented through the Law of Cosines.\n\nTaylor, K. E. 2001. “Summarizing Multiple Aspects of Model Performance in a Single Diagram.” Journal of Geophysical Research 106 (D7): 7183–92.\nThe openair version of the Taylor Diagram has several enhancements that increase its flexibility. In particular, the straightforward way of producing conditioning plots should prove valuable under many circumstances (using the type option). Many examples of Taylor Diagrams focus on model-observation comparisons for several models using all the available data. However, more insight can be gained into model performance by partitioning the data in various ways e.g. by season, daylight/nighttime, day of the week, by levels of a numeric variable e.g. wind speed or by land-use type etc.\nWe first show a diagram and then pick apart the different components to understand how to interpret it. The diagram can look overly complex but once it is understood how to interpret the three main characteristics it becomes much easier to understand. A typical diagram is shown in Figure 19.1 for nine anonymised models used for predicting hourly O3 concentrations at 15 sites around the UK.\n\nlibrary(openair)\nlibrary(tidyverse)\n\n\nload(\"book_data/modelData.RData\")\nhead(modTest)\n\n        site                date o3   mod   group\n1 Aston.Hill 2006-01-01 00:00:00 NA    NA model 1\n2 Aston.Hill 2006-01-01 01:00:00 74 65.28 model 1\n3 Aston.Hill 2006-01-01 02:00:00 72 64.64 model 1\n4 Aston.Hill 2006-01-01 03:00:00 72 64.46 model 1\n5 Aston.Hill 2006-01-01 04:00:00 70 64.88 model 1\n6 Aston.Hill 2006-01-01 05:00:00 66 65.80 model 1\n\n\n\nTaylorDiagram(modTest, \n              obs = \"o3\", \n              mod = \"mod\", \n              group = \"group\")\n\n\n\nFigure 19.1: An example of the use of the TaylorDiagram function.\n\n\n\n\nThe plots shown in Figure 19.2 break the Taylor Diagrams into three components to aid interpretation. The first plot (top left) highlights the comparison of variability in for each model compared with the measurements. The variability is represented by the standard deviation of the observed and modelled values. The plot shows that the observed variability (given by the standard deviation) is about 27 (μg m-3) and is marked as ‘observed’ on the x-axis. The magnitude of the variability is measured as the radial distance from the origin of the plot (the red line with the arrow shows the standard deviation for model \\(g\\), which is about 25~μg m-3). To aid interpretation the radial dashed line is shown from the ‘observed’ point. Each model is shown in this case by the position of the letters a to i. On this basis it can be seen that models 1, \\(a\\), \\(b\\) have more variability than the measurements (because they extend beyond the dashed line), whereas the others have less variability than the measurements. Models \\(a\\) and \\(b\\) are also closed to the dashed line and therefore have the closest variability compared with the observations.\nThe next statistic to consider is the correlation coefficient, \\(R\\) shown by the top-right Figure in Figure 19.2. This is shown on the arc and points that lie closest to the x-axis have the highest correlation. The grey lines help to show this specific correlation coefficients. The red arc shows \\(R\\)=0.7 for model \\(g\\). The best performing models with the highest \\(R\\) are models \\(b\\) and \\(g\\) with correlation coefficients around 0.7. Two models stand out as having much worse correlations with the observations: models \\(e\\) and \\(i\\) (values of around 0.4).\nFinally, the lower plot in Figure 19.2 highlights the centred root-mean square error (RMS). It is centred because the mean values of the data (observations and predictions) are subtracted first. The concentric dashed lines emanating from the `observed’ point show the value of the RMS error — so points furthest from the ‘observed’ value are the worst performing models because they have the highest RMS errors. On this basis, model \\(g\\) has the lowest error of about 20~μg m-3, shown again by the red line. Models \\(e\\) and \\(i\\) are considerably worse because they have RMS errors of around 30 μg m-3.\nSo which model is best? Taken as a whole it is probably model \\(g\\) because it has reasonably similar variability compared with the observations, the highest correlation and the least RMS error. However, models \\(f\\) and \\(b\\) also look to be good. Perhaps it is easier to conclude that models \\(e\\) and \\(i\\) are not good .\nNote that in cases where there is a column site it makes sense to use type = \"site\" to ensure that the statistics are calculated on a per site basis and each panel represents a single site.\n\n\n\n\n\n(a) Standard deviation.\n\n\n\n\n\n\n(b) Correlation coefficient.\n\n\n\n\n\n\n\n\n(c) Centred RMS.\n\n\n\n\nFigure 19.2: Taylor Diagrams broken down to highlight how to interpret the three main statistics. The red line/arrow indicate how to read interpret each of the three statistics."
  },
  {
    "objectID": "taylor-diagram.html#examples-of-taylor-diagrams",
    "href": "taylor-diagram.html#examples-of-taylor-diagrams",
    "title": "19  Taylor Diagam",
    "section": "\n19.2 Examples of Taylor Diagrams",
    "text": "19.2 Examples of Taylor Diagrams\nThe example used here carries on from the previous section using data from a Defra model evaluation exercise. As mentioned previously, the use of the type option offers enormous flexibility for comparing models. However, we will only focus on the seasonal evaluation of the models. In the call below, group is the column that identified the model and type is the conditioning variable that produces in this case four panels — one for each season. Note that in this case we focus on a single site.\n\n## select a single site\nLH <- filter(modTest, site == \"Lullington.Heath\")\nTaylorDiagram(LH, obs = \"o3\", mod = \"mod\", \n              group = \"group\", type = \"season\")\n\n\n\nFigure 19.3: Use of the TaylorDiagam function to show model performance for 9 models used to predict O3 concentrations at the Lullington Heath site.\n\n\n\n\nFigure 19.3 contains a lot of useful information. Consider the summer comparison first. All models tend to underestimate the variability of O3 concentrations because they all lies within the black dashed line. However, models 7 and 9 are close to the observed variability. The general underestimate of the variability for summer conditions might reflect that the models do not adequately capture regional O3 episodes when concentrations are high. Models 7 and 8 do best in terms of high correlation with the measurements (around 0.8) and lowest RMS error (around 20–22 μg m-3). Models 3, 5 and 6 tend to do worse on all three statistics during the summer.\nBy contrast, during wintertime conditions models 1 and 3 are clearly best. From an evaluation perspective it would be useful to understand why some models are better for wintertime conditions and others better in summer and this is clearly something that could be investigated further.\nThere are many other useful comparisons that can be undertaken easily. A few of these are shown below, but not plotted.\n\n## by receptor comparison\nTaylorDiagram(modTest, obs = \"o3\", mod = \"mod\", \n              group = \"group\", type = \"site\")\n\n## by month comparison for a SINGLE site\nTaylorDiagram(subset(modTest, site == \"Harwell\"), \n              obs = \"o3\", mod = \"mod\",\n              group = \"group\", type = \"month\")\n\n## By season AND daylight/nighttime\nTaylorDiagram(subset(modTest, site == \"Harwell\"), \n              obs = \"o3\", mod = \"mod\",\n              group = \"group\", \n              type = c(\"season\", \"daylight\"))\n\nIt is also possible to combine different groups of model results. For example, rather than plot how the models perform at a single site it can be useful to show how they compare at all sites. To do this it is necessary to normalise the data because there will be different values of the observed variable across different sites. In this case we can supply the option group = c(\"group\", \"site\"). This will show the variation by model for all sites. The results are shown in Figure 19.4. These results show that in general models tend to predict in similarly good (or bad) ways across all sites as shown by the grouping of points on Figure 19.4.\n\nTaylorDiagram(modTest, obs = \"o3\", mod = \"mod\", \n              group = c(\"group\", \"site\"), \n              normalise = TRUE, cex = 1, \n              pch = c(15:19, 15:18))\n\n\n\nFigure 19.4: Use of the TaylorDiagam function to show model performance for 9 models used for all sites."
  },
  {
    "objectID": "conditional-quantiles.html#sec-conditionalQuantileBack",
    "href": "conditional-quantiles.html#sec-conditionalQuantileBack",
    "title": "20  Conditional quantiles",
    "section": "\n20.1 Background",
    "text": "20.1 Background\nConditional quantiles are a very useful way of considering model performance against observations for continuous measurements (Wilks 2005). The conditional quantile plot splits the data into evenly spaced bins. For each predicted value bin e.g. from 0 to 10 ppb the corresponding values of the observations are identified and the median, 25/75th and 10/90 percentile (quantile) calculated for that bin. The data are plotted to show how these values vary across all bins. For a time series of observations and predictions that agree precisely the median value of the predictions will equal that for the observations for each bin.\n\nWilks, Daniel S. 2005. Statistical Methods in the Atmospheric Sciences, Volume 91, Second Edition (International Geophysics). 2nd ed. Hardcover; Academic Press.\nThe conditional quantile plot differs from the quantile-quantile plot (Q-Q plot) that is often used to compare observations and predictions. A Q-Q~plot separately considers the distributions of observations and predictions, whereas the conditional quantile uses the corresponding observations for a particular interval in the predictions. Take as an example two time series, the first a series of real observations and the second a lagged time series of the same observations representing the predictions. These two time series will have identical (or very nearly identical) distributions (e.g. same median, minimum and maximum). A Q-Q plot would show a straight line showing perfect agreement, whereas the conditional quantile will not. This is because in any interval of the predictions the corresponding observations now have different values.\nPlotting the data in this way shows how well predictions agree with observations and can help reveal many useful characteristics of how well model predictions agree with observations — across the full distribution of values. A single plot can therefore convey a considerable amount of information concerning model performance. The basic function is considerably enhanced by allowing flexible conditioning easily e.g. to evaluate model performance by season, day of the week and so on, as in other openair functions.\nTo make things more interesting we will use data from a model evaluation exercise organised by Defra in 2010/2011. Many models were evaluated but we only consider hourly ozone predictions from the CMAQ model being used at King’s College London.\nFirst the data are loaded:\n\nload(\"book_data/CMAQozone.RData\")\nclass(CMAQ.KCL$date) <- c(\"POSIXct\", \"POSIXt\")\nhead(CMAQ.KCL)\n\n        site                date o3 rollingO3Meas   mod rollingO3Mod    group\n1 Aston.Hill 2006-01-01 00:00:00 NA            NA 92.80           NA CMAQ.KCL\n2 Aston.Hill 2006-01-01 01:00:00 74            NA 92.18           NA CMAQ.KCL\n3 Aston.Hill 2006-01-01 02:00:00 72            NA 92.14           NA CMAQ.KCL\n4 Aston.Hill 2006-01-01 03:00:00 72            NA 91.72           NA CMAQ.KCL\n5 Aston.Hill 2006-01-01 04:00:00 70            NA 91.50           NA CMAQ.KCL\n6 Aston.Hill 2006-01-01 05:00:00 66            NA 92.28           NA CMAQ.KCL\n\n\nThe data consists of hourly observations of O3 in μg m-3 at 15 rural O3 sites in the UK together with predicted values.1 First, we consider O3 predictions across all sites to help illustrate the purpose of the function. The results are shown in Figure 20.1. An explanation of the Figure is given in its caption.\nWe firs consider an example of the use of conditional quantiles applied to the KCL CMAQ model for 15 rural O3 monitoring sites in 2006, for hourly data. The blue line shows the results for a perfect model. In this case the observations cover a range from 0 to 270 μg m-3. The red line shows the median values of the predictions and corresponding observations. The maximum predicted value is 125 μg m-3, somewhat less than the maximum observed value. The shading shows the predicted quantile intervals i.e. the 25/75th and the 10/90th. A perfect model would lie on the blue line and have a very narrow spread. There is still some spread because even for a perfect model a specific quantile interval will contain a range of values. However, for the number of bins used in this plot the spread will be very narrow. Finally, the histogram shows the counts of predicted values.\n\nlibrary(openair) # load the package\nconditionalQuantile(CMAQ.KCL, obs = \"o3\", mod = \"mod\")\n\n\n\nFigure 20.1: Example of the use of conditional quantiles applied to the KCL CMAQ model for 15 rural O3 monitoring sites in 2006, for hourly data. The blue line shows the results for a perfect model. In this case the observations cover a range from 0 to 270 μg m-3. The red line shows the median values of the predictions and corresponding observations. The maximum predicted value is 125 μg m-3, somewhat less than the maximum observed value. The shading shows the predicted quantile intervals i.e. the 25/75th and the 10/90th. A perfect model would lie on the blue line and have a very narrow spread. There is still some spread because even for a perfect model a specific quantile interval will contain a range of values. However, for the number of bins used in this plot the spread will be very narrow. Finally, the histogram shows the counts of predicted values.\n\n\n\n\nA more informative analysis can be undertaken by considering conditional quantiles separately by site, which is easily done using the type option. The results are shown in Figure 20.2. It is now easier to see where the model performs best and how it varies by site type. For example, at a remote site in Scotland like Strath Vaich it is clear that the model does not capture either the lowest or highest O3 concentrations very well.\n\nconditionalQuantile(CMAQ.KCL, \n                    obs = \"o3\", \n                    mod = \"mod\", \n                    type = \"site\")\n\n\n\nFigure 20.2: Conditional quantiles by site for 15 O3 monitoring sites in the UK.\n\n\n\n\nAs with other openair functions, the ability to consider conditioning can really help with interpretation. For example, what do the conditional quantiles at Lullington Heath (in south-east England) look like by season? This is easily done by subsetting the data to select that site and setting type = \"season\", as shown in Figure 20.3. These results show that winter predictions have good coverage i.e. with width of the blue `perfect model’ line is the same as the observations. However, the predictions tend to be somewhat lower than observations for most concentrations (the median line is below the blue line) — and the width of the 10/75th and 10/90th percentiles is quite broad. However, the area where the model is less good is in summer and autumn because the predictions have low coverage (the red line only covers less than half of the observation line and the width of the percentiles is wide).\nOf course, it is also easy to plot by hour of the day, day of the week, by daylight/nighttime and so on — easily. All these approaches can help better understand why a model does not perform very well rather than just quantifying its performance. Also, these types of analysis are particularly useful when more than one model is involved in a comparison as in the recent Defra model evaluation exercise, which we will come back to later when some results are published.\n\nconditionalQuantile(subset(CMAQ.KCL, site == \"Lullington.Heath\"), \n                    obs = \"o3\",\n                    mod = \"mod\", type = \"season\")\n\n\n\nFigure 20.3: Conditional quantiles at Lullington Heath conditioned by season."
  },
  {
    "objectID": "conditional-quantiles.html#sec-conditionalEval",
    "href": "conditional-quantiles.html#sec-conditionalEval",
    "title": "20  Conditional quantiles",
    "section": "\n20.2 Conditional evaluation",
    "text": "20.2 Conditional evaluation\nThere are numerous ways in which model performance can be assessed, including the use of common statistical measures described in Chapter 18. These approaches are very useful for comparing models against observations and other models. However, model developers would generally like to know why a model may have poor performance under some situations. This is a much more challenging issue to address. However, useful information can be gained by considering how other variables vary simultaneously.\nThe conditionalEval function provides information on how other variables vary across the same intervals as shown on the conditional quantile plot. There are two types of variable that can be considered by setting the value of statistic. First, statistic can be another variable in the data frame. In this case the plot will show the different proportions of statistic across the range of predictions. For example statistic = \"season\" will show for each interval of mod the proportion of predictions that were spring, summer, autumn or winter. This is useful because if model performance is worse for example at high concentrations of mod then knowing that these tend to occur during a particular season etc. can be very helpful when trying to understand why a model fails. See Section 25.2 for more details on the types of variable that can be statistic. Another example would be statistic = \"ws\" (if wind speed were available in the data frame), which would then split wind speed into four quantiles and plot the proportions of each. Again, this would help show whether model performance in predicting concentrations of O3 for example is related to low to high wind speed conditions.\nconditionalEval can also simultaneously plot the model performance of other observed/predicted variable pairs according to different model evaluation statistics. These statistics derive from the Chapter 18) function and include MB, NMB, r, COE, MGE, NMGE, RMSE and FAC2. More than one statistic can be supplied e.g. statistic = c(\"NMB\", \"COE\"). Bootstrap samples are taken from the corresponding values of other variables to be plotted and their statistics with 95% confidence intervals calculated. In this case, the model performance of other variables is shown across the same intervals of mod, rather than just the values of single variables. In this second case the model would need to provide observed/predicted pairs of other variables.\nFor example, a model may provide predictions of NOx and wind speed (for which there are also observations available). The conditionalEval function will show how well these other variables are predicted for the same prediction intervals of the main variable assessed in the conditional quantile plot e.g. ozone. In this case, values are supplied to var.obs (observed values for other variables) and var.mod (modelled values for other variables). For example, to consider how well the model predicts NOx and wind speed var.obs = c(\"nox.obs\", \"ws.obs\") and var.mod = c(\"nox.mod\",  \"ws.mod\") would be supplied (assuming nox.obs, nox.mod, ws.obs,  ws.mod are present in the data frame). The analysis could show for example, when ozone concentrations are under-predicted, the model may also be shown to over-predict concentrations of NOx at the same time, or under-predict wind speeds. Such information can thus help identify the underlying causes of poor model performance. For example, an under-prediction in wind speed could result in higher surface NOx concentrations and lower ozone concentrations. Similarly if wind speed predictions were good and NOx was over predicted it might suggest an over-estimate of NOx emissions. One or more additional variables can be plotted.\nA special case is statistic = \"cluster\". In this case a data frame is provided that contains the cluster calculated by trajCluster and importTraj. Alternatively users could supply their own pre-calculated clusters. These calculations can be very useful in showing whether certain back trajectory clusters are associated with poor (or good) model performance. Note that in the case of statistic = \"cluster\" there will be fewer data points used in the analysis compared with the ordinary statistics above because the trajectories are available for every three hours. Also note that statistic = \"cluster\" cannot be used together with the ordinary model evaluation statistics such as MB. The output will be a bar chart showing the proportion of each interval of mod by cluster number.\nFar more insight can be gained into model performance through conditioning using type. For example, type = \"season\" will plot conditional quantiles and the associated model performance statistics of other variables by each season. type can also be a factor or character field e.g. representing different models used.\nAs an example, similar data to that described above from CMAQ have been used as an example.\n\n\n\nA subset of the data for the North Kensington site can be imported as shown below.\n\ncondDat <- readRDS(url(\"https://davidcarslaw.com/data/openair/condDat.rds\"))\n\nThe file contains observed and modelled hourly values for O3, NOx, wind speed, wind direction, temperature and relative humidity.\n\nhead(condDat)\n\n                  date O3.obs  NOx.obs ws.obs wd.obs temp.obs rh.obs O3.mod\n5  2006-01-01 00:00:00     10 29.43665 4.6296    190      4.9     89  14.80\n10 2006-01-01 01:00:00     15 17.55393     NA    210      5.1     90  17.46\n15 2006-01-01 02:00:00     11 19.64817 2.5720    220      4.9     94  18.31\n20 2006-01-01 03:00:00     11 19.15393 3.6008    270      5.7     91  18.25\n25 2006-01-01 04:00:00     11 17.03037 3.0864    270      5.0     94  18.08\n30 2006-01-01 05:00:00     12 15.98325 3.6008    260      5.8     94  14.87\n   NOx.mod ws.mod wd.mod temp.mod rh.mod\n5    24.00   2.78    224     3.85  93.16\n10   19.91   2.63    226     3.85  92.77\n15   18.25   2.52    236     2.85  99.40\n20   18.33   2.48    253     2.85  99.19\n25   18.09   2.24    275     3.85  97.49\n30   21.38   2.43    285     4.85  94.41\n\n\nThe conditionalEval function can be used straightforwardly to provide information on how predictions depend on another variable in general. In this case the option statistic can refer to another variable in the data frame to see how the quality of predictions depend on values of that variable. For example, in Figure 20.4 it can be seen how wind speed varies across the O3 prediction intervals. At low predicted concentrations of O3 there is a high proportion of low wind speed conditions (0 to 2.57 m s-1). When O3 is predicted to be around 40 ppb the wind speed tends to be higher — and finally at higher predicted concentrations of O3 the wind speed tends to decrease again. The important aspect of plotting data in this way is that it can directly relate the prediction performance to values of other variables, which should help develop a much better idea of the conditions that matter most. The user can therefore develop a good feel for the types of conditions where a model performs well or poorly and this might provide clues as to the underlying reasons for specific model behaviour.\n\nconditionalEval(condDat, obs = \"O3.obs\", mod = \"O3.mod\",\n                statistic = \"ws.obs\",\n                col.var = \"Set3\")\n\n\n\nFigure 20.4: Conditional quantiles at for O3 concentrations (left plot). On the right is a plot showing how the wind speed varies across the O3 prediction intervals.\n\n\n\n\nIn an extension to Figure 20.4 it is possible to derive information on the simultaneous model performance of other variables. Figure 20.5 shows the conditional quantile plot for hourly O3 predictions. This shows among other things that concentrations of O3 tend to be under-predicted for concentrations less than about 20 ppb. The Figure on the right shows the simultaneous model performance for wind speed and NOx for the same prediction intervals as shown in the conditional quantile plot. The plot on the right shows that for low concentrations of predicted O3 there is a tendency for NOx concentrations to be overestimated (NMB \\(\\approx\\) 0.2 to 0.4) and wind speeds to be underestimated (NMB \\(\\approx\\) -0.2 to -0.3). One possible explanation for this behaviour is that the meteorological model tends to produce wind speeds that are too low, which would result in higher concentrations of NOx, which in turn would result in lower concentrations of O3. Note that it is possible to include more than one statistic, which would be plotted in a new panel e.g. statistic = c(\"NMB\", \"r\").\nIn essence the conditionalEval function provides more information on model performance that can help better diagnose potential problems. Clearly, there are many other ways in which the results can be analysed, which will depend on the data available.\n\nconditionalEval(condDat, obs = \"O3.obs\", mod = \"O3.mod\",\n                var.obs = c(\"NOx.obs\", \"ws.obs\"),\n                var.mod = c(\"NOx.mod\", \"ws.mod\"),\n                statistic = \"NMB\",\n                var.names = c(\"nox\", \"wind speed\"))\n\n\n\nFigure 20.5: Conditional quantiles at for O3 concentrations (left plot). On the right is the model performance for wind speed and NOx predictions, which in this case is for the Normalised Mean Bias.\n\n\n\n\nA plot using temperature predictions shows that for most of the range in O3 predictions there is very little bias in temperature (although there is some negative bias in temperature for very low concentration O3 predictions):\n\n conditionalEval(condDat, obs = \"O3.obs\", mod = \"O3.mod\",\n                var.obs = c(\"temp.obs\", \"ws.obs\"),\n                var.mod = c(\"temp.mod\", \"ws.mod\"),\n                statistic = \"NMB\", var.names = c(\"temperature\", \"wind speed\"))\n\nFinally, (but not shown) it can be very useful to consider model performance in terms of air mass origin. In the example below, trajectories are imported, a cluster analysis undertaken and then evaluated using conditionalEval.\n\n## import trajectories for 2006\ntraj <- importTraj(\"london\", 2006)\n\n## carry out a cluster analysis\ncl <- trajCluster(traj, method = \"Angle\", n.cluster = 5)\n\n## merge with orginal model eval data\ncondDat <- merge(condDat, cl, by = \"date\")\n\n## plot it\nconditionalEval(condDat, obs = \"O3.obs\", mod = \"O3.mod\",\n                statistic = \"cluster\",\n                col.var = \"Set3\")"
  },
  {
    "objectID": "maps-overview.html#installation",
    "href": "maps-overview.html#installation",
    "title": "\n21  openairmaps primer\n",
    "section": "\n21.1 Installation",
    "text": "21.1 Installation\nopenairmaps is on CRAN, so can be simply downloaded using install.packages().\n\ninstall.packages(\"openairmaps\")\n\nIf you would like to access new features and bug fixes before they’re pushed to CRAN, the development version of openairmaps can be downloaded directly from GitHub.\n\nlibrary(remotes)\ninstall_github(\"davidcarslaw/openairmaps\")\n\nIf you would like to use openairmaps in your analysis, don’t forget to load it using the library() function.\n\nlibrary(openairmaps)\nlibrary(tidyverse)\n\n\n\n\n\n\n\nThis chapter was produced using R version 4.2.2 and openairmaps version 0.7.0."
  },
  {
    "objectID": "maps-overview.html#background",
    "href": "maps-overview.html#background",
    "title": "\n21  openairmaps primer\n",
    "section": "\n21.2 Background",
    "text": "21.2 Background\nAs the R ecosystem developed, rmarkdown and, more recently, Quarto) emerged as capable tools for combining data analysis with document preparation. While these approaches can render typical .docx and .pdf outputs, one of their most common output formats is the HTML document. This format has many strengths, but a key one is interactivity; HTML widgets allow documents to be more informative and engaging. Numerous packages have been developed to easily develop these interactive widgets, such as plotly for plots, DT for tables, and leaflet for maps. The openairmaps package concerns itself with making leaflet maps.\nAir quality data analysis — particularly as it pertains to long term monitoring data — naturally lends itself to being visualised spatially on a map. Monitoring networks are geographically distributed, and ignoring their geographical context may lead to incomplete insights at best and incorrect conclusions at worst! Furthermore, many air quality analysis tools are directional, asking questions of the data along the lines of “do elevated concentrations come from the North, South, East or West?” The natural question that follows is “well, what actually is it to the North/South/East/West that could be causing elevated concentrations?” — a map can help answer that question straightforwardly."
  },
  {
    "objectID": "maps-overview.html#leaflet-primer",
    "href": "maps-overview.html#leaflet-primer",
    "title": "\n21  openairmaps primer\n",
    "section": "\n21.3 leaflet Primer",
    "text": "21.3 leaflet Primer\nWhile openairmaps can be used without a thorough understanding of the leaflet package, some knowledge can definitely help! Figure 21.1 shows an example leaflet map. Try the following:\n\nClick and drag to move the map around — you can go anywhere in the world!\nUse your scroll wheel or the +/- options (top left) to zoom in and out.\nHover over the marker to see its label.\nClick on the marker to see a popup.\nUse the “layer control” menu (top right) to show and hide the marker, and swap between base maps.\n\n\nlibrary(leaflet)\n\nleaflet() %>%\n  addTiles(group = \"OSM\") %>%\n  addProviderTiles(\"CartoDB.Positron\", group = \"CartoDB.Positron\") %>%\n  addMarkers(\n    lat = 51.5034,\n    lng = -0.1276,\n    label = \"10 Downing Street\",\n    popup = \"This is where the UK Prime Minister lives!\",\n    group = \"Marker\"\n  ) %>%\n  addLayersControl(\n    baseGroups = c(\"CartoDB.Positron\", \"OSM\"), \n    overlayGroups = \"Marker\"\n  )\n\n\nFigure 21.1: An example of a leaflet map.\n\n\n\nThe anatomy of a leaflet map may look a little unusual, so to unpack:\n\nEvery leaflet map starts with the leaflet() function.\nDifferent base maps can be added using addTiles() and addProviderTiles(). There are many of these base maps — some are minimalist and some are busy, some are colourful and some are black-and-white, some are plain and some are highly stylised!\nMarkers can be added using addMarkers() and other similar functions. These markers can have different features such as hover labels and popups.\nThe addLayersControl() function creates a menu which allows users to show/hide or swap between certain features of the map.\n\nDon’t worry if you don’t fully understand all of this — the “all-in-one” functions in openairmaps deal with much of this for you! The only leaflet awareness you’ll need to use these functions is knowledge of the available base map providers, which can be accessed using leaflet::providers. These are shown in the collapsible below (the list is very long!).\n\n\n\n\n\n\nView Providers\n\n\n\n\n\n\nleaflet::providers\n\n$OpenStreetMap\n[1] \"OpenStreetMap\"\n\n$OpenStreetMap.Mapnik\n[1] \"OpenStreetMap.Mapnik\"\n\n$OpenStreetMap.DE\n[1] \"OpenStreetMap.DE\"\n\n$OpenStreetMap.CH\n[1] \"OpenStreetMap.CH\"\n\n$OpenStreetMap.France\n[1] \"OpenStreetMap.France\"\n\n$OpenStreetMap.HOT\n[1] \"OpenStreetMap.HOT\"\n\n$OpenStreetMap.BZH\n[1] \"OpenStreetMap.BZH\"\n\n$OpenSeaMap\n[1] \"OpenSeaMap\"\n\n$OpenPtMap\n[1] \"OpenPtMap\"\n\n$OpenTopoMap\n[1] \"OpenTopoMap\"\n\n$OpenRailwayMap\n[1] \"OpenRailwayMap\"\n\n$OpenFireMap\n[1] \"OpenFireMap\"\n\n$SafeCast\n[1] \"SafeCast\"\n\n$Thunderforest\n[1] \"Thunderforest\"\n\n$Thunderforest.OpenCycleMap\n[1] \"Thunderforest.OpenCycleMap\"\n\n$Thunderforest.Transport\n[1] \"Thunderforest.Transport\"\n\n$Thunderforest.TransportDark\n[1] \"Thunderforest.TransportDark\"\n\n$Thunderforest.SpinalMap\n[1] \"Thunderforest.SpinalMap\"\n\n$Thunderforest.Landscape\n[1] \"Thunderforest.Landscape\"\n\n$Thunderforest.Outdoors\n[1] \"Thunderforest.Outdoors\"\n\n$Thunderforest.Pioneer\n[1] \"Thunderforest.Pioneer\"\n\n$Thunderforest.MobileAtlas\n[1] \"Thunderforest.MobileAtlas\"\n\n$Thunderforest.Neighbourhood\n[1] \"Thunderforest.Neighbourhood\"\n\n$OpenMapSurfer\n[1] \"OpenMapSurfer\"\n\n$OpenMapSurfer.Roads\n[1] \"OpenMapSurfer.Roads\"\n\n$OpenMapSurfer.Hybrid\n[1] \"OpenMapSurfer.Hybrid\"\n\n$OpenMapSurfer.AdminBounds\n[1] \"OpenMapSurfer.AdminBounds\"\n\n$OpenMapSurfer.ContourLines\n[1] \"OpenMapSurfer.ContourLines\"\n\n$OpenMapSurfer.Hillshade\n[1] \"OpenMapSurfer.Hillshade\"\n\n$OpenMapSurfer.ElementsAtRisk\n[1] \"OpenMapSurfer.ElementsAtRisk\"\n\n$Hydda\n[1] \"Hydda\"\n\n$Hydda.Full\n[1] \"Hydda.Full\"\n\n$Hydda.Base\n[1] \"Hydda.Base\"\n\n$Hydda.RoadsAndLabels\n[1] \"Hydda.RoadsAndLabels\"\n\n$MapBox\n[1] \"MapBox\"\n\n$Stamen\n[1] \"Stamen\"\n\n$Stamen.Toner\n[1] \"Stamen.Toner\"\n\n$Stamen.TonerBackground\n[1] \"Stamen.TonerBackground\"\n\n$Stamen.TonerHybrid\n[1] \"Stamen.TonerHybrid\"\n\n$Stamen.TonerLines\n[1] \"Stamen.TonerLines\"\n\n$Stamen.TonerLabels\n[1] \"Stamen.TonerLabels\"\n\n$Stamen.TonerLite\n[1] \"Stamen.TonerLite\"\n\n$Stamen.Watercolor\n[1] \"Stamen.Watercolor\"\n\n$Stamen.Terrain\n[1] \"Stamen.Terrain\"\n\n$Stamen.TerrainBackground\n[1] \"Stamen.TerrainBackground\"\n\n$Stamen.TerrainLabels\n[1] \"Stamen.TerrainLabels\"\n\n$Stamen.TopOSMRelief\n[1] \"Stamen.TopOSMRelief\"\n\n$Stamen.TopOSMFeatures\n[1] \"Stamen.TopOSMFeatures\"\n\n$TomTom\n[1] \"TomTom\"\n\n$TomTom.Basic\n[1] \"TomTom.Basic\"\n\n$TomTom.Hybrid\n[1] \"TomTom.Hybrid\"\n\n$TomTom.Labels\n[1] \"TomTom.Labels\"\n\n$Esri\n[1] \"Esri\"\n\n$Esri.WorldStreetMap\n[1] \"Esri.WorldStreetMap\"\n\n$Esri.DeLorme\n[1] \"Esri.DeLorme\"\n\n$Esri.WorldTopoMap\n[1] \"Esri.WorldTopoMap\"\n\n$Esri.WorldImagery\n[1] \"Esri.WorldImagery\"\n\n$Esri.WorldTerrain\n[1] \"Esri.WorldTerrain\"\n\n$Esri.WorldShadedRelief\n[1] \"Esri.WorldShadedRelief\"\n\n$Esri.WorldPhysical\n[1] \"Esri.WorldPhysical\"\n\n$Esri.OceanBasemap\n[1] \"Esri.OceanBasemap\"\n\n$Esri.NatGeoWorldMap\n[1] \"Esri.NatGeoWorldMap\"\n\n$Esri.WorldGrayCanvas\n[1] \"Esri.WorldGrayCanvas\"\n\n$OpenWeatherMap\n[1] \"OpenWeatherMap\"\n\n$OpenWeatherMap.Clouds\n[1] \"OpenWeatherMap.Clouds\"\n\n$OpenWeatherMap.CloudsClassic\n[1] \"OpenWeatherMap.CloudsClassic\"\n\n$OpenWeatherMap.Precipitation\n[1] \"OpenWeatherMap.Precipitation\"\n\n$OpenWeatherMap.PrecipitationClassic\n[1] \"OpenWeatherMap.PrecipitationClassic\"\n\n$OpenWeatherMap.Rain\n[1] \"OpenWeatherMap.Rain\"\n\n$OpenWeatherMap.RainClassic\n[1] \"OpenWeatherMap.RainClassic\"\n\n$OpenWeatherMap.Pressure\n[1] \"OpenWeatherMap.Pressure\"\n\n$OpenWeatherMap.PressureContour\n[1] \"OpenWeatherMap.PressureContour\"\n\n$OpenWeatherMap.Wind\n[1] \"OpenWeatherMap.Wind\"\n\n$OpenWeatherMap.Temperature\n[1] \"OpenWeatherMap.Temperature\"\n\n$OpenWeatherMap.Snow\n[1] \"OpenWeatherMap.Snow\"\n\n$HERE\n[1] \"HERE\"\n\n$HERE.normalDay\n[1] \"HERE.normalDay\"\n\n$HERE.normalDayCustom\n[1] \"HERE.normalDayCustom\"\n\n$HERE.normalDayGrey\n[1] \"HERE.normalDayGrey\"\n\n$HERE.normalDayMobile\n[1] \"HERE.normalDayMobile\"\n\n$HERE.normalDayGreyMobile\n[1] \"HERE.normalDayGreyMobile\"\n\n$HERE.normalDayTransit\n[1] \"HERE.normalDayTransit\"\n\n$HERE.normalDayTransitMobile\n[1] \"HERE.normalDayTransitMobile\"\n\n$HERE.normalDayTraffic\n[1] \"HERE.normalDayTraffic\"\n\n$HERE.normalNight\n[1] \"HERE.normalNight\"\n\n$HERE.normalNightMobile\n[1] \"HERE.normalNightMobile\"\n\n$HERE.normalNightGrey\n[1] \"HERE.normalNightGrey\"\n\n$HERE.normalNightGreyMobile\n[1] \"HERE.normalNightGreyMobile\"\n\n$HERE.normalNightTransit\n[1] \"HERE.normalNightTransit\"\n\n$HERE.normalNightTransitMobile\n[1] \"HERE.normalNightTransitMobile\"\n\n$HERE.reducedDay\n[1] \"HERE.reducedDay\"\n\n$HERE.reducedNight\n[1] \"HERE.reducedNight\"\n\n$HERE.basicMap\n[1] \"HERE.basicMap\"\n\n$HERE.mapLabels\n[1] \"HERE.mapLabels\"\n\n$HERE.trafficFlow\n[1] \"HERE.trafficFlow\"\n\n$HERE.carnavDayGrey\n[1] \"HERE.carnavDayGrey\"\n\n$HERE.hybridDay\n[1] \"HERE.hybridDay\"\n\n$HERE.hybridDayMobile\n[1] \"HERE.hybridDayMobile\"\n\n$HERE.hybridDayTransit\n[1] \"HERE.hybridDayTransit\"\n\n$HERE.hybridDayGrey\n[1] \"HERE.hybridDayGrey\"\n\n$HERE.hybridDayTraffic\n[1] \"HERE.hybridDayTraffic\"\n\n$HERE.pedestrianDay\n[1] \"HERE.pedestrianDay\"\n\n$HERE.pedestrianNight\n[1] \"HERE.pedestrianNight\"\n\n$HERE.satelliteDay\n[1] \"HERE.satelliteDay\"\n\n$HERE.terrainDay\n[1] \"HERE.terrainDay\"\n\n$HERE.terrainDayMobile\n[1] \"HERE.terrainDayMobile\"\n\n$FreeMapSK\n[1] \"FreeMapSK\"\n\n$MtbMap\n[1] \"MtbMap\"\n\n$CartoDB\n[1] \"CartoDB\"\n\n$CartoDB.Positron\n[1] \"CartoDB.Positron\"\n\n$CartoDB.PositronNoLabels\n[1] \"CartoDB.PositronNoLabels\"\n\n$CartoDB.PositronOnlyLabels\n[1] \"CartoDB.PositronOnlyLabels\"\n\n$CartoDB.DarkMatter\n[1] \"CartoDB.DarkMatter\"\n\n$CartoDB.DarkMatterNoLabels\n[1] \"CartoDB.DarkMatterNoLabels\"\n\n$CartoDB.DarkMatterOnlyLabels\n[1] \"CartoDB.DarkMatterOnlyLabels\"\n\n$CartoDB.Voyager\n[1] \"CartoDB.Voyager\"\n\n$CartoDB.VoyagerNoLabels\n[1] \"CartoDB.VoyagerNoLabels\"\n\n$CartoDB.VoyagerOnlyLabels\n[1] \"CartoDB.VoyagerOnlyLabels\"\n\n$CartoDB.VoyagerLabelsUnder\n[1] \"CartoDB.VoyagerLabelsUnder\"\n\n$HikeBike\n[1] \"HikeBike\"\n\n$HikeBike.HikeBike\n[1] \"HikeBike.HikeBike\"\n\n$HikeBike.HillShading\n[1] \"HikeBike.HillShading\"\n\n$BasemapAT\n[1] \"BasemapAT\"\n\n$BasemapAT.basemap\n[1] \"BasemapAT.basemap\"\n\n$BasemapAT.grau\n[1] \"BasemapAT.grau\"\n\n$BasemapAT.overlay\n[1] \"BasemapAT.overlay\"\n\n$BasemapAT.highdpi\n[1] \"BasemapAT.highdpi\"\n\n$BasemapAT.orthofoto\n[1] \"BasemapAT.orthofoto\"\n\n$nlmaps\n[1] \"nlmaps\"\n\n$nlmaps.standaard\n[1] \"nlmaps.standaard\"\n\n$nlmaps.pastel\n[1] \"nlmaps.pastel\"\n\n$nlmaps.grijs\n[1] \"nlmaps.grijs\"\n\n$nlmaps.luchtfoto\n[1] \"nlmaps.luchtfoto\"\n\n$NASAGIBS\n[1] \"NASAGIBS\"\n\n$NASAGIBS.ModisTerraTrueColorCR\n[1] \"NASAGIBS.ModisTerraTrueColorCR\"\n\n$NASAGIBS.ModisTerraBands367CR\n[1] \"NASAGIBS.ModisTerraBands367CR\"\n\n$NASAGIBS.ViirsEarthAtNight2012\n[1] \"NASAGIBS.ViirsEarthAtNight2012\"\n\n$NASAGIBS.ModisTerraLSTDay\n[1] \"NASAGIBS.ModisTerraLSTDay\"\n\n$NASAGIBS.ModisTerraSnowCover\n[1] \"NASAGIBS.ModisTerraSnowCover\"\n\n$NASAGIBS.ModisTerraAOD\n[1] \"NASAGIBS.ModisTerraAOD\"\n\n$NASAGIBS.ModisTerraChlorophyll\n[1] \"NASAGIBS.ModisTerraChlorophyll\"\n\n$NLS\n[1] \"NLS\"\n\n$JusticeMap\n[1] \"JusticeMap\"\n\n$JusticeMap.income\n[1] \"JusticeMap.income\"\n\n$JusticeMap.americanIndian\n[1] \"JusticeMap.americanIndian\"\n\n$JusticeMap.asian\n[1] \"JusticeMap.asian\"\n\n$JusticeMap.black\n[1] \"JusticeMap.black\"\n\n$JusticeMap.hispanic\n[1] \"JusticeMap.hispanic\"\n\n$JusticeMap.multi\n[1] \"JusticeMap.multi\"\n\n$JusticeMap.nonWhite\n[1] \"JusticeMap.nonWhite\"\n\n$JusticeMap.white\n[1] \"JusticeMap.white\"\n\n$JusticeMap.plurality\n[1] \"JusticeMap.plurality\"\n\n$Wikimedia\n[1] \"Wikimedia\"\n\n$GeoportailFrance\n[1] \"GeoportailFrance\"\n\n$GeoportailFrance.parcels\n[1] \"GeoportailFrance.parcels\"\n\n$GeoportailFrance.ignMaps\n[1] \"GeoportailFrance.ignMaps\"\n\n$GeoportailFrance.maps\n[1] \"GeoportailFrance.maps\"\n\n$GeoportailFrance.orthos\n[1] \"GeoportailFrance.orthos\"\n\n$OneMapSG\n[1] \"OneMapSG\"\n\n$OneMapSG.Default\n[1] \"OneMapSG.Default\"\n\n$OneMapSG.Night\n[1] \"OneMapSG.Night\"\n\n$OneMapSG.Original\n[1] \"OneMapSG.Original\"\n\n$OneMapSG.Grey\n[1] \"OneMapSG.Grey\"\n\n$OneMapSG.LandLot\n[1] \"OneMapSG.LandLot\"\n\n\n\n\n\nIf you would like to use openairmaps in a more advanced way (for example, using the “marker” class of functions), you may find it useful to learn some leaflet. Its authors have written an excellent free guide to help you get started."
  },
  {
    "objectID": "maps-overview.html#sec-functions",
    "href": "maps-overview.html#sec-functions",
    "title": "\n21  openairmaps primer\n",
    "section": "\n21.4 Functions",
    "text": "21.4 Functions\nopenairmaps currently has three main families of functions — those being network visualisation, directional analysis, and trajectory analysis — plus a handful of “utility” functions. These are summarised in Table 21.1. The functions are further divided into two categories:\n\n“All-in-one” functions are most like openair functions in that they will construct a map from the ground-up. These functions provide the quickest route from data to a HTML map.\n“Marker” functions are most like leaflet functions in that they add layers onto pre-existing leaflet maps. These are more complicated to use, but are more powerful if you are more familiar with how leaflet works.\n\n\n\n\n\n\n\n\nTable 21.1:  The openairmaps toolkit. \n  \n  \nFamily\n      Description\n      All-in-one\n      Markers\n    \n\n\n\nNetwork Visualisation\n\n\nVisualises any of the networks made available by importMeta().\n\n\nnetworkMap()\n\n\n\n\n\nDirectional Analysis\n\n\nUses any of the openair directional analysis plots (e.g., polarPlot()) as markers, allowing them to be viewed in their geospatial context.\n\n\nannulusMap()freqMap()percentileMap()polarMap()pollroseMap()windroseMap()\n\n\naddPolarMarkers()\n\n\n\n\nTrajectory Analysis\n\n\nCreates interactive visualisations of HYSPLIT trajectories, allowing for finer investigations when compared with their static openair counterparts.\n\n\ntrajMap()trajLevelMap()\n\n\naddTrajPaths()\n\n\n\n\n\n\n\n\nThe next few pages will discuss each of these function families in turn, starting with network visualisation."
  },
  {
    "objectID": "maps-network.html#introduction",
    "href": "maps-network.html#introduction",
    "title": "\n22  Network Visualisation\n",
    "section": "\n22.1 Introduction",
    "text": "22.1 Introduction\nopenair makes it easy to access any of the UK’s air quality monitoring networks. The following networks are available:\n\nThe Automatic Urban and Rural Network (AURN), the UK’s national network.\nThe devolved UK networks; Air Quality England (AQE), Scotland (SAQN), Wales (WAQN) and Northern Ireland (NI).\nLocally managed English networks operated by individual local authorities.\nThe Imperial College London (formerly Kings College London) network.\nA simplified Europe-wide air quality network (although the saqgetr package better serves obtaining European AQ data).\n\nLets consider the metadata available for the AURN.\n\nlibrary(openair)\naurn_meta <- importMeta(source = \"aurn\", all = TRUE)\ndplyr::glimpse(aurn_meta)\n\nRows: 2,729\nColumns: 14\n$ code            <chr> \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\", \"ABD\"…\n$ site            <chr> \"Aberdeen\", \"Aberdeen\", \"Aberdeen\", \"Aberdeen\", \"Aberd…\n$ site_type       <chr> \"Urban Background\", \"Urban Background\", \"Urban Backgro…\n$ latitude        <dbl> 57.15736, 57.15736, 57.15736, 57.15736, 57.15736, 57.1…\n$ longitude       <dbl> -2.094278, -2.094278, -2.094278, -2.094278, -2.094278,…\n$ variable        <chr> \"O3\", \"NO\", \"NO2\", \"NOx\", \"SO2\", \"CO\", \"PM10\", \"NV10\",…\n$ Parameter_name  <chr> \"Ozone\", \"Nitric oxide\", \"Nitrogen dioxide\", \"Nitrogen…\n$ start_date      <dttm> 2003-08-01, 1999-09-18, 1999-09-18, 1999-09-18, 2001-…\n$ end_date        <chr> \"2021-09-20\", \"2021-09-20\", \"2021-09-20\", \"2021-09-20\"…\n$ ratified_to     <dttm> 2021-09-20, 2021-09-20, 2021-09-20, 2021-09-20, 2007-…\n$ zone            <chr> \"North East Scotland\", \"North East Scotland\", \"North E…\n$ agglomeration   <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ local_authority <chr> \"Aberdeen City\", \"Aberdeen City\", \"Aberdeen City\", \"Ab…\n$ source          <chr> \"aurn\", \"aurn\", \"aurn\", \"aurn\", \"aurn\", \"aurn\", \"aurn\"…\n\n\nThis dataset has 2729 rows and 14 columns, which is a lot of information to explore in a tabular format. A more effective way to communicate this information could be a map, which can be created automatically using openairmaps."
  },
  {
    "objectID": "maps-network.html#the-networkmap-function",
    "href": "maps-network.html#the-networkmap-function",
    "title": "\n22  Network Visualisation\n",
    "section": "\n22.2 The networkMap() function",
    "text": "22.2 The networkMap() function\nNetwork visualisation is the simplest family of functions in openairmaps — in fact, it only has one member. networkMap() is a function which you can consider as an analogue to importMeta() from openair. Consider networkMap() as an analogue to importMeta() — it can map any of the sources that importMeta() can, using the same codes (\"aurn\", \"saqn\", \"local\", \"kcl\", etc.) to select sites. Figure 22.1 visualises the active AURN sites as of 2023-02-20.\n\nlibrary(openairmaps)\nnetworkMap(source = \"aurn\")\n\n\nFigure 22.1: An interactive map of the AURN monitoring network.\n\n\n\nnetworkMap() is a quite simple function, with the following arguments for customisation:\n\nsource: Any number of the importMeta() sources — can be “aurn”, “saqn” (or “saqd”), “aqe”, “waqn”, “ni”, “local”, “kcl” or “europe”.\ncontrol: Any column of the equivalent importMeta() dataset, which is used to create a “layer control” menu to allow readers to filter for certain sites. The control option is quite opinionated, and selects an appropriate style of layer control depending on the column selected (e.g., pollutants are switched between, whereas multiple site types can be selected at once)\ndate: By default, networkMap() shows currently active monitoring sites. By specifying date, sites active at that date will be shown. This may be of interest if you want to explore the history of your chosen monitoring site. Dates can either be provided as a character string in the “YYYY-MM-DD” format, or alternatively as a single year (e.g., date = 2020) which will show sites still operational at the end of that year.\ncluster: By default, markers are clustered together until users zoom in close. This default behaviour improves the appearance and performance of the HTML map widget. The cluster argument allows you to turn this feature off.1\nprovider: Any number of the leaflet providers (see leaflet::providers or the list in the openairmaps overview page).\n\nSome of these arguments are demonstrated in Figure 22.2, which shows the AURN, AQE Network, and locally managed English networks.2 Pay particular attention to the layer control menu, which allows you to toggle different site types on and off.\n\nnetworkMap(source = c(\"aurn\", \"aqe\", \"local\"),\n           control = \"site_type\")\n\n\nFigure 22.2: Demonstrating more features of networkMap()."
  },
  {
    "objectID": "maps-network.html#do-it-yourself-network-maps",
    "href": "maps-network.html#do-it-yourself-network-maps",
    "title": "\n22  Network Visualisation\n",
    "section": "\n22.3 “Do It Yourself” Network Maps",
    "text": "22.3 “Do It Yourself” Network Maps\nIf you are only interested in a few sites, you may wish to create your own, smaller network map. Currently, openairmaps only contains functionality to visualise entire networks, but it is relatively easy to create a map of your own using leaflet. While Figure 22.3 shows a network map of just sites in York, its associated code chunk can be used as a template for other small network maps. It uses the buildPopup() function from openairmaps which was written for use with the directional analysis mapping functions, so is described in greater detail on the next page.\n\nlibrary(leaflet)\nlibrary(dplyr)\nlibrary(stringr)\n\n# import all Meta data for the AURN\naurn_meta <- importMeta(\"aurn\", all = TRUE)\n\n# prep data for leaflet\nmap_data <- \n  aurn_meta %>%\n  # get sites in York\n  filter(local_authority == \"York\") %>% \n  # build a popup\n  buildPopup(\n    latitude = \"latitude\",\n    longitude = \"longitude\",\n    cols = c(\"code\", \"site\", \"site_type\", \"zone\", \"local_authority\"), \n    names = c(\"AURN Code\" = \"code\", \"Name\" = \"site\", \n              \"Site Type\" = \"site_type\", \n              \"Zone\" = \"zone\", \"LA\" = \"local_authority\")\n  ) %>%\n  # get unique sites\n  distinct(site, .keep_all = TRUE)\n\n# create a basic leaflet map\nleaflet(map_data) %>%\n  addTiles() %>%\n  addMarkers(popup = ~popup)\n\n\nFigure 22.3: Demonstrating more features of networkMap()."
  },
  {
    "objectID": "maps-polar.html#data-requirements",
    "href": "maps-polar.html#data-requirements",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.1 Data Requirements",
    "text": "23.1 Data Requirements\nopenairmaps contains the polar_data dataset to allow users to test the directional analysis functions. The structure of this data set is provided below, and a summary is given in Table 23.1. The important feature of this data when compared to openair::mydata is latitude and longitude information, which openairmaps needs to place the directional analysis markers in the correct positions.\n\nlibrary(openairmaps)\ndplyr::glimpse(polar_data)\n\nRows: 35,040\nColumns: 13\n$ date       <dttm> 2009-01-01 00:00:00, 2009-01-01 01:00:00, 2009-01-01 02:00…\n$ nox        <dbl> 113, 40, 48, 36, 40, 50, 50, 53, 80, 111, 206, 113, 86, 82,…\n$ no2        <dbl> 46, 32, 36, 29, 32, 36, 34, 34, 50, 59, 67, 61, 52, 53, 52,…\n$ pm2.5      <dbl> 42, 45, 43, 37, 36, 33, 33, 31, 27, 28, 37, 30, 27, 29, 27,…\n$ pm10       <dbl> 46, 49, 46, NA, 38, 32, 36, 32, 30, 32, 39, 37, 32, 33, 34,…\n$ site       <chr> \"London Bloomsbury\", \"London Bloomsbury\", \"London Bloomsbur…\n$ lat        <dbl> 51.52229, 51.52229, 51.52229, 51.52229, 51.52229, 51.52229,…\n$ lon        <dbl> -0.125889, -0.125889, -0.125889, -0.125889, -0.125889, -0.1…\n$ site_type  <chr> \"Urban Background\", \"Urban Background\", \"Urban Background\",…\n$ wd         <dbl> 58.92536, 74.46675, 30.00000, 45.00000, 70.00000, 46.63627,…\n$ ws         <dbl> 2.066667, 1.900000, 1.550000, 2.100000, 1.500000, 2.100000,…\n$ visibility <dbl> 5000.000, 4933.333, 5000.000, 4900.000, 5000.000, 6000.000,…\n$ air_temp   <dbl> 0.8666667, 0.8666667, 0.8000000, 0.8500000, 0.8666667, 0.96…\n\n\n\n\n\n\n\n\n\nTable 23.1:  A statistical summary of the polar_data dataset. \n  \n  \nCharacteristic\n      \nLondon Bloomsbury, N = 8,7601\n\n      \nLondon Cromwell Road 2, N = 8,7601\n\n      \nLondon Marylebone Road, N = 8,7601\n\n      \nLondon N. Kensington, N = 8,7601\n\n    \n\n\ndate\n2009-01-01 to 2009-12-31 23:00:00\n2009-01-01 to 2009-12-31 23:00:00\n2009-01-01 to 2009-12-31 23:00:00\n2009-01-01 to 2009-12-31 23:00:00\n\n\nnox\n71 (44, 122)\n138 (96, 199)\n258 (139, 418)\n34 (19, 65)\n\n\nno2\n52 (36, 71)\n69 (53, 88)\n99 (67, 141)\n29 (15, 48)\n\n\npm2.5\n13 (10, 19)\nNA (NA, NA)\n19 (13, 27)\n10 (7, 17)\n\n\npm10\n16 (11, 23)\nNA (NA, NA)\n31 (21, 43)\n17 (12, 24)\n\n\nlat\n51.522\n51.495\n51.523\n51.521\n\n\nlon\n-0.126\n-0.179\n-0.155\n-0.213\n\n\nwd\n213 (134, 265)\n213 (134, 265)\n213 (134, 265)\n213 (134, 265)\n\n\nws\n3.77 (2.60, 5.30)\n3.77 (2.60, 5.30)\n3.77 (2.60, 5.30)\n3.77 (2.60, 5.30)\n\n\nvisibility\n14,436 (11,177, 16,843)\n14,436 (11,177, 16,843)\n14,436 (11,177, 16,843)\n14,436 (11,177, 16,843)\n\n\nair_temp\n12 (7, 16)\n12 (7, 16)\n12 (7, 16)\n12 (7, 16)\n\n\n\n\n1 Range; Median (IQR); Median\n    \n\n\n\n\n\nIf you would prefer to use data from different sites or years, the import*() functions from openair make it easy to obtain pollution data with associated site latitude/longitude. The key thing to remember is to use the meta = TRUE argument when using a function like importAURN() to have the lat/lon (& site type) appended to your imported data.\n\nsunderland <- openair::importAURN(site = c(\"sun2\", \"sunr\"), year = 2015, meta = TRUE)\nnames(sunderland)\n\n [1] \"site\"      \"code\"      \"date\"      \"nox\"       \"no2\"       \"no\"       \n [7] \"o3\"        \"pm2.5\"     \"v2.5\"      \"nv2.5\"     \"ws\"        \"wd\"       \n[13] \"air_temp\"  \"latitude\"  \"longitude\" \"site_type\"\n\n\nBy “directional analysis”, we are referring to the outputs from openair functions like polarPlot(). As a reminder as to what these figures look like, see Figure 23.1.\n\nset.seed(123)\nopenair::polarAnnulus(polar_data)\nopenair::polarFreq(polar_data)\nopenair::percentileRose(polar_data)\nopenair::polarPlot(polar_data)\nopenair::pollutionRose(polar_data)\nopenair::windRose(polar_data)\nopenair::polarDiff(polar_data, dplyr::mutate(polar_data, nox = jitter(nox, factor = 5)))\n\n\n\n\n\n(a) Polar Annulus\n\n\n\n\n\n\n(b) Polar Frequency\n\n\n\n\n\n\n\n\n(c) Percentile Rose\n\n\n\n\n\n\n(d) Polar Plot\n\n\n\n\n\n\n\n\n(e) Pollution Rose\n\n\n\n\n\n\n(f) Wind Rose\n\n\n\n\n\n\n\n\n(g) Polar Diff\n\n\n\n\nFigure 23.1: All of the directional analysis figures which can be plotted on a map."
  },
  {
    "objectID": "maps-polar.html#overview",
    "href": "maps-polar.html#overview",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.2 Overview",
    "text": "23.2 Overview\nThe easiest way to get polar plots on a map is through the use of the all-in-one mapping functions. These are all named using the pattern {function-name}Map, where {function_name} is a short hand for the equivalent openair function. A reference is provided in Table 23.2.\n\n\n\n\n\n\n\nTable 23.2:  A reference table for openairmaps directional analysis mapping functions. \n  \n  \nopenair\n      openairmaps\n      scale arguments\n      unique arguments\n    \n\n\n\npolarAnnulus()\n\n\nannulusMap()\n\n\nlimits\n\n\nperiod\n\n\n\n\npolarFreq()\n\n\nfreqMap()\n\n\nbreaks\n\n\nstatistic\n\n\n\n\npercentileRose()\n\n\npercentileMap()\n\n\npercentile\n\n\n--\n\n\n\n\npolarPlot()\n\n\npolarMap()\n\n\nlimits\n\n\nx\n\n\n\n\npollutionRose()\n\n\npollroseMap()\n\n\nbreaks\n\n\nstatistic\n\n\n\n\nwindRose()\n\n\nwindroseMap()\n\n\nws.int, breaks\n\n\n--\n\n\n\n\npolarDiff()\n\n\n`diffMap()\n\n\nlimits\n\n\nx\n\n\n\n\n\n\n\n\nEffectively all of these functions have very similar arguments, although some are unique to the specific function (also shown in Table 23.2). The important ones to pay attention to are:\n\n\ndata: The data you would like to map. Ensure that lat/lon information is present.1\n\n\n\npollutant: The pollutant(s) of interest. If multiple pollutants are provided, a “layer control” menu will allow readers to swap between them.\nlatitude, longitude: The lat/lon column names. If they are not specified, the functions will attempt to guess them based on common names (e.g., “lon”, “lng”, “long” and “longitude” for longitude).\ncontrol: A column to use to create a “layer control” menu. Specifying control effectively splits the input data along the specified column, creating multiple separate sets of directional analysis plots. Common columns to pass to control will be those created by openair::cutData() or openair::splitByDate().2\n\n\npopup: A column to be used to create a HTML “popup” that appears when users click the markers. This would be useful to label each marker with its corresponding site name or code, although other information could be usefully included (e.g., site type, average pollutant concentrations, and so on). A more complicated popup can be created using the buildPopup() function.\nlabel: Much the same as “popup”, but the message will appear when users hover-over the marker rather than click on it. Labels are often much shorter than popups.\nprovider: The leaflet base map provider(s) you’d like to use. If multiple providers are provided, a “layer control” menu will allow readers to swap between them. Note that you can provide multiple pollutants and providers!\nThe “scale” arguments (e.g., limits for polarMap()). By specifying a scale, all polar markers will use the same colour scale, making them quantitatively comparable to one another. Specifying a scale will also draw a shared legend at the top-right of the plot, unless draw.legend is set to FALSE.\nalpha: Controls the transparency of the polar markers, as sometimes making them semi-transparent may be desirable (for examples, if they are slightly overlapping, or seeing more of the basemap is useful). alpha should be a number between 0 and 1, where 1 is completely opaque and 0 is completely transparent.\nThe two “marker diameter” arguments, which control the size and resolution of the polar markers. It is assumed that circular markers are desired, so any number provided will be used as the marker width and height. If, for whatever reason, a non-circular marker is desired, a vector in the form c(width, height) can be provided.\nd.icon changes the actual size of the markers on the map, defaulting to 200.\nd.fig changes the size of the actual openair figure, defaulting to 3.5 inches. In practice, this translates to changing the resolution of the figure on the map, so you should look to adjust d.fig in the same direction as d.icon so that the axis scales remain readable.\n...: Any additional arguments to pass to the equivalent openair function."
  },
  {
    "objectID": "maps-polar.html#simple-demonstrations",
    "href": "maps-polar.html#simple-demonstrations",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.3 Simple Demonstrations",
    "text": "23.3 Simple Demonstrations\npolarMap() is demonstrated in Figure 23.2. Try clicking on each of the markers to see which sites they correspond to.\n\npolarMap(\n  polar_data,\n  pollutant = \"nox\",\n  latitude = \"lat\",\n  longitude = \"lon\",\n  popup = \"site\"\n)\n\n\nFigure 23.2: A demonstration of polarMap().\n\n\n\nAnother example, this time using annulusMap(), is given in Figure 23.3. Note that this time there are two different pollutants plotted, which can be swapped between using the layer control menu. openairmaps automatically deals with subscripts in common pollutant names.\n\nannulusMap(\n  polar_data,\n  pollutant = c(\"nox\", \"no2\"), \n  provider = \"CartoDB.Positron\",\n  latitude = \"lat\",\n  longitude = \"lon\"\n)\n\n\nFigure 23.3: A more complex demonstration, this time using annulusMap()."
  },
  {
    "objectID": "maps-polar.html#colour-scales",
    "href": "maps-polar.html#colour-scales",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.4 Colour Scales",
    "text": "23.4 Colour Scales\nFigure 23.2 could be described as using polarMap() in a “qualitative” mode — each site is using its own colour scale, so they cannot be easily compared quantitatively. There are two ways to use polarMap() in a more “quantitative” way:\n\nUse the appropriate “scale” argument to set a colour scale that all markers will share. For polarMap() (and annulusMap()) this is the “limits” argument, which works the same way as in polarPlot() (and annulusPlot()). In fact, all of the “scales” arguments shown in Table 23.2 work in the exact same way as their corresponding openair function. Setting a shared scale will draw an easy-to-read shared legend, which can be disabled using the draw.legend argument.\nSet the key argument to be TRUE, which will draw the colour bar next to each individual marker. This may be advantageous if one site is much more polluted compared to another one, but the individual colour bars can be confusing and difficult to read depending on the chosen base map.\n\n\n\nShared Scale\nDistinct Scales\n\n\n\n\npolarMap(\n  polar_data,\n  pollutant = \"nox\",\n  latitude = \"lat\",\n  longitude = \"lon\",\n  popup = \"site\",\n  limits = c(0, 500)\n)\n\n\nFigure 23.4: A demonstration of polarMap() with a shared colour scale.\n\n\n\n\n\n\npolarMap(\n  polar_data,\n  pollutant = \"nox\",\n  latitude = \"lat\",\n  longitude = \"lon\",\n  popup = \"site\",\n  key = TRUE\n)\n\n\nFigure 23.5: A demonstration of polarMap() with individual colour scales."
  },
  {
    "objectID": "maps-polar.html#use-of-control",
    "href": "maps-polar.html#use-of-control",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.5 Use of control\n",
    "text": "23.5 Use of control\n\nFigure 23.6 uses percentileMap() and demonstrates how to use the “control” option to create a custom “layer control” menu and “label” and “popup” to label the markers, as well as passing on arguments to the equivalent openair function — in this case, passing the “intervals” argument to percentileRose() so that all of the markers are on the same radial axis.\n\npolar_data %>%\n  openair::cutData(\"weekend\") %>% \n  percentileMap(\n    pollutant = \"nox\",\n    control = \"weekend\",\n    latitude = \"lat\",\n    longitude = \"lon\", \n    provider = \"Esri.WorldTopoMap\",\n    cols = \"viridis\",\n    popup = \"site\",\n    label = \"site_type\",\n    intervals = c(0, 200, 400, 600, 800, 1000)\n  )\n\n\nFigure 23.6: A demonstration of percentileMap() using the ‘control’ option and passing arguments to openair::percentileRose()."
  },
  {
    "objectID": "maps-polar.html#building-popups",
    "href": "maps-polar.html#building-popups",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.6 Building Popups",
    "text": "23.6 Building Popups\nSo far, popups have used a single column to label the markers, but you will often want to encode more data than just the site name or type. For example, you may want to use the site name and type and the average wind speed and the dates it was active! To do so, you can use the buildPopup() function. This function has a handful of arguments:\n\ndata: the data you are going to use with, e.g., polarMap().\ncols: the columns you would like to encode in your popup.\nlatitude & longitude: the decimal latitude/longitude, which buildPopup() will use to identify individual sites to create labels for.\nnames: a named vector used to rename columns in the popup.\ncontrol: optional. This should only be used if you are going to use the control option in, e.g., polarMap() and you’d expect different popups for the different layers (i.e., it isn’t needed for site names/types, but it is needed for pollutant concentrations).\nfun.character, fun.numeric & fun.dttm: the functions used to summarise character/factor, numeric, and date-time columns. These have nice defaults, but you may wish to override them.\n\nThink of buildPopup() as an intermediate between your data and the polar mapping function. All it does on its own is return the input data with a “popup” column appended, which can then be used with the popup argument of the mapping function. Figure 23.7 demonstrates the use of buildPopup() — try swapping between the layers and clicking on each of the markers.\n\npolar_data %>%\n  openair::cutData(\"weekend\") %>%\n  buildPopup(\n    cols = c(\"site\", \"site_type\", \"date\", \"nox\"),\n    names = c(\n      \"Site\" = \"site\",\n      \"Site Type\" = \"site_type\",\n      \"Date Range\" = \"date\",\n      \"Average nox\" = \"nox\"\n    ),\n    control = \"weekend\"\n  ) %>%\n  pollroseMap(pollutant = \"nox\",\n              popup = \"popup\",\n              breaks = 6,\n              control = \"weekend\")\n\n\nFigure 23.7: A demonstration of the buildPopup() function."
  },
  {
    "objectID": "maps-polar.html#marker-function",
    "href": "maps-polar.html#marker-function",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.7 Marker Function",
    "text": "23.7 Marker Function\nThe directional analysis marker function is addPolarMarkers(), which behaves similarly (but not identically) to leaflet::addMarkers(). You will need to define the data you’re using, the lat/lng3 columns, a column to distinguish different sites (type), and an openair function (fun). As with leaflet::addMarkers() and similar functions, you can define group and layerId, which allows you to create more complex maps than can be achieved using the all-in-one openairmaps functions.\nTo demonstrate, Figure 23.8 has been created. This uses the polarFreq() function to plot multiple polar pollutant frequency plots for oxides of nitrogen. What is different about this map is that users can select the specific statistic they are interested in – in this case, mean, median or maximum. This is achieved by using the group arguments and addLayersControl().\n\nlibrary(leaflet)\nlibrary(openair)\nleaflet() %>%\n  addProviderTiles(\n    \"Stamen.Toner\"\n  ) %>% \n  addPolarMarkers(\n    lng = \"lon\", lat = \"lat\",\n    pollutant = \"nox\",\n    group = \"Mean\",\n    data = polar_data,\n    fun = polarFreq,\n    statistic = \"mean\"\n  ) %>% \n  addPolarMarkers(\n    lng = \"lon\", lat = \"lat\",\n    pollutant = \"nox\",\n    group = \"Median\",\n    data = polar_data,\n    fun = polarFreq,\n    statistic = \"median\"\n  ) %>% \n  addPolarMarkers(\n    lng = \"lon\", lat = \"lat\",\n    pollutant = \"nox\",\n    group = \"Max\",\n    data = polar_data,\n    fun = polarFreq,\n    statistic = \"max\"\n  ) %>%\n  addLayersControl(\n    baseGroups = c(\"Mean\", \"Median\", \"Max\")\n  )\n\n\nFigure 23.8: Using addPolarMarkers to create a more complex map.\n\n\n\nOne could imagine different applications, using this approach. For example:\n\nGiving users the option to swap between different periods for a polarAnnulus() map, or different polar coordinates in a polarPlot() map (i.e., different x arguments).\nAllowing users to swap between different plot types (e.g., have “Wind Rose”, “Pollution Rose” and “Polar Plot” on the layer control menu).\n\nThe options are pretty much endless for the kinds of things you could achieve using this approach. If one of the all-in-one functions doesn’t give you the flexibility you need, try to see if you can create your vision yourself from scratch using leaflet and the addPolarMarkers() function."
  },
  {
    "objectID": "maps-polar.html#static-maps",
    "href": "maps-polar.html#static-maps",
    "title": "\n23  Directional Analysis\n",
    "section": "\n23.8 Static Maps",
    "text": "23.8 Static Maps\nWhile interactive maps are useful for exploratory analysis and HTML documents/websites, there are numerous situations in which a static directional analysis map may be desired. For example, academic publications often demand submissions which compile to PDF. openairmaps provides “static” versions of all seven directional analysis maps, identified by appending the word “Static” to the end of the corresponding interactive map (e.g., polarMapStatic()).\nThe static maps are designed to be almost identical to the interactive maps, with very similar arguments to help easily switch between the two. There are a handful of exceptions, however:\n\nHTML maps have the control argument, whereas static maps have the facet argument. facet creates separate panels in the same figure in place of having a menu to switch between marker sets. Different panels are also created when multiple pollutants are provided. The arrangement of these panels can be controlled using facet.nrow.\nHTML maps use leaflet, whereas static maps use ggplot2 and ggmap. The functions automatically attempt to estimate an appropriate bounding box to draw the map. THe zoom of this map can be controlled using zoom, or users can provide their own map themselves using the ggmap argument. This latter argument takes the output of ggmap::get_stamenmap().\nStatic maps naturally do not have the popup and label arguments. However, a benefit of being based in ggplot2 is that limited further customisation is possible, such as manually adding labels using ggplot2::geom_label().\nHTML maps have different default d.icon and d.fig values.\n\nAn example “static map” is provided in ?fig-polarMapStatic.\n\npolarMapStatic(\n  polar_data,\n  pollutant = c(\"nox\",\"pm2.5\"),\n  latitude = \"lat\",\n  longitude = \"lon\",\n  d.icon = 100,\n  d.fig = 2.5,\n  alpha = .75\n)"
  },
  {
    "objectID": "maps-traj.html#data",
    "href": "maps-traj.html#data",
    "title": "\n24  Trajectory Analysis\n",
    "section": "\n24.1 Data",
    "text": "24.1 Data\nopenairmaps contains the traj_data dataset to allow users to test the directional analysis functions. The structure of this data set is provided below. This was obtained using the importTraj() function in openair; if you are using your own trajectory data, try to match this structure (including column names) so that the openairmaps functions will work correctly.\n\nlibrary(openairmaps)\ndplyr::glimpse(traj_data)\n\nRows: 5,432\nColumns: 17\n$ date     <dttm> 2010-04-15, 2010-04-15, 2010-04-15, 2010-04-15, 2010-04-15, …\n$ receptor <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ year     <dbl> 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2…\n$ month    <int> 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4…\n$ day      <int> 15, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1…\n$ hour     <int> 0, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9,…\n$ hour.inc <dbl> 0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -1…\n$ lat      <dbl> 51.500, 51.692, 51.879, 52.063, 52.246, 52.431, 52.616, 52.79…\n$ lon      <dbl> -0.100, 0.139, 0.378, 0.618, 0.859, 1.102, 1.343, 1.580, 1.81…\n$ height   <dbl> 10.0, 10.4, 10.5, 10.5, 10.4, 10.2, 9.9, 9.7, 9.8, 10.2, 10.9…\n$ pressure <dbl> 1013.5, 1014.2, 1014.8, 1015.2, 1015.6, 1015.9, 1016.1, 1016.…\n$ date2    <dttm> 2010-04-15 00:00:00, 2010-04-14 23:00:00, 2010-04-14 22:00:0…\n$ nox      <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1…\n$ no2      <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1…\n$ o3       <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 7…\n$ pm2.5    <dbl> 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 2…\n$ pm10     <dbl> 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 2…"
  },
  {
    "objectID": "maps-traj.html#overview",
    "href": "maps-traj.html#overview",
    "title": "\n24  Trajectory Analysis\n",
    "section": "\n24.2 Overview",
    "text": "24.2 Overview\nThere are two all-in-one mapping functions for trajectory analysis in openairmaps, which will be discussed in turn.\n\ntrajMap(), which is an analogue to openair::trajPlot().\ntrajLevelMap(), which is an analogue to openair::trajLevel().\n\nBoth of these functions require the data to be in the structure shown in the previous section."
  },
  {
    "objectID": "maps-traj.html#mapping-trajectory-paths",
    "href": "maps-traj.html#mapping-trajectory-paths",
    "title": "\n24  Trajectory Analysis\n",
    "section": "\n24.3 Mapping Trajectory Paths",
    "text": "24.3 Mapping Trajectory Paths\nBy default, trajMap() will plot your trajectories in black on a default “OpenStreetMap” base map, but the colour argument allows you to colour by any column of your data. For example, Figure 24.1 shows the trajectory paths coloured by arrival date. A popup is also automatically generated — try clicking on the points to get information about that particular air mass.\n\ntrajMap(traj_data, colour = \"date\")\n\n\nFigure 24.1: A simple demonstration of trajMap.\n\n\n\nWhat may be of greater interest is colouring the trajectories by the concentration of pollutant on their arrival. The colour argument can be used for this. When colour is specified, trajectory paths are plotted in order of the numeric value of the “colour” column, so the highest concentrations (which are assumed to be of most interest) are most easily accessible. Specifying colour also creates a legend and appends a relevant line to the popup — explore the example in Figure 24.2.\n\ntrajMap(\n  traj_data,\n  colour = \"pm10\",\n  npoints = 6,\n  cols = \"magma\",\n  provider = \"CartoDB.Positron\"\n)\n\n\nFigure 24.2: A more thorough application of trajMap, this time coloured by PM concentrations.\n\n\n\nThere is a control option in trajMap() which allows for a layer control menu to be generated. This might be useful if you have a lot of trajectory paths and want to let users reduce the clutter. It also pairs nicely with clustered trajectories returned by openair::trajCluster(). An example of this is given below, in ?fig-trajNeon.\n\nclustdata <- openair::trajCluster(traj_data)\n\n\ntrajMap(\n  data = clustdata$data$traj,\n  colour = \"cluster\",\n  control = \"cluster\",\n  cols = c(\"#9B5DE5\", \"#F15BB5\", \"#FEE440\", \"#00BBF9\", \"#00F5D4\"),\n  provider = \"Stamen.Toner\"\n)"
  },
  {
    "objectID": "maps-traj.html#mapping-gridded-trajectories",
    "href": "maps-traj.html#mapping-gridded-trajectories",
    "title": "\n24  Trajectory Analysis\n",
    "section": "\n24.4 Mapping Gridded Trajectories",
    "text": "24.4 Mapping Gridded Trajectories\ntrajLevelMap() is the other all-in-one trajectory function and is, in many ways, simpler than trajMap(). Think of trajLevelMap() as you do openair::trajLevel() — it has many of the same arguments and does effectively the same thing, but returns an interactive map. In Figure 24.3, try hovering over and clicking each of the tiles to learn more specific details about each bin.\n\ntrajLevelMap(\n  traj_data,\n  statistic = \"pscf\",\n  pollutant = \"pm10\",\n  cols = \"viridis\",\n  provider = \"Esri\"\n)\n\n\nFigure 24.3: An interactive ‘frequency’ map plotted by trajLevelMap.\n\n\n\nAt time of writing, all of the different trajLevel() statistics are supported in trajLevelMap()."
  },
  {
    "objectID": "maps-traj.html#marker-function",
    "href": "maps-traj.html#marker-function",
    "title": "\n24  Trajectory Analysis\n",
    "section": "\n24.5 Marker Function",
    "text": "24.5 Marker Function\nThe marker function equivalent of trajMap() is addTrajPaths() which, much like addPolarMarkers(), allows you to customise your trajectory maps. For example, Figure 24.4 shows a way to plot multiple sets of trajectory data on one map (in this case, one arriving in London and the other in Paris).\n\nlibrary(leaflet)\nlibrary(openair)\n\nfrance <- importTraj(\"paris\", year = 2009) %>%\n  selectByDate(\n    start = \"15/4/2009\",\n    end = \"21/4/2009\"\n  )\n\nuk <- importTraj(year = 2009) %>%\n  selectByDate(\n    start = \"15/4/2009\",\n    end = \"21/4/2009\"\n  )\n\nleaflet() %>%\n  addTiles() %>%\n  addTrajPaths(data = uk,\n               color = \"blue\",\n               group = \"London, UK\", \n               opacity = .25) %>%\n  addMarkers(data = dplyr::slice_head(uk, n = 1),\n             lat = ~lat, lng = ~lon,\n             group = \"London, UK\", label = \"UK\") %>%\n  addTrajPaths(data = france,\n               color = \"red\",\n               group = \"Paris, France\", \n               opacity = .25) %>%\n  addMarkers(data = dplyr::slice_head(france, n = 1), \n             lat = ~lat, lng = ~lon,\n             group = \"Paris, France\", label = \"FR\") %>%\n  addLayersControl(overlayGroups = c(\"Paris, France\", \"London, UK\"))\n\n\nFigure 24.4: A demonstration of the use of addTrajPaths."
  },
  {
    "objectID": "utility-functions.html#sec-selectByDate",
    "href": "utility-functions.html#sec-selectByDate",
    "title": "25  Utility functions",
    "section": "\n25.1 Selecting data by date",
    "text": "25.1 Selecting data by date\nSelecting by date/time in R can be intimidating for new users—and time-consuming for all users. The selectByDate function aims to make this easier by allowing users to select data based on the British way of expressing date i.e. d/m/y. This function should be very useful in circumstances where it is necessary to select only part of a data frame.\nFirst load the packages we need.\n\nlibrary(openair)\nlibrary(tidyverse)\n\n\n## select all of 1999\ndata.1999 <- selectByDate(mydata, start = \"1/1/1999\", end = \"31/12/1999\")\nhead(data.1999)\n\n# A tibble: 6 × 10\n  date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n1 1999-01-01 00:00:00  5.04   140    88    35     4    21  3.84 1.02     18\n2 1999-01-01 01:00:00  4.08   160   132    41     3    17  5.24 2.7      11\n3 1999-01-01 02:00:00  4.8    160   168    40     4    17  6.51 2.87      8\n4 1999-01-01 03:00:00  4.92   150    85    36     3    15  4.18 1.62     10\n5 1999-01-01 04:00:00  4.68   150    93    37     3    16  4.25 1.02     11\n6 1999-01-01 05:00:00  3.96   160    74    29     5    14  3.88 0.725    NA\n\ntail(data.1999)\n\n# A tibble: 6 × 10\n  date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n1 1999-12-31 18:00:00  4.68   190   226    39    NA    29  5.46  2.38    23\n2 1999-12-31 19:00:00  3.96   180   202    37    NA    27  4.78  2.15    23\n3 1999-12-31 20:00:00  3.36   190   246    44    NA    30  5.88  2.45    23\n4 1999-12-31 21:00:00  3.72   220   231    35    NA    28  5.28  2.22    23\n5 1999-12-31 22:00:00  4.08   200   217    41    NA    31  4.79  2.17    26\n6 1999-12-31 23:00:00  3.24   200   181    37    NA    28  3.48  1.78    22\n\n## easier way\ndata.1999 <- selectByDate(mydata, year = 1999)\n\n## more complex use: select weekdays between the hours of 7 am to 7 pm\nsub.data <- selectByDate(mydata, day = \"weekday\", hour = 7:19)\n\n## select weekends between the hours of 7 am to 7 pm in winter (Dec, Jan, Feb)\nsub.data <- selectByDate(mydata, day = \"weekend\", hour = 7:19,\n                           month = c(\"dec\", \"jan\", \"feb\"))\n\nThe function can be used directly in other functions. For example, to make a polar plot using year 2000 data:\n\npolarPlot(selectByDate(mydata, year = 2000), pollutant = \"so2\")"
  },
  {
    "objectID": "utility-functions.html#sec-cutData",
    "href": "utility-functions.html#sec-cutData",
    "title": "25  Utility functions",
    "section": "\n25.2 Making intervals — cutData",
    "text": "25.2 Making intervals — cutData\nThe cutData function is a utility function that is called by most other functions but is useful in its own right. Its main use is to partition data in many ways, many of which are built-in to openair\nNote that all the date-based types e.g. month/year are derived from a column date. If a user already has a column with a name of one of the date-based types it will not be used.\nFor example, to cut data into seasons:\n\nmydata <- cutData(mydata, type = \"season\")\nhead(mydata)\n\n# A tibble: 6 × 11\n  date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n1 1998-01-01 00:00:00  0.6    280   285    39     1    29  4.72  3.37    NA\n2 1998-01-01 01:00:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n3 1998-01-01 02:00:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n4 1998-01-01 03:00:00  2.16   170   493    52     3    35  7.66 10.2     NA\n5 1998-01-01 04:00:00  2.4    180   468    78     2    34  8.07  8.91    NA\n6 1998-01-01 05:00:00  3      190   264    42     0    16  5.50  3.05    NA\n# … with 1 more variable: season <ord>\n\n\nThis adds a new field season that is split into four seasons. There is an option hemisphere that can be used to use southern hemisphere seasons when set as hemisphere = \"southern\".\nThe type can also be another field in a data frame e.g.\n\nmydata <- cutData(mydata, type = \"pm10\")\nhead(mydata)\n\n# A tibble: 6 × 11\n  date                   ws    wd   nox   no2    o3 pm10         so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <fct>      <dbl> <dbl> <int>\n1 1998-01-01 00:00:00  0.6    280   285    39     1 pm10 22 t…  4.72  3.37    NA\n2 1998-01-01 01:00:00  2.16   230    NA    NA    NA pm10 31 t… NA    NA       NA\n3 1998-01-01 02:00:00  2.76   190    NA    NA     3 pm10 31 t…  6.83  9.60    NA\n4 1998-01-01 03:00:00  2.16   170   493    52     3 pm10 31 t…  7.66 10.2     NA\n5 1998-01-01 04:00:00  2.4    180   468    78     2 pm10 31 t…  8.07  8.91    NA\n6 1998-01-01 05:00:00  3      190   264    42     0 pm10 1 to…  5.50  3.05    NA\n# … with 1 more variable: season <ord>\n\ndata(mydata) ## re-load mydata fresh\n\nThis divides PM10 concentrations into four quantiles — roughly equal numbers of PM10 concentrations in four levels.\nMost of the time users do not have to call cutData directly because most functions have a type option that is used to call cutData directly e.g.\n\npolarPlot(mydata, pollutant = \"so2\", type = \"season\")\n\nHowever, it can be useful to call cutData before supplying the data to a function in a few cases. First, if one wants to set seasons to the southern hemisphere as above. Second, it is possible to override the division of a numeric variable into four quantiles by using the option n.levels. More details can be found in the cutData help file."
  },
  {
    "objectID": "utility-functions.html#sec-selectRunning",
    "href": "utility-functions.html#sec-selectRunning",
    "title": "25  Utility functions",
    "section": "\n25.3 Selecting run lengths of values above a threshold — pollution episodes",
    "text": "25.3 Selecting run lengths of values above a threshold — pollution episodes\nA seemingly easy thing to do that has relevance to air pollution episodes is to select run lengths of contiguous values of a pollutant above a certain threshold. For example, one might be interested in selecting O3 concentrations where there are at least 8 consecutive hours above 90~ppb. In other words, a selection that combines both a threshold and persistence. These periods can be very important from a health perspective and it can be useful to study the conditions under which they occur. But how do you select such periods easily? The selectRunning utility function has been written to do this. It could be useful for all sorts of situations e.g.\n\nSelecting hours when primary pollutant concentrations are persistently high — and then applying other openair functions to analyse the data in more depth.\nIn the study of particle suspension or deposition etc. it might be useful to select hours when wind speeds remain high or rainfall persists for several hours to see how these conditions affect particle concentrations.\nIt could be useful in health impact studies to select blocks of data where pollutant concentrations remain above a certain threshold.\n\nAs an example we are going to consider O3 concentrations at a semi-rural site in south-west London (Teddington). The data can be downloaded as follows:\n\nted <- importKCL(site = \"td0\", year = 2005:2009, met = TRUE)\n## see how many rows there are\nnrow(ted)\n\n[1] 43824\n\n\nWe are going to contrast two pollution roses of O3 concentration. The first shows hours where the criterion is not met, and the second where it is met. The subset of hours is defined by O3 concentrations above 90 ppb for periods of at least 8-hours i.e. what might be considered as ozone episode conditions.\n\nted <- selectRunning(ted, pollutant = \"o3\", \n                         threshold = 90, \n                         run.len = 8)\n\nThe selectRunning function returns a new column criterion that flags whether the condition is met or not. The user can control the text provided, which by default is “yes” and “no”.\n\ntable(ted$criterion)\n\n\n   no   yes \n42425  1399 \n\n\nNow we are going to produce two pollution roses shown in Figure 25.1. Note, however that many other types of analysis could be carried out now the data have been partitioned.\n\npollutionRose(ted, pollutant = \"o3\", \n          type = \"criterion\")\n\n\n\nFigure 25.1: Example of using the selectRunning function to select episode hours to produce pollution roses of O3 concentration.\n\n\n\n\nThe results are shown in Figure 25.1. The pollution rose for for the “no” criterion (left plot of Figure 25.1 shows that the highest O3 concentrations tend to occur for wind directions from the south-west, where there is a high proportion of measurements. By contrast, the when the criterion is met (right plot of Figure 25.1 is very different. In this case there is a clear set of conditions where these criteria are met i.e. lengths of at least 8-hours where the O3 concentration is at least 90 ppb. It is clear the highest concentrations are dominated by south-easterly conditions i.e. corresponding to easterly flow from continental Europe where there has been time to the O3 chemistry to take place.\nThe code below shows (as an example), that the summer of 2006 had a high proportion of conditions where the criterion was met.\n\ntimeProp(ted, pollutant = \"o3\", \n         proportion = \"criterion\", \n         avg.time = \"month\", \n         cols = \"viridis\")\n\nIt is also useful to consider what controls the highest NOx concentrations at a central London roadside site. For example, the code below (not plotted) shows very strongly that the persistently highest NOx concentrations are dominated by south-westerly winds. As mentioned earlier, there are many other types of analysis that can be carried out now the data set identifies where the criterion is or is not met.\n\nepisode <- selectRunning(mydata, pollutant = \"nox\", \n                         threshold = 500, \n                         run.len = 5)\n\npollutionRose(episode, pollutant = \"nox\", type = \"criterion\")"
  },
  {
    "objectID": "utility-functions.html#sec-rollingMean",
    "href": "utility-functions.html#sec-rollingMean",
    "title": "25  Utility functions",
    "section": "\n25.4 Calculating rolling means",
    "text": "25.4 Calculating rolling means\nSome air pollution statistics such as for O3 and particulate matter are expressed as rolling means and it is useful to be able to calculate these. It can also be useful to help smooth-out data for clearer plotting. The rollingMean function makes these calculations. One detail that can be important is that for some statistics a mean is only considered valid if there are a sufficient number of valid readings over the averaging period. Often there is a requirement for at least 75% data capture. For example, with an averaging period of 8 hours and a data capture threshold of 75%, at least 6 hours are required to calculate the mean.\nThe function is called as follows; in this case to calculate 8-hour rolling mean concentrations of O3.\n\nmydata <- rollingMean(mydata, pollutant = \"o3\", width = 8,\n                       new.name = \"rollingo3\", data.thresh = 75)\ntail(mydata)\n\n# A tibble: 6 × 11\n  date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n  <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n1 2005-06-23 07:00:00   1.5   250   404   156     4    49    NA  1.81    28\n2 2005-06-23 08:00:00   1.5   260   388   145     6    48    NA  1.64    26\n3 2005-06-23 09:00:00   1.5   210   404   168     7    58    NA  1.29    34\n4 2005-06-23 10:00:00   2.6   240   387   175    10    55    NA  1.29    34\n5 2005-06-23 11:00:00   3.1   220   312   125    15    52    NA  1.29    33\n6 2005-06-23 12:00:00   3.1   220   287   119    17    55    NA  1.29    35\n# … with 1 more variable: rollingo3 <dbl>\n\n\nNote that calculating rolling means shortens the length of the data set. In the case of O3, no calculations are made for the last 7 hours.\nType ?rollingMean into R for more details. Note that the function currently only works with a single site."
  },
  {
    "objectID": "utility-functions.html#sec-timeAverage",
    "href": "utility-functions.html#sec-timeAverage",
    "title": "25  Utility functions",
    "section": "\n25.5 Aggregating data by different time intervals",
    "text": "25.5 Aggregating data by different time intervals\nAggregating data by different averaging periods is a common and important task. There are many reasons for aggregating data in this way:\n\nData sets may have different averaging periods and there is a need to combine them. For example, the task of combining an hourly air quality data set with a 15-minute average meteorological data set. The need here would be to aggregate the 15-minute data to 1-hour before merging.\nIt is extremely useful to consider data with different averaging times straightforwardly. Plotting a very long time series of hourly or higher resolution data can hide the main features and it would be useful to apply a specific (but flexible) averaging period to the data for plotting.\nThose who make measurements during field campaigns (particularly for academic research) may have many instruments with a range of different time resolutions. It can be useful to re-calculate time series with a common averaging period; or maybe help reduce noise.\nIt is useful to calculate statistics other than means when aggregating e.g. percentile values, maximums etc.\nFor statistical analysis there can be short-term autocorrelation present. Being able to choose a longer averaging period is sometimes a useful strategy for minimising autocorrelation.\n\nIn aggregating data in this way, there are a couple of other issues that can be useful to deal with at the same time. First, the calculation of proper vector-averaged wind direction is essential. Second, sometimes it is useful to set a minimum number of data points that must be present before the averaging is done. For example, in calculating monthly averages, it may be unwise to not account for data capture if some months only have a few valid points.\nWhen a data capture threshold is set through data.thresh it is necessary for timeAverage to know what the original time interval of the input time series is. The function will try and calculate this interval based on the most common time gap (and will print the assumed time gap to the screen). This works fine most of the time but there are occasions where it may not e.g. when very few data exist in a data frame. In this case the user can explicitly specify the interval through interval in the same format as avg.time e.g. interval = \"month\". It may also be useful to set start.date and end.date if the time series do not span the entire period of interest. For example, if a time series ended in October and annual means are required, setting end.date to the end of the year will ensure that the whole period is covered and that data.thresh is correctly calculated. The same also goes for a time series that starts later in the year where start.date should be set to the beginning of the year.\nAll these issues are (hopefully) dealt with by the timeAverage function. The options are shown below, but as ever it is best to check the help that comes with the openair package.\nTo calculate daily means from hourly (or higher resolution) data:\n\ndaily <- timeAverage(mydata, avg.time = \"day\")\ndaily\n\n# A tibble: 2,731 × 11\n   date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n   <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 1998-01-01 00:00:00  6.84  188.  154.  39.4 6.87   18.2  3.15  2.70   NaN\n 2 1998-01-02 00:00:00  7.07  223.  132.  39.5 6.48   27.8  3.94  1.77   NaN\n 3 1998-01-03 00:00:00 11.0   226.  120.  38.0 8.41   20.2  3.20  1.74   NaN\n 4 1998-01-04 00:00:00 11.5   223.  105.  35.3 9.61   21.0  2.96  1.62   NaN\n 5 1998-01-05 00:00:00  6.61  237.  175.  46.0 4.96   24.2  4.52  2.13   NaN\n 6 1998-01-06 00:00:00  4.38  197.  214.  45.3 1.35   34.6  5.70  2.53   NaN\n 7 1998-01-07 00:00:00  7.61  219.  193.  44.9 4.42   31.0  5.67  2.48   NaN\n 8 1998-01-08 00:00:00  8.58  216.  161.  43.1 4.96   36    4.68  2.10   NaN\n 9 1998-01-09 00:00:00  6.7   206.  163.  38   3.62   38.0  5.13  2.36   NaN\n10 1998-01-10 00:00:00  2.98  167.  219.  44.9 0.375  37.0  4.91  2.23   NaN\n# … with 2,721 more rows, and 1 more variable: rollingo3 <dbl>\n\n\nMonthly 95th percentile values:\n\nmonthly <- timeAverage(mydata, avg.time = \"month\", \n                       statistic = \"percentile\",\n                       percentile = 95)\nmonthly\n\n# A tibble: 90 × 11\n   date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n   <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 1998-01-01 00:00:00 11.2   45    371   68.6  14    53    11.1  3.99  NA  \n 2 1998-02-01 00:00:00  8.16  16.7  524.  92     7    68.9  17.5  5.63  NA  \n 3 1998-03-01 00:00:00 10.6   37.6  417.  85    15    61    18.4  4.85  NA  \n 4 1998-04-01 00:00:00  8.16  44.4  384   81.5  20    52    14.6  4.17  NA  \n 5 1998-05-01 00:00:00  7.56  40.6  300   80    25    61    12.7  3.55  40  \n 6 1998-06-01 00:00:00  8.47  50.7  377   74.2  15    53    12.2  4.28  33.9\n 7 1998-07-01 00:00:00  9.22  36.7  386.  80.0  NA    52.4  13.9  4.52  32  \n 8 1998-08-01 00:00:00  7.92  48.4  337.  87.0  16    58.2  13.0  3.78  38  \n 9 1998-09-01 00:00:00  6     66.7  334.  81.3  14    64    18.2  4.25  47  \n10 1998-10-01 00:00:00 12     33.9  439.  84    15.1  54    12.0  4.81  33  \n# … with 80 more rows, and 1 more variable: rollingo3 <dbl>\n\n\n2-week averages but only calculate if at least 75% of the data are available:\n\ntwoweek <- timeAverage(mydata, avg.time = \"2 week\", \n                       data.thresh = 75)\ntwoweek\n\n# A tibble: 196 × 11\n   date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n   <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 1997-12-29 00:00:00  6.98  212.  167.  41.4  4.63  29.3  4.47  2.17  NA  \n 2 1998-01-12 00:00:00  4.91  221.  173.  42.1  4.70  28.8  5.07  1.86  NA  \n 3 1998-01-26 00:00:00  2.78  242.  233.  51.4  2.30  34.9  8.07  2.45  NA  \n 4 1998-02-09 00:00:00  4.43  215.  276.  57.1  2.63  43.7  8.98  2.94  NA  \n 5 1998-02-23 00:00:00  6.89  237.  248.  56.7  4.99  28.8  9.79  2.57  NA  \n 6 1998-03-09 00:00:00  2.97  288.  160.  44.8  5.64  32.7  8.65  1.62  NA  \n 7 1998-03-23 00:00:00  4.87  192.  224.  53.6  5.52  35.9 10.2   2.34  NA  \n 8 1998-04-06 00:00:00  3.24  294.  144.  43.4 10.1   23.8  5.48  1.40  NA  \n 9 1998-04-20 00:00:00  4.38  195.  177.  47.6 10.5   31.4  5.54  1.73  NA  \n10 1998-05-04 00:00:00  3.97  285.  134.  45.5 10.2   38.6  5.49  1.41  24.6\n# … with 186 more rows, and 1 more variable: rollingo3 <dbl>\n\n\nNote that timeAverage has a type option to allow for the splitting of variables by a grouping variable. The most common use for type is when data are available for different sites and the averaging needs to be done on a per site basis.\nFirst, retaining by site averages:\n\n# import some data for two sites\ndat <- importAURN(c(\"kc1\", \"my1\"), year = 2011:2013)\n\n# annual averages by site\ntimeAverage(dat, avg.time = \"year\", type = \"site\")\n\n# A tibble: 6 × 17\n# Groups:   site [2]\n  site       date                   co   nox   no2    no    o3   so2  pm10 pm2.5\n  <fct>      <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 London Ma… 2011-01-01 00:00:00 0.656 306.   97.2 137.   18.5  6.86  38.4  24.5\n2 London Ma… 2012-01-01 00:00:00 0.589 313.   94.0 143.   15.0  8.13  30.8  21.5\n3 London Ma… 2013-01-01 00:00:00 0.506 281.   84.7 128.   17.7  5.98  29.1  20.1\n4 London N.… 2011-01-01 00:00:00 0.225  53.8  36.1  11.6  39.4  2.06  23.7  16.3\n5 London N.… 2012-01-01 00:00:00 0.266  57.4  36.7  13.3  38.5  2.03  20.2  14.6\n6 London N.… 2013-01-01 00:00:00 0.250  57.9  36.9  13.7  38.4  2.01  23.1  14.7\n# … with 7 more variables: v10 <dbl>, v2.5 <dbl>, nv10 <dbl>, nv2.5 <dbl>,\n#   ws <dbl>, wd <dbl>, air_temp <dbl>\n\n\nRetain site name and site code:\n\n# can also retain site code\ntimeAverage(dat, avg.time = \"year\", type = c(\"site\", \"code\"))\n\n# A tibble: 6 × 18\n# Groups:   site, code [2]\n  site       code  date                   co   nox   no2    no    o3   so2  pm10\n  <fct>      <fct> <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 London Ma… MY1   2011-01-01 00:00:00 0.656 306.   97.2 137.   18.5  6.86  38.4\n2 London Ma… MY1   2012-01-01 00:00:00 0.589 313.   94.0 143.   15.0  8.13  30.8\n3 London Ma… MY1   2013-01-01 00:00:00 0.506 281.   84.7 128.   17.7  5.98  29.1\n4 London N.… KC1   2011-01-01 00:00:00 0.225  53.8  36.1  11.6  39.4  2.06  23.7\n5 London N.… KC1   2012-01-01 00:00:00 0.266  57.4  36.7  13.3  38.5  2.03  20.2\n6 London N.… KC1   2013-01-01 00:00:00 0.250  57.9  36.9  13.7  38.4  2.01  23.1\n# … with 8 more variables: pm2.5 <dbl>, v10 <dbl>, v2.5 <dbl>, nv10 <dbl>,\n#   nv2.5 <dbl>, ws <dbl>, wd <dbl>, air_temp <dbl>\n\n\nAverage all data across sites (drops site and code):\n\ntimeAverage(dat, avg.time = \"year\")\n\n# A tibble: 3 × 16\n  date                   co   nox   no2    no    o3   so2  pm10 pm2.5   v10\n  <dttm>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 2011-01-01 00:00:00 0.439  181.  67.1  74.9  31.5  4.31  31.4  20.5  5.40\n2 2012-01-01 00:00:00 0.424  182.  64.7  76.7  26.9  5.08  25.6  18.1  4.23\n3 2013-01-01 00:00:00 0.378  169.  60.7  70.7  28.0  3.79  26.8  17.4  4.29\n# … with 6 more variables: v2.5 <dbl>, nv10 <dbl>, nv2.5 <dbl>, ws <dbl>,\n#   wd <dbl>, air_temp <dbl>\n\n\ntimeAverage also works the other way in that it can be used to derive higher temporal resolution data e.g. hourly from daily data or 15-minute from hourly data. An example of usage would be the combining of daily mean particle data with hourly meteorological data. There are two ways these two data sets can be combined: either average the meteorological data to daily means or calculate hourly means from the particle data. The timeAverage function when used to ‘expand’ data in this way will repeat the original values the number of times required to fill the new time scale. In the example below we calculate 15-minute data from hourly data. As it can be seen, the first line is repeated four times and so on.\n\ndata15 <- timeAverage(mydata, \n                      avg.time = \"15 min\", \n                      fill = TRUE)\nhead(data15, 20)\n\n# A tibble: 20 × 11\n   date                   ws    wd   nox   no2    o3  pm10   so2    co  pm25\n   <dttm>              <dbl> <int> <int> <int> <int> <int> <dbl> <dbl> <int>\n 1 1998-01-01 00:00:00  0.6    280   285    39     1    29  4.72  3.37    NA\n 2 1998-01-01 00:15:00  0.6    280   285    39     1    29  4.72  3.37    NA\n 3 1998-01-01 00:30:00  0.6    280   285    39     1    29  4.72  3.37    NA\n 4 1998-01-01 00:45:00  0.6    280   285    39     1    29  4.72  3.37    NA\n 5 1998-01-01 01:00:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n 6 1998-01-01 01:15:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n 7 1998-01-01 01:30:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n 8 1998-01-01 01:45:00  2.16   230    NA    NA    NA    37 NA    NA       NA\n 9 1998-01-01 02:00:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n10 1998-01-01 02:15:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n11 1998-01-01 02:30:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n12 1998-01-01 02:45:00  2.76   190    NA    NA     3    34  6.83  9.60    NA\n13 1998-01-01 03:00:00  2.16   170   493    52     3    35  7.66 10.2     NA\n14 1998-01-01 03:15:00  2.16   170   493    52     3    35  7.66 10.2     NA\n15 1998-01-01 03:30:00  2.16   170   493    52     3    35  7.66 10.2     NA\n16 1998-01-01 03:45:00  2.16   170   493    52     3    35  7.66 10.2     NA\n17 1998-01-01 04:00:00  2.4    180   468    78     2    34  8.07  8.91    NA\n18 1998-01-01 04:15:00  2.4    180   468    78     2    34  8.07  8.91    NA\n19 1998-01-01 04:30:00  2.4    180   468    78     2    34  8.07  8.91    NA\n20 1998-01-01 04:45:00  2.4    180   468    78     2    34  8.07  8.91    NA\n# … with 1 more variable: rollingo3 <dbl>\n\n\nThe timePlot can apply this function directly to make it very easy to plot data with different averaging times and statistics."
  },
  {
    "objectID": "utility-functions.html#sec-calcPercentile",
    "href": "utility-functions.html#sec-calcPercentile",
    "title": "25  Utility functions",
    "section": "\n25.6 Calculating percentiles",
    "text": "25.6 Calculating percentiles\ncalcPercentile makes it straightforward to calculate percentiles for a single pollutant. It can take account of different averaging periods, data capture thresholds — see Section 25.5 for more details.\nFor example, to calculate the 25, 50, 75 and 95th percentiles of O3 concentration by year:\n\ncalcPercentile(mydata, pollutant  = \"o3\", \n               percentile = c(25, 50, 75, 95),\n               avg.time = \"year\")\n\n        date percentile.25 percentile.50 percentile.75 percentile.95\n1 1998-01-01             2             4             7            16\n2 1999-01-01             2             4             9            21\n3 2000-01-01             2             4             9            22\n4 2001-01-01             2             4            10            24\n5 2002-01-01             2             4            10            24\n6 2003-01-01             2             4            11            24\n7 2004-01-01             2             5            11            23\n8 2005-01-01             3             7            16            28"
  },
  {
    "objectID": "utility-functions.html#sec-corPlot",
    "href": "utility-functions.html#sec-corPlot",
    "title": "25  Utility functions",
    "section": "\n25.7 Correlation matrices",
    "text": "25.7 Correlation matrices\nUnderstanding how different variables are related to one another is always important. However, it can be difficult to easily develop an understanding of the relationships when many different variables are present. One of the useful techniques used is to plot a correlation matrix, which provides the correlation between all pairs of data. The basic idea of a correlation matrix has been extended to help visualise relationships between variables by (Friendly 2002) and (Sarkar 2007).\n\nFriendly, M. 2002. “Corrgrams: Exploratory Displays for Correlation Matrices.” The American Statistician 56 (4): 316–25.\n\nSarkar, Deepayan. 2007. Lattice Multivariate Data Visualization with R. New York: Springer.\nThe corPlot shows the correlation coded in three ways: by shape (ellipses), colour and the numeric value. The ellipses can be thought of as visual representations of scatter plot. With a perfect positive correlation a line at 45 degrees positive slope is drawn. For zero correlation the shape becomes a circle — imagine a ‘fuzz’ of points with no relationship between them.\nWith many variables it can be difficult to see relationships between variables i.e. which variables tend to behave most like one another. For this reason hierarchical clustering is applied to the correlation matrices to group variables that are most similar to one another (if cluster = TRUE.)\nIt is also possible to use the openair type option to condition the data in many flexible ways, although this may become difficult to visualise with too many panels.\nAn example of the corPlot function is shown in Figure 25.2. In this Figure it can be seen the highest correlation coefficient is between PM10 and PM2.5 (r = 0. 84) and that the correlations between SO2, NO2 and NOx are also high. O3 has a negative correlation with most pollutants, which is expected due to the reaction between NO and O3. It is not that apparent in Figure 25.2 that the order the variables appear is due to their similarity with one another, through hierarchical cluster analysis. In this case we have chosen to also plot a dendrogram that appears on the right of the plot. Dendrograms provide additional information to help with visualising how groups of variables are related to one another. Note that dendrograms can only be plotted for type = \"default\" i.e. for a single panel plot.\n\ncorPlot(mydata, dendrogram = TRUE)\n\n\n\nFigure 25.2: Example of a correlation matrix showing the relationships between variables.\n\n\n\n\nNote also that the corPlot accepts a type option, so it possible to condition the data in many flexible ways, although this may become difficult to visualise with too many panels. For example:\n\ncorPlot(mydata, type = \"season\")\n\nWhen there are a very large number of variables present, the corPlot is a very effective way of quickly gaining an idea of how variables are related. As an example (not plotted) it is useful to consider the hydrocarbons measured at Marylebone Road. There is a lot of information in the hydrocarbon plot (about 40 species), but due to the hierarchical clustering it is possible to see that isoprene, ethane and propane behave differently to most of the other hydrocarbons. This is because they have different (non-vehicle exhaust) origins. Ethane and propane results from natural gas leakage whereas isoprene is biogenic in origin (although some is from vehicle exhaust too). It is also worth considering how the relationships change between the species over the years as hydrocarbon emissions are increasingly controlled, or maybe the difference between summer and winter blends of fuels and so on.\n\nhc <- importAURN(site = \"my1\", year = 2005, hc = TRUE)\n## now it is possible to see the hydrocarbons that behave most\n## similarly to one another\ncorPlot(hc)"
  },
  {
    "objectID": "scatter-plot.html#sec-ScatterPurpose",
    "href": "scatter-plot.html#sec-ScatterPurpose",
    "title": "26  Scatter plots",
    "section": "\n26.1 Purpose",
    "text": "26.1 Purpose\nScatter plots are extremely useful and a very commonly used analysis technique for considering how variables relate to one another. R does of course have many capabilities for plotting data in this way. However, it can be tricky to add linear relationships, or split scatter plots by levels of other variables etc. The purpose of the scatterPlot function is to make it straightforward to consider how variables are related to one another in a way consistent with other openair functions. We have added several capabilities that can be used just by setting different options, some of which are shown below.\nThere is less need for this function now that ggplot2 is available, but it still has some benefits for intercative use.\n\nA smooth fit is automatically added to help reveal the underlying relationship between two variables together with the estimated 95% confidence intervals of the fit. This is in general an extremely useful thing to do because it helps to show the (possibly) non-linear relationship between variables in a very robust way — or indeed whether the relationship is linear.\nIt is easy to add a linear regression line. The resulting equation is shown on the plot together with the R\\(^2\\) value.\nFor large data sets there is the possibility to `bin’ the data using hexagonal binning or kernel density estimates. This approach is very useful when there is considerable over-plotting.\nIt is easy to show how two variables are related to one another dependent on levels of a third variable. This capability is very useful for exploring how different variables depend on one another and can help reveal the underlying important relationships.\nA plot of two variables can be colour-coded by a continuous colour scale of a third variable.\nIt can handle date/time x-axis formats to provide an alternative way of showing time series, which again can be colour-coded by a third variable.\n\nThe scatterPlot function isn’t really specific to atmospheric sciences, in the same way as other plots. It is more a function for convenience, written in a style that is consistent with other openair functions. Nevertheless, along with the timePlot function they do form an important part of openair because of the usefulness of understanding show variables relate to one another. Furthermore, there are many options to make it easy to explore data in an interactive way without worrying about processing data or formatting plots."
  },
  {
    "objectID": "scatter-plot.html#sec-ScatterEx",
    "href": "scatter-plot.html#sec-ScatterEx",
    "title": "26  Scatter plots",
    "section": "\n26.2 Examples",
    "text": "26.2 Examples\nWe provide a few examples of use and as usual, users are directed towards the help pages (type ?scatterPlot) for more extensive examples.\nFirst we select a subset of data (2003) using the openair selectByDate function and plot NOx vs. NO2\n\nlibrary(openair) # load the package\n\ndata2003 <- selectByDate(mydata, year = 2003)\nscatterPlot(data2003, x = \"nox\", y = \"no2\")\n\n\n\nFigure 26.1: Scatter plot of hourly NOx vs. NO2 at Marylebone Road for 2003.\n\n\n\n\nOften with several years of data, points are over-plotted and it can be very difficult to see what the underlying relationship looks like. One very effective method to use in these situations is to ‘bin’ the data and to colour the intervals by the number of counts of occurrences in each bin. There are various ways of doing this, but ‘hexagonal binning’ is particularly effective because of the way hexagons can be placed next to one another.1 To use hexagonal binning it will be necessary to install the hexbin package:\n\n26.2.1 Hexaganol binning\nNow it should be possible to make the plot by setting the method option to method = \"hexbin\", as shown in Figure @ref(fig:scatterPlot2). The benefit of hexagonal binning is that it works equally well with enormous data sets e.g. several million records. In this case Figure 26.2 provides a clearer indication of the relationship between NOx and NO2 than Figure 26.1 because it reveals where most of the points lie, which is not apparent from Figure 26.1. Note that For method = \"hexbin\" it can be useful to transform the scale if it is dominated by a few very high values. This is possible by supplying two functions: one that that applies the transformation and the other that inverses it. For log scaling for example (the default), trans = function(x) log(x) and inv = function(x) exp(x). For a square root transform use trans = sqrt and inv =  function(x) x^2. To not apply any transformation trans = NULL and inv = NULL should be used.\n\nscatterPlot(data2003, x = \"nox\", y = \"no2\", method = \"hexbin\", col= \"turbo\")\n\n\n\nFigure 26.2: Scatter plot of hourly NOx vs. NO2 at Marylebone Road using hexagonal binning. The number of occurrences in each bin is colour-coded (not on a linear scale). It is now possible to see where most of the data lie and a better indication of the relationship between NOx and NO2 is revealed.\n\n\n\n\nNote that when method = \"hexbin\" there are various options that are useful e.g. a border around each bin and the number of bins. For example, to place a grey border around each bin and set the bin size try:\n\nscatterPlot(mydata, x = \"nox\", y = \"no2\", \n            method = \"hexbin\", col = \"turbo\",\n            border = \"grey\", xbin = 15)\n\nThe hexagonal binning and other binning methods are useful but often the choice of bin size is somewhat arbitrary. Another useful approach is to use a kernel density estimate to show where most points lie. This is possible in scatterPlot with the method =   \"density\" option. Such a plot is shown in Figure 26.3.\n\nscatterPlot(selectByDate(mydata, year = 2003),\n            x = \"nox\", y = \"no2\",\n            method = \"density\", \n            cols = \"turbo\")\n\n\n\nFigure 26.3: Scatter plot of hourly NOx vs. NO2 at Marylebone Road using a kernel density estimate to show where most of the points lie. The intensity is a measure of how many points lie in a unit area of NOx and NO2 concentration.\n\n\n\n\nSometimes it is useful to consider how the relationship between two variables varies by levels of a third. In openair this approach is possible by setting the option type. When type is another numeric variables, four plots are produced for different quantiles of that variable. We illustrate this point by considering how the relationship between NOx and NO2 varies with different levels of O3. We also take the opportunity to not plot the smooth line, but plot a linear fit instead and force the layout to be a 2 by 2 grid.\n\nscatterPlot(data2003, x = \"nox\", y = \"no2\", \n            type = \"o3\", smooth = FALSE,\n            linear = TRUE, layout = c(2, 2))\n\n\n\nFigure 26.4: Scatter plot of hourly NOx vs. NO2 at Marylebone Road by different levels of O3.\n\n\n\n\nBelow is an extended example that brings together data manipulation, refined plot options and linear fitting of two variables with NOx. The aim is to plot the weekly concentration of NOx against PM10 and PM2.5 and fit linear equations to both relationships. To do this we need the \\(x\\) variable as NOx and the \\(y\\) variable as PM10 or PM2.5, which means we also need a column that will act as a grouping column i.e. identifies whether the \\(y\\) is PM10 or PM2.5.\n\n# load the packages we need\nlibrary(tidyverse)\n\n# select the variables of interest\nsubdat <- select(mydata, date, nox, pm10, pm25) # calculate weekly averages\nsubdat <- timeAverage(subdat, avg.time = \"week\")\n\n# reshape so we have two variable columns\nsubdat <- pivot_longer(subdat, cols = c(pm10, pm25), \n                       names_to = \"pollutant\")\nhead(subdat)\n\n# A tibble: 6 × 4\n  date                  nox pollutant value\n  <dttm>              <dbl> <chr>     <dbl>\n1 1997-12-29 00:00:00  128. pm10       21.8\n2 1997-12-29 00:00:00  128. pm25      NaN  \n3 1998-01-05 00:00:00  189. pm10       33.6\n4 1998-01-05 00:00:00  189. pm25      NaN  \n5 1998-01-12 00:00:00  203. pm10       29.1\n6 1998-01-12 00:00:00  203. pm25      NaN  \n\n\nNow we will plot weekly NOx versus PM10 and PM2.5 and fit a linear equation to both — and adjust some of the symbols (shown in Figure 26.5).\n\nscatterPlot(subdat, x = \"nox\", y = \"value\", \n            group = \"pollutant\", \n            pch = 21:22, cex = 1.6, \n            fill = c(\"dodgerblue\", \"tomato\"), \n            col = \"white\", \n            linear = TRUE, \n            xlab = \"nox (ppb)\", \n            ylab = \"PM concentration (ug/m3)\")\n\n\n\nFigure 26.5: Scatter plot of weekly NOx vs. PM10and PM2.5 at Marylebone Road with linear equations shown and plot symbols modified.\n\n\n\n\nTo gain a better idea of where the data lie and the linear fits, adding some transparency helps:\n\nscatterPlot(subdat, x = \"nox\", y = \"value\", \n            group = \"variable\", \n            pch = 21:22, cex = 1.6, \n            fill = c(\"dodgerblue\", \"tomato\"), \n            col = \"white\", \n            linear = TRUE, \n            xlab = \"nox (ppb)\", \n            ylab = \"PM concentration (ug/m3)\",\n            alpha = 0.2)\n\nThe above example will also work with type. For example, to consider how NOx againts PM10 and PM2.5 varies by season:\n\nscatterPlot(subdat, x = \"nox\", y = \"value\", \n            group = \"variable\", \n            pch = 21:22, cex = 2, \n            fill = c(\"dodgerblue\", \"tomato\"), \n            col = \"white\", linear = TRUE, \n            xlab = \"nox (ppb)\", \n            ylab = \"PM concentration (ug/m3)\", \n            type = \"season\")\n\nFinally, we show how to plot a continuous colour scale for a third numeric variable setting the value of z to the third variable. Figure 26.6 shows again the relationship between NOx and NO2 but this time colour-coded by the concentration of O3. We also take the opportunity to split the data into seasons and weekday/weekend by setting type =   c(\"season\", \"weekend\"). There is an enormous amount of information that can be gained from plots such as this. Differences between weekdays and the weekend can highlight changes in emission sources, splitting by seasons can show seasonal influences in meteorology and background O3 and colouring the data by the concentration of O3 helps to show how O3 concentrations affect NO2 concentrations. For example, consider the summertime-weekday panel where it clearly shows that the higher NO2 concentrations are associated with high O3 concentrations. Indeed there are some hours where NO2 is >100 ppb at quite low concentrations of NOx (\\(\\approx\\) 200 ppb). It would also be interesting instead of using O3 concentrations from Marylebone Road to use O3 from a background site.\nFigure 26.6 was very easily produced but contains a huge amount of useful information showing the relationship between NOx and NO2 dependent upon the concentration of O3, the season and the day of the week. There are of course numerous other plots that are equally easily produced.\n\nscatterPlot(data2003, \n            x = \"nox\", y = \"no2\", z = \"o3\", \n            type = c(\"season\", \"weekend\"),\n            limits = c(0, 30))\n\n\n\nFigure 26.6: Scatter plot of hourly NOx vs. NO2 at Marylebone Road by different levels of O3 split by season and weekday-weekend.\n\n\n\n\nFigure 26.7 shows that scatterPlot can also handles dates on the x-axis; in this case shown for SO2 concentrations coloured by wind direction for August 2003.\n\nscatterPlot(selectByDate(data2003, month = 8), \n            x = \"date\", y = \"so2\",\n             z = \"wd\")\n\n\n\nFigure 26.7: Scatter plot of date vs. SO2- at Marylebone Road by different levels of wind direction for August 2003.\n\n\n\n\nSimilar to Chapter 11, scatterPlot can also plot wind vector arrows if wind speed and wind direction are available in the data frame. Figure 26.8 shows an example of using the windflow option. The Figure also sets many other options including showing the concentration of O3 as a colour, setting the colour scale used and selecting a few days of interest using the selectByDate function. Figure 26.8 shows that when the wind direction changes to northerly, the concentration of NO2 decreases and that of O3 increases.\n\nscatterPlot(selectByDate(mydata, start = \"1/6/2001\", \n                         end = \"5/6/2001\"), \n            x = \"date\", y = \"no2\", z = \"o3\", \n            col = \"increment\", \n            windflow = list(scale = 0.15), \n            key.footer = \"o3\\n (ppb)\", \n            main = NULL, ylab = \"no2 (ppb)\")\n\n\n\nFigure 26.8: Scatter plot of date vs. NO2 with the colour scale representing O3. The wind flow vectors are also shown."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Applequist, Scott. 2012. “Wind Rose Bias Correction.”\nJournal of Applied Meteorology and Climatology 51 (7): 1305–9.\n\n\nAra Begum, Bilkis, Eugene Kim, Cheol-Heon Jeong, Doh-Won Lee, and Philip\nK. Hopke. 2005. “Evaluation of the potential\nsource contribution function using the 2002 Quebec forest fire\nepisode.” Atmospheric Environment 39 (20):\n3719–24. https://doi.org/10.1016/j.atmosenv.2005.03.008.\n\n\nAshbaugh, Lowell L., William C. Malm, and Willy Z. Sadeh. 1985.\n“A residence time probability analysis of\nsulfur concentrations at grand Canyon National Park.”\nAtmospheric Environment (1967) 19 (8): 1263–70. https://doi.org/10.1016/0004-6981(85)90256-2.\n\n\nBrook, Jeffrey R., David Johnson, and Alexandre Mamedov. 2004.\n“Determination of the Source Areas Contributing to Regionally High\nWarm Season PM2.5in Eastern North\nAmerica.” Journal of the Air & Waste Management\nAssociation 54 (9): 1162–69. https://doi.org/10.1080/10473289.2004.10470984.\n\n\nCarslaw, D. C., and S. D. Beevers. 2013. “Characterising and\nUnderstanding Emission Sources Using Bivariate Polar Plots and k-Means\nClustering.” Environmental Modelling & Software 40\n(0): 325–29. https://doi.org/10.1016/j.envsoft.2012.09.005.\n\n\nCarslaw, D. C., S. D. Beevers, K. Ropkins, and M. C. Bell. 2006.\n“Detecting and Quantifying Aircraft and Other on-Airport\nContributions to Ambient Nitrogen Oxides in the Vicinity of a Large\nInternational Airport.” Atmospheric Environment 40 (28):\n5424–34.\n\n\nCarslaw, D. C., S. D. Beevers, and J. E. Tate. 2007. “Modelling\nand Assessing Trends in Traffic-Related Emissions Using a Generalised\nAdditive Modelling Approach.” Atmospheric Environment 41\n(26): 5289–99.\n\n\nChatfield, Christopher. 2004. The Analysis of Time Series : An\nIntroduction / Chris Chatfield. 6th ed. Boca Raton, FL ; London :\nChapman & Hall/CRC.\n\n\nCOMEAP. 2011. “Review of the UK Air Quality Index: A Report by the\nCommittee on the Medical Effects of Air Pollutants.” http://comeap.org.uk/documents/reports/130-review-of-the-uk-air-quality-index.html.\n\n\nDroppo, James G, and Bruce A Napier. 2008. “Wind Direction Bias in\nGenerating Wind Roses and Conducting Sector-Based Air Dispersion\nModeling.” Journal of the Air & Waste Management\nAssociation 58 (7): 913–18.\n\n\nFleming, Z. L., P. S. Monks, and A. J. Manning. 2012. “Review: Untangling the influence of air-mass history in\ninterpreting observed atmospheric composition.”\nAtmospheric Research 104-105: 1–39. https://doi.org/10.1016/j.atmosres.2011.09.009.\n\n\nFriendly, M. 2002. “Corrgrams: Exploratory\nDisplays for Correlation Matrices.” The American\nStatistician 56 (4): 316–25.\n\n\nGrange, Stuart K, Alastair C Lewis, and David C Carslaw. 2016.\n“Source Apportionment Advances Using Polar Plots of Bivariate\nCorrelation and Regression Statistics.” Atmospheric\nEnvironment 145: 128–34.\n\n\nHastie, T. J., and R. J. Tibshirani. 1990. Generalized Additive\nModels. London: Chapman; Hall.\n\n\nHenry, Ronald, Gary A. Norris, Ram Vedantham, and Jay R. Turner. 2009.\n“Source Region Identification Using Kernel\nSmoothing.” {Article}. Environmental Science\n& Technology 43 (11): 4090–97.\nhttps://doi.org/{10.1021/es8011723}.\n\n\nHirsch, R. M., J. R. Slack, and R. A. Smith. 1982. “Techniques of\nTrend Analysis for Monthly Water-Quality Data.” Water\nResources Research 18 (1): 107–21.\n\n\nHsu, Ying-Kuang, Thomas M. Holsen, and Philip K. Hopke. 2003.\n“Comparison of hybrid receptor models to\nlocate PCB sources in Chicago.” Atmospheric\nEnvironment 37 (4): 545–62. https://doi.org/10.1016/S1352-2310(02)00886-5.\n\n\nKunsch, H. R. 1989. “The Jackknife and the Bootstrap for General\nStationary Observations.” Annals of Statistics 17 (3):\n1217–41.\n\n\nLegates, D. R., and G. J. McCabe. 2012. “A Refined Index of Model\nPerformance: A Rejoinder.” International Journal of\nClimatology.\n\n\nLegates, D. R., and G. J. McCabe Jr. 1999. “Evaluating the Use of\n‘Goodness-of-Fit’ Measures in Hydrologic and Hydroclimatic\nModel Validation.” Water Resources Research 35 (1):\n233–41.\n\n\nLupu, Alexandru, and Willy Maenhaut. 2002. “Application and comparison of two statistical trajectory\ntechniques for identification of source regions of atmospheric aerosol\nspecies.” Atmospheric Environment 36: 5607–18.\n\n\nMasiol, Mauro, Stefania Squizzato, Meng-Dawn Cheng, David Q. Rich, and\nPhilip K. Hopke. 2019. “Differential Probability Functions for\nInvestigating Long-Term Changes in Local and Regional Air Pollution\nSources.” Aerosol and Air Quality Research 19 (4):\n724–36. https://doi.org/10.4209/aaqr.2018.09.0327.\n\n\nPekney, Natalie J., Cliff I. Davidson, Liming Zhou, and Philip K. Hopke.\n2006. “Application of PSCF and CPF to\nPMF-Modeled Sources of PM 2.5 in Pittsburgh.” Aerosol\nScience and Technology 40 (10): 952–61. https://doi.org/10.1080/02786820500543324.\n\n\nSarkar, Deepayan. 2007. Lattice Multivariate Data Visualization with\nR. New York: Springer.\n\n\nSeibert, P, H Kromp-Kolb, U Baltensperger, and DT Jost. 1994.\n“Trajectory Analysis of High-Alpine Air Pollution Data.”\nNATO Challenges of Modern Society 18: 595–95.\n\n\nSen, P. K. 1968. “Estimates of Regression Coefficient Based on\nKendall’s Tau.” Journal of the American Statistical\nAssociation 63(324): 1379–89.\n\n\nStein, A. F., R. R. Draxler, G. D. Rolph, B. J. B. Stunder, M. D. Cohen,\nand F. Ngan. 2015. “NOAA’s HYSPLIT Atmospheric\nTransport and Dispersion Modeling System.” Bulletin of the\nAmerican Meteorological Society 96 (12): 2059–77. https://doi.org/10.1175/bams-d-14-00110.1.\n\n\nTaylor, K. E. 2001. “Summarizing Multiple Aspects of Model\nPerformance in a Single Diagram.” Journal of Geophysical\nResearch 106 (D7): 7183–92.\n\n\nTheil, H. 1950. “A Rank Invariant Method of Linear and Polynomial\nRegression Analysis, i, II, III.” Proceedings of the\nKoninklijke Nederlandse Akademie Wetenschappen, Series A – Mathematical\nSciences 53: 386–92, 521–25, 1397–1412.\n\n\nUria-Tellaetxe, I, and D. C. Carslaw. 2014. “Conditional Bivariate\nProbability Function for Source Identification.”\nEnvironmental Modelling & Software 59: 1–9. https://doi.org/10.1016/j.envsoft.2014.05.002.\n\n\nWestmoreland, E. J., N Carslaw, D. C. Carslaw, A. Gillah, and E. Bates.\n2007. “Analysis of Air Quality Within a Street Canyon Using\nStatistical and Dispersion Modelling Techniques.” Atmospheric\nEnvironment 41 (39): 9195–205.\n\n\nWilcox, Rand R. 2010. Fundamentals of Modern\nStatistical Methods: Substantially Improving Power and\nAccuracy. 2nd ed. Springer New York. http://www.springerlink.com/content/978-1-4419-5524-1.\n\n\nWilks, Daniel S. 2005. Statistical Methods in\nthe Atmospheric Sciences, Volume 91, Second Edition (International\nGeophysics). 2nd ed. Hardcover; Academic Press.\n\n\nWillmott, Cort J, Scott M Robeson, and Kenji Matsuura. 2011. “A\nRefined Index of Model Performance.” International Journal of\nClimatology.\n\n\nWood, S. N. 2006. Generalized Additive Models: An Introduction with\nr. Chapman; Hall/CRC.\n\n\nWu, Cheng, and Jian Zhen Yu. 2018. “Evaluation of Linear\nRegression Techniques for Atmospheric Applications: The Importance of\nAppropriate Weighting.” Atmospheric Measurement\nTechniques 11 (2): 1233–50. https://doi.org/10.5194/amt-11-1233-2018.\n\n\nYork, Derek. 1968. “Least Squares Fitting of a Straight Line with\nCorrelated Errors.” Earth and Planetary Science Letters\n5 (January): 320–24. https://doi.org/10.1016/s0012-821x(68)80059-7.\n\n\nYork, Derek, Norman M. Evensen, Margarita López Martıńez, and Jonás De\nBasabe Delgado. 2004. “Unified Equations for the Slope, Intercept,\nand Standard Errors of the Best Straight Line.” American\nJournal of Physics 72 (3): 367–75. https://doi.org/10.1119/1.1632486.\n\n\nYu, K. N., Y. P. Cheung, T. Cheung, and R. C. Henry. 2004.\n“Identifying the Impact of Large Urban Airports on Local Air\nQuality by Nonparametric Regression.” Atmospheric\nEnvironment 38 (27): 4501–7.\n\n\nZhao, Weixiang, Philip K. Hopke, and Liming Zhou. 2007. “Spatial\nDistribution of Source Locations for Particulate Nitrate and Sulfate in\nthe Upper-Midwestern United States.” Atmospheric\nEnvironment 41 (9): 1831–47. https://doi.org/10.1016/j.atmosenv.2006.10.060.\n\n\nZhou, Chuanlong, Hao Zhou, Thomas M. Holsen, Philip K. Hopke, Eric S.\nEdgerton, and James J. Schwab. 2019. “Ambient Ammonia\nConcentrations Across New York State.” Journal of Geophysical\nResearch: Atmospheres 124 (14): 8287–8302. https://doi.org/10.1029/2019jd030380.\n\n\nZhou, L. 2004. “Comparison of Two Trajectory Based Models for\nLocating Particle Sources for Two Rural New York Sites.”\nAtmospheric Environment 38 (13): 1955–63. https://doi.org/10.1016/j.atmosenv.2003.12.034."
  },
  {
    "objectID": "appendix-annotate.html#adding-text",
    "href": "appendix-annotate.html#adding-text",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.1 Adding text",
    "text": "A.1 Adding text\nTo add text (or other annotations) it is necessary to know the coordinates on a plot for where the text will go, which will depend on the data plotted. In this extended example using the timePlot function, the y-axis will be in ordinary numerical units, whereas the x-axis will be in a date-time format (POSIXct).\nThere are various ways that annotations can be added, but the method used here is to add to the previous plot using a function called trellis.last.object() to which we want to add a later. This may seem complicated, but once a few examples are considered, the method becomes very powerful, flexible and straightforward. In a multi-panel plot such as Figure A.1 it is also useful to specify which rows/columns should be added to. If they are not specified then the annotation will appear in all panels.\nFirst, a plot should be produced to which we wish to add some text.\n\n## make sure latticeExtra is loaded\nlibrary(openair)\nlibrary(lubridate)\nlibrary(latticeExtra)\ntimePlot(selectByDate(mydata, year = 2003, month = \"aug\"),\n         pollutant = c(\"nox\", \"o3\", \"pm25\", \"pm10\", \"ws\"),\n         y.relation = \"free\")\n\nSo, considering @ref(fig:timePlotAnnotate), this is how the text `some missing data’ was added to the top panel.\n\ntrellis.last.object() +\n    layer(ltext(x = ymd_hm(\"2003-08-04 12:00\"), y = 200,\n                labels = \"some missing data\"), rows = 1)\n\nSo what does this do? First, the trellis.last.object() is simply the last plot that was plotted. Next the layer function is used to add some text. The text itself is added using the ltext lattice function. It is worth having a look at the help for ltext as that gives an overview of all the common annotations and other options. We have chosen to plot the text at position x = ‘2003-08-04’ and y = 200 and the label itself. A useful option to ltext is pos. Values can be 1, 2, 3 and 4, and indicate positions below (the default), to the left of, above and to the right of the specified coordinates"
  },
  {
    "objectID": "appendix-annotate.html#adding-text-and-a-shaded-area",
    "href": "appendix-annotate.html#adding-text-and-a-shaded-area",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.2 Adding text and a shaded area",
    "text": "A.2 Adding text and a shaded area\nThis time we will highlight an interval in row 2 (O3) and write some text on top. Note that this time we use the lpolygon function and choose to put it under everything else on the plot. For the text, we have chosen a colour (yellow) font type 2 (bold) and made it a bit bigger (cex = 1.5). Note also the y values extend beyond the actual limits shown on the plot — just to make sure they cover the whole region.\nThe polygon could of course be horizontal and more than one producing a series of ‘bands’ e.g. air quality indexes. A more sophisticated approach is shown later for PM2.5\n\n## add shaded polygon\ntrellis.last.object() +\n    layer(lpolygon(x = c(ymd_hm(\"2003-08-07 00:00\"),\n                   ymd_hm(\"2003-08-07 00:00\"), ymd_hm(\"2003-08-12 00:00\"),\n                   ymd_hm(\"2003-08-12 00:00\")), y = c(-20, 600, 600, -20),\n                   col = \"grey\", border = NA), under = TRUE, rows = 2)\n## add text\ntrellis.last.object() +\n    layer(ltext(x = ymd_hm(\"2003-08-09 12:00\"), y = 50,\n                labels = \"!!episode!!\", col = \"yellow\",\n                font = 2, cex = 1.5), rows = 2)\n\nThe small shaded, semi-transparent area shown in the bottom panel was added as follows:\n\n## add shaded polygon\nplt <- plt +\n    layer(lpolygon(x = c(ymd(\"2003-08-21\"), ymd(\"2003-08-21\"),\n                   ymd_hm(\"2003-08-23 00:00\"), ymd_hm(\"2003-08-23 00:00\")),\n                   y = c(4, 8, 8, 4), col = \"blue\", border = NA,\n                   alpha = 0.2), rows = 5)"
  },
  {
    "objectID": "appendix-annotate.html#adding-an-arrow",
    "href": "appendix-annotate.html#adding-an-arrow",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.3 Adding an arrow",
    "text": "A.3 Adding an arrow\nThe arrow shown on the first panel of Figure A.1 was added as follows. Note the code = 3 placed arrows at both ends. Note that angle is the angle from the shaft of the arrow to the edge of the arrow head.\n\ntrellis.last.object() +\n    layer(larrows(ymd_hm(\"2003-08-01 00:00\"), 100,\n                  ymd_hm(\"2003-08-08 14:00\"),\n                  100, code = 3, angle = 30), rows = 1)"
  },
  {
    "objectID": "appendix-annotate.html#adding-a-reference-line-and-text",
    "href": "appendix-annotate.html#adding-a-reference-line-and-text",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.4 Adding a reference line and text",
    "text": "A.4 Adding a reference line and text\nThis code adds a vertical dashed reference line shown in the 4th panel (PM10) along with some text aligned at 90 degrees using the srt option of ltext.\n\ntrellis.last.object() +\n    layer(panel.abline(v = ymd_hm(\"2003-08-25 00:00\"), lty = 5),\n          rows = 4)\ntrellis.last.object() +\n    layer(ltext(x = ymd_hm(\"2003-08-25 08:00\"), y = 60,\n                labels = \"reference line\", srt = 90), rows = 4)"
  },
  {
    "objectID": "appendix-annotate.html#highlight-a-specific-point",
    "href": "appendix-annotate.html#highlight-a-specific-point",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.5 Highlight a specific point",
    "text": "A.5 Highlight a specific point\nUp until now annotations have been added using arbitrary coordinates in each panel. What if we wanted to highlight a particular point, or more generally work with the actual data that are plotted. Knowing how to refer to existing data greatly extends the power of these functions.\nIt is possible to refer to a specific point in a panel simply by indexing the point of interest i.e. x, y. For example, to mark the 200th PM10 concentration (without knowing the actual date or value):\n\n## add a specific point\ntrellis.last.object() +\n    layer(lpoints(x[200], y[200], pch = 16, cex = 1.5),\n          rows = 4)\n\nWhat if we wanted to highlight the maximum O3 concentration? It is possible to work out the index first and then use that to refer to that point. Note the } to allow for the code to span multiple commands.\n\n## add a point to the max O3 concentration\ntrellis.last.object() +\n    layer({maxy <- which.max(y);\n           lpoints(x[maxy], y[maxy], col = \"black\", pch = 16)},\n          rows = 2)\n\n## label max ozone\ntrellis.last.object() +\n    layer({maxy <- which.max(y);\n           ltext(x[maxy], y[maxy], paste(y[maxy], \"ppb\"),\n                 pos = 4)}, rows = 2)"
  },
  {
    "objectID": "appendix-annotate.html#add-a-filled-polygon",
    "href": "appendix-annotate.html#add-a-filled-polygon",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.6 Add a filled polygon",
    "text": "A.6 Add a filled polygon\nIt can be seen in the top panel of Figure A.1 that some data are highlighted by filling the area below the line. This approach can be useful more generally in plotting. While it is possible to draw polygons easily and refer to the data itself, there needs to be a way for dealing with gaps in data, otherwise these gaps could be filled in perhaps unpredictable ways. A function has been written to draw a polygon taking into account gaps (poly.na).\n\npoly.na <- function(x1, y1, x2, y2, col = \"black\", alpha = 0.2) {\n  for(i in seq(2, length(x1)))\n    if (!any(is.na(y2[c(i - 1, i)])))\n      lpolygon(c(x1[i - 1], x1[i], x2[i], x2[i - 1]),\n               c(y1[i - 1], y1[i], y2[i], y2[i - 1]),\n               col = col, border = NA, alpha = alpha)\n}\n\nThis time we work out the ids of the data spanning an area of interest. Then the poly.na function is used. Note that the alpha transparency is by default 0.2 but another value can easily be supplied, as shown in the air quality ‘bands’ example.\n\ntrellis.last.object() +\n    layer({id <- which(x >= ymd_hm(\"2003-08-11 00:00\") &\n                       x <= ymd_hm(\"2003-08-25 00:00\"));\n           poly.na(x[id], y[id], x[id], rep(0, length(id)),\n                   col = \"darkorange\")}, rows = 1)"
  },
  {
    "objectID": "appendix-annotate.html#add-air-quality-bands-as-polygons",
    "href": "appendix-annotate.html#add-air-quality-bands-as-polygons",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.7 Add air quality bands as polygons",
    "text": "A.7 Add air quality bands as polygons\nIt is a simple extension to go from using a polygon below the data to polygons at certain intervals e.g. air quality indexes. These are shown for PM2.5 and the bands considered are 0–20, 20–30, 30–40 and >40.\n\ntrellis.last.object() +\n    layer(poly.na(x, y, x, rep(0, length(x)),\n                  col = \"green\", alpha = 1), rows = 3)\ntrellis.last.object() +\n    layer(poly.na(x, ifelse(y <20, NA, y), x,\n                  rep(20, length(x)), col = \"yellow\", alpha = 1),\n          rows = 3)\ntrellis.last.object() +\n    layer(poly.na(x, ifelse(y <30, NA, y),\n                  x, rep(30, length(x)),\n                  col = \"orange\", alpha = 1), rows = 3)\ntrellis.last.object() +\n    layer(poly.na(x, ifelse(y <40, NA, y),\n                  x, rep(40, length(x)),\n                  col = \"red\", alpha = 1), rows = 3)"
  },
  {
    "objectID": "appendix-annotate.html#polar-plot-examples",
    "href": "appendix-annotate.html#polar-plot-examples",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.8 Polar plot examples",
    "text": "A.8 Polar plot examples\nMany of the examples considered above are relevant to all other functions e.g. how to add text, choosing rows and columns to plot in. Polar coordinate plots are different because of the coordinate system used and this section considers a few examples.\nOne useful approach is to be able to draw an arc, perhaps highlighting an area of interest. A simple, but flexible function has been written to do this. It takes arguments theta1 and theta2 that define the angular area of interest and lower and upper to set the lower and upper wind speed, respectively. It also has additional arguments theta3 and theta4 which optionally set the angles for the ‘upper’ wind speed.\n\narc <- function(theta1 = 30, theta2 = 60, theta3 = theta1, theta4 = theta2,\n                lower = 1, upper = 10){\n  ## function to work out coordinates for an arc sector\n  if (theta2 < theta1) {\n    ang1 <- seq(theta1, 360, length = abs(theta2 - theta1))\n    ang2 <- seq(0, theta2, length = abs(theta2 - theta1))\n    angles.low <- c(ang1, ang2)\n\n    ## for upper angles\n    ang1 <- seq(theta1, 360, length = abs(theta4 - theta3))\n    ang2 <- seq(0, theta2, length = abs(theta4 - theta3))\n    angles.high <- c(ang1, ang2)\n\n  } else {\n    angles.low <- seq(theta1, theta2, length = abs(theta2 - theta1))\n    angles.high <- seq(theta3, theta4, length = abs(theta4 - theta3))\n  }\n  x1 <- lower * sin(pi * angles.low / 180)\n  y1 <- lower * cos(pi * angles.low / 180)\n  x2 <- rev(upper * sin(pi * angles.high / 180))\n  y2 <- rev(upper * cos(pi * angles.high / 180))\n  data.frame(x = c(x1, x2), y = c(y1, y2))\n\n}\n\n\n\n\n\nFigure A.2: Annotations on a polar plot.\n\n\n\n\nFollowing on from the previous examples, some annotations have been added to the basic polar plot for SO2 as shown in Figure A.2. Note that in these plots (0, 0) is the middle of the plot and the radial distance will be determined by the wind speed — or whatever the radial variable is. This way of plotting arcs can also be applied to other functions that show directional data.\n\npolarPlot(mydata, pollutant = \"so2\", col = \"turbo\")\ntrellis.last.object() + layer(ltext(-12, -12, \"A\", cex = 2))\ntrellis.last.object() + layer(ltext(10, 2, \"B\", cex = 2, col = \"white\"))\ntrellis.last.object() + layer(lsegments(0, 0, -11.5, -11.5, lty = 5))\n## add and arc to highlight area of interest\ntrellis.last.object() +\n    layer(lpolygon(x = arc(theta1 = 60, theta2 = 120, lower = 2,\n                   upper = 15)$x, y = arc(theta1 = 60,\n                                  theta2 = 120, lower = 2,\n                                  upper = 15)$y, lty = 1, lwd = 2))"
  },
  {
    "objectID": "appendix-annotate.html#using-grid-graphics-identify-locations-interactively",
    "href": "appendix-annotate.html#using-grid-graphics-identify-locations-interactively",
    "title": "Appendix A — Annotating openair plots",
    "section": "\nA.9 Using grid graphics — identify locations interactively",
    "text": "A.9 Using grid graphics — identify locations interactively\nThe examples above provide a precise way of annotating plots for single or multi-panels openair displays. However, these methods won’t work for plots that consist of completely separate plots such as the four plots in timeVariation. There are however other methods that can be used to annotate such plots using the package grid, which forms the basis of lattice graphics. There is enormous capability for annotating plots using the grid package and only a few simple examples are given here.\nGiven a a basic timeVariation plot, how could texts be added at any location — say in the middle monthly plot? One very useful function for this type of annotation that allows the user to interactively choose a location is the grid.locator() function in the grid package. That function can be called with different coordinate systems — but the one we want defines the bottom-left corner as (0, 0) and the top right as (1, 1).\nFirst, make a timeVariation plot.\n\ntimeVariation(mydata)\n\nNow let’s choose a location on the plot interactively using the mouse and selecting somewhere in the middle of the monthly plot.\n\nlibrary(grid)\n## bring up the interactive location chooser\ngrid.locator(unit = \"npc\")\n\nWhat should happen is that in the R console the coordinates are given for that point. In my case these were x = 0.503 and y = 0.338. These coordinates can now be used as the basis of adding some text or other annotation. In the example below, the grid.text function is used to add some text for these coordinates making the font bigger (cex = 2), bold (font = 2) and blue (col = \"blue\").\n\ngrid.text(x = 0.503, y = 0.338, label = \"here!\",\n          gp = gpar(cex = 2, font = 2, col = \"blue\"))\n\nEven with this basic approach, some sophisticated annotation is possible with any openair plot. There are many other functions that can be used from the grid package that would allow for polygons, segments and other features to be drawn is a similar way to the examples earlier in this section. Continuing with the same example, here is how to add an arrow pointing to the maximum concentration shown on the top plot for Saturday (again using the grid.locator function).\n\ngrid.lines(x = c(0.736, 0.760), y = c(0.560, 0.778),\n           arrow = arrow())\ngrid.text(x = 0.736, y = 0.560, label = \"maximum\", just = \"left\")"
  },
  {
    "objectID": "appendix-hysplit.html",
    "href": "appendix-hysplit.html",
    "title": "Appendix B — Production of HYSPLIT trajectory files",
    "section": "",
    "text": "As discussed in Chapter 10, openair can import pre-calculated trajectory data for specified locations for whole years run at a 3-hour interval. The data are stored on a Ricardo webserver to make it easy to import 96-hour back trajectory data. Several users have requested how they can run HYSPLIT themselves e.g. for different trajectory start heights or for many locations. It should be noted that Hysplit is a powerful and flexible model and to exploit its full potential, users should consider using Hysplit directly (Stein et al. 2015).\n\nStein, A. F., R. R. Draxler, G. D. Rolph, B. J. B. Stunder, M. D. Cohen, and F. Ngan. 2015. “NOAA’s HYSPLIT Atmospheric Transport and Dispersion Modeling System.” Bulletin of the American Meteorological Society 96 (12): 2059–77. https://doi.org/10.1175/bams-d-14-00110.1.\nA well-written R package called splitr by Rich Iannone (see here) provides a lot of flexibility for working with different meteorological inputs and Hysplit options. Users should consider splitr if they want more control over Hysplit options — especially for the detailed investigation of short time periods such as pollution episodes where higher resolution meteorological data is useful.\nThis focus of this section is to run Hysplit with the Reanalysis meteorological data over periods of a year to allow for the analysis of longer-term air quality data analysis. The code below assumes that full years are run, but it could be adopted for shorter periods. There are three main parts to producing trajectory files:\n\nDownload and install the NOAA Hysplit model, somewhere with write access (see below).\nDownload the monthly meteorological (.gbl) files also from the NOAA website.\nObtain the code to run Hysplit.\n\nTo run back trajectories it is necessary to download the meteorological data files. The easiest way to download the meteorological files is using the function below.\n\ngetMet <- function(year = 2013, month = 1,\n                   path_met = \"~/TrajData/\") {\n  for (i in seq_along(year)) {\n    for (j in seq_along(month)) {\n      download.file(\n        url = paste0(\n          \"ftp://arlftp.arlhq.noaa.gov/archives/reanalysis/RP\",\n          year[i], sprintf(\"%02d\", month[j]), \".gbl\"\n        ),\n        destfile = paste0(\n          path_met, \"RP\", year[i],\n          sprintf(\"%02d\", month[j]), \".gbl\"\n        ),\n        mode = \"wb\"\n      )\n    }\n  }\n}\n\nThe function will download monthly met files (each about 120 MB) to the chosen directory. Note that the met data files only need be downloaded once. For example, to download files for 2013:\n\ngetMet(year = 2013, month = 1:12)\n\nIt is first necessary on ensure that the devtools package is installed, which is needed to load some R functions stored as a GitHub gist (some code that can be shared publicly). Copy the code below and paste it into R, which will load the necessary functions needed to run Hysplit.\n\nlibrary(devtools)\nsource_gist(\"https://gist.github.com/davidcarslaw/c67e33a04ff6e1be0cd7357796e4bdf5\",\n            filename = \"run_hysplit.R\")\n\nNow there should be several loaded functions, including run_hysplit. To run Hysplit, have a look at the examples here.\nFor example\n\ndata_out <- run_hysplit(\nlatitude = 36.134, \n longitude = -5.347, \n  runtime = -96, \n  start_height = 10, \n  model_height = 10000, \n  start = 2015,\n  end = \"2015-01-10\",\n  hysplit_exec = \"~/hysplit/exec\", \n  hysplit_input = \"~/trajData\", \n  hysplit_output = \"~/temp\",\n  site = \"gibraltar\")\n\nThe data_out can then be used directly in openair trajectory functions. The code will also return information from Hysplit such as pressure, temperature, terrain and boundary layer height.\nMost of the options should be self-explanatory but hysplit_exec is the path to the Hysplit executable, hysplit_input is the path to the meteorological files (downloaded as described above) and hysplit_output is the directory where Hysplit will write its temporary files.\nOnce run it is then advisable to store the data somewhere. Save it like:\n\nsaveRDS(data_out, file = \"~/trajProc/myTrajData.rds\")\n\nThen it is easy to read in later and use e.g.\n\ntraj <- readRDS(\"~/trajProc/myTrajData.rds\")"
  },
  {
    "objectID": "appendix-trends.html",
    "href": "appendix-trends.html",
    "title": "Appendix C — A closer look at trends",
    "section": "",
    "text": "Understanding trends is a core component of air quality and the atmospheric sciences in general. openair provides two main functions for considering trends (smoothTrend and TheilSen; see Chapter 17 and Chapter 16, the latter useful for linear trend estimates. Understanding trends and quantifying them robustly is not so easy and careful analysis would treat each time series individually and consider a wide range of diagnostics. In this section we take advantage of some of the excellent capabilities that R has to consider fitting trend models. Experience with real atmospheric composition data shows that trends are rarely linear, which is unfortunate given how much of statistics has been built around the linear model.\nGeneralized Additive Models (GAMs) offer a flexible approach to calculating trends and in particular, the mgcv package contains many functions that are very useful for such modelling. Some of the details of this type of model are presented in Wood (2006) and the mgcv package itself.\n\nWood, S. N. 2006. Generalized Additive Models: An Introduction with r. Chapman; Hall/CRC.\nThe example considered is 23 years of O3 measurements at Mace Head on the West coast of Ireland. The example shows the sorts of steps that might be taken to build a model to explain the trend. The data are first imported and then the year, month and trend' estimated. Note thattrend’ here is simply a decimal date that can be used to construct various explanatory models.\nFirst we import the data:\n\n\n\n\nlibrary(mgcv)\ndat <- importAURN(site = \"mh\", year = 1988:2010)\n\n\n## calculate monthly means\nmonthly <- timeAverage(dat, avg.time = \"month\")\n## now calculate components for the models\nmonthly$year <- as.numeric(format(monthly$date, \"%Y\"))\nmonthly$month <- as.numeric(format(monthly$date, \"%m\"))\nmonthly <- transform(monthly, trend = year + (month - 1) / 12)\n\nIt is always a good idea to plot the data first:\n\ntimePlot(monthly, pollutant = \"o3\")\n\n\n\nFigure C.1: Monthly mean O3 concentrations at Mace Head, Ireland (1998–2010).\n\n\n\n\nFigure C.1 shows that there is a clear seasonal variation in O3 concentrations, which is certainly expected. Less obvious is whether there is a trend.\nEven though it is known there is a seasonal signal in the data, we will first of all ignore it and build a simple model that only has a trend component (model M0).\n\nM0 <- gam(o3 ~ s(trend), data = monthly)\nsummary(M0)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\no3 ~ s(trend)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.3428     0.6198   115.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n         edf Ref.df     F p-value   \ns(trend)   1      1 6.958 0.00882 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.0212   Deviance explained = 2.48%\nGCV = 106.82  Scale est. = 106.04    n = 276\n\n\nThis model only explains about 2% of the variation as shown by the adjusted r2. More of a problem however is that no account has been taken of the seasonal variation. An easy way of seeing the effect of this omission is to plot the autocorrelation function (ACF) of the residuals, shown in Figure C.2). This Figure clearly shows the residuals have a strong seasonal pattern. Chatfield (2004) provides lots of useful information on time series modelling.\n\nChatfield, Christopher. 2004. The Analysis of Time Series : An Introduction / Chris Chatfield. 6th ed. Boca Raton, FL ; London : Chapman & Hall/CRC.\n\nacf(residuals(M0))\n\n\n\nFigure C.2: ACF for the residuals of model M0.\n\n\n\n\nA refined model should therefore take account of the seasonal variation in O3 concentrations. Therefore, we add a term taking account of the seasonal variation. Note also that we choose a cyclic spline for the monthly component (bs = \"cc\"), which joins the first and last points i.e. January and December.\n\nM1 <- gam(o3 ~ s(trend) + s(month, bs = \"cc\"), data = monthly)\nsummary(M1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\no3 ~ s(trend) + s(month, bs = \"cc\")\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.3428     0.3738   190.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F  p-value    \ns(trend) 1.218  1.404 17.95 1.31e-05 ***\ns(month) 6.111  8.000 59.35  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.644   Deviance explained = 65.4%\nGCV = 39.766  Scale est. = 38.566    n = 276\n\n\nNow we have a model that explains much more of the variation with an r2 of 0.65. Also, the p-values for the trend and seasonal components are both highly statistically significant. Let’s have a look at the separate components for trend and seasonal variation:\n\nplot.gam(M1, select = 1, shade = TRUE)\n\n\n\nFigure C.3: The trend component of model M1.\n\n\n\n\n\nplot.gam(M1, select = 2, shade = TRUE)\n\n\n\nFigure C.4: The seasonal component of model M1.\n\n\n\n\nThe seasonal component shown in Figure C.4 clearly shows the strong seasonal effect on O3 at this site (peaking in April). The trend component is actually linear in this case and could be modelled as such. This model looks much better, but as is often the case autocorrelation could remain important. The ACF is shown in Figure C.5 and shows there is still some short-term correlation in the residuals.\nNote also that there are other diagnostic tests one should consider when comparing these models that are not shown here e.g. such as considering the normality of the residuals. Indeed a consideration of the residuals shows that the model fails to some extent in explaining the very low values of O3, which can be seen in Figure C.1. These few points (which skew the residuals) may well be associated with air masses from the polluted regions of Europe. Better and more useful models would likely be possible if the data were split my airmass origin, which is something that will be returned to when openair includes a consideration of back trajectories.\n\nacf(residuals(M1))\n\n\n\nFigure C.5: ACF for the residuals of model M1.\n\n\n\n\nFurther tests, also considering the partial autocorrelation function (PACF) suggest that an AR1 model is suitable for modelling this short-term autocorrelation. This is where modelling using a GAMM (Generalized Additive Mixed Model) comes in because it is possible to model the short-term autocorrelation using a linear mixed model. The gamm function uses the package nmle and the Generalized Linear Mixed Model (GLMM) fitting routine. In the M2 model below the correlation structure is considered explicitly.\n\nM2 <- gamm(o3 ~ s(month, bs = \"cc\")  + s(trend), \n           data = monthly,\n           correlation = corAR1(form = ~ month | year))\nsummary(M2$gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\no3 ~ s(month, bs = \"cc\") + s(trend)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.3163     0.4933   144.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n           edf Ref.df     F  p-value    \ns(month) 6.915      8 42.18  < 2e-16 ***\ns(trend) 1.000      1 15.07 0.000131 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.643   \n  Scale est. = 38.863    n = 276\n\n\nThe ACF plot is shown in Figure C.6 and shows that the autocorrelation has been dealt with and we can be rather more confident about the trend component (not plotted). Note that in this case we need to use the normalized residuals to get residuals that take account of the fitted correlation structure.\n\nacf(residuals(M2$lme, type = \"normalized\"))\n\n\n\nFigure C.6: ACF for the residuals of model M2.\n\n\n\n\nNote that model M2 assumes that the trend and seasonal terms vary independently of one another. However, if the seasonal amplitude and/or phase change over time then a model that accounts for the interaction between the two may be better. Indeed, this does seem to be the case here, as shown by the improved fit of the model below. This model uses a tensor product smooth (te) and the reason for doing this and not using an isotropic smooth (s) is that the trend and seasonal components are essentially on different scales. We would not necessarily want to apply the same level of smoothness to both components. An example of covariates on the same scale would be latitude and longitude.\n\nM3 <- gamm(o3 ~ s(month, bs = \"cc\")  + \n             te(trend, month), data = monthly,\n           correlation = corAR1(form = ~ month | year))\nsummary(M3$gam)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\no3 ~ s(month, bs = \"cc\") + te(trend, month)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.3213     0.4582   155.7   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                  edf Ref.df      F  p-value    \ns(month)        6.886  8.000 22.220  < 2e-16 ***\nte(trend,month) 4.162  4.162  7.959 7.93e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.667   \n  Scale est. = 36.121    n = 276\n\n\nIt becomes a bit more difficult to plot the two-way interaction between the trend and the month, but it is possible with a surface as shown in Figure C.7. This plot shows for example that during summertime the trends component varies little. However for the autumn and winter months there has been a much greater increase in the trend component for O3.\n\nplot(M3$gam, select = 2, scheme = 1, theta = 225, phi = 10,ticktype = \"detailed\")\n\n\n\nFigure C.7: Plot showing the two-way interaction between the trend and seasonal components.\n\n\n\n\nWhile there have been many steps involved in this short analysis, the data at Mace Head are not typical of most air quality data observed, say in urban areas. Much of the data considered in these areas does not appear to have significant autocorrelation in the residuals once the seasonal variation has been accounted for, therefore avoiding the complexities of taking account of the correlation structure of the data. It may be for example that sites like Mace Head and a pollutant such as O3 are much more prone to larger scale atmospheric processes that are not captured by these models."
  }
]